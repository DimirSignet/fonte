{"sha": "0ce5e0d34d02d0021b4232f923ccfd49b75aa6e3", "log": "Better unit test coverage for StrTokenizer.java.  ", "commit": "\n--- a/src/java/org/apache/commons/lang/text/StrTokenizer.java\n+++ b/src/java/org/apache/commons/lang/text/StrTokenizer.java\n     //-----------------------------------------------------------------------\n \n     /**\n+     * Returns a clone of <code>CSV_TOKENIZER_PROTOTYPE</code>.\n+     * \n+     * @return a clone of <code>CSV_TOKENIZER_PROTOTYPE</code>.\n+     */\n+    private static StrTokenizer getCSVClone() {\n+        return (StrTokenizer) CSV_TOKENIZER_PROTOTYPE.clone();\n+    }\n+\n+    /**\n      * Gets a new tokenizer instance which parses Comma Seperated Value strings\n      * initializing it with the given input.  The default for CSV processing\n      * will be trim whitespace from both ends (which can be overriden with\n      * @return a new tokenizer instance which parses Comma Seperated Value strings\n      */\n     public static StrTokenizer getCSVInstance() {\n-        return (StrTokenizer)CSV_TOKENIZER_PROTOTYPE.clone();\n+        return getCSVClone();\n     }\n \n     /**\n      * @return a new tokenizer instance which parses Comma Seperated Value strings\n      */\n     public static StrTokenizer getCSVInstance(String input) {\n-        StrTokenizer tok = (StrTokenizer)(CSV_TOKENIZER_PROTOTYPE.clone());\n+        StrTokenizer tok = getCSVClone();\n         tok.reset(input);\n         return tok;\n     }\n      * @return a new tokenizer instance which parses Comma Seperated Value strings\n      */\n     public static StrTokenizer getCSVInstance(char[] input) {\n-        StrTokenizer tok = (StrTokenizer)(CSV_TOKENIZER_PROTOTYPE.clone());\n+        StrTokenizer tok = getCSVClone();\n         tok.reset(input);\n         return tok;\n     }\n+\n+    /**\n+     * Returns a clone of <code>TSV_TOKENIZER_PROTOTYPE</code>.\n+     * \n+     * @return a clone of <code>TSV_TOKENIZER_PROTOTYPE</code>.\n+     */\n+    private static StrTokenizer getTSVClone() {\n+        return (StrTokenizer) TSV_TOKENIZER_PROTOTYPE.clone();\n+    }\n+\n \n     /**\n      * Gets a new tokenizer instance which parses Tab Seperated Value strings.\n      * @return a new tokenizer instance which parses Tab Seperated Value strings.\n      */\n     public static StrTokenizer getTSVInstance() {\n-        return (StrTokenizer)TSV_TOKENIZER_PROTOTYPE.clone();\n+        return getTSVClone();\n     }\n \n     /**\n      * @return a new tokenizer instance which parses Tab Seperated Value strings.\n      */\n     public static StrTokenizer getTSVInstance(String input) {\n-        StrTokenizer tok = (StrTokenizer)(TSV_TOKENIZER_PROTOTYPE.clone());\n+        StrTokenizer tok = getTSVClone();\n         tok.reset(input);\n         return tok;\n     }\n      * @return a new tokenizer instance which parses Tab Seperated Value strings.\n      */\n     public static StrTokenizer getTSVInstance(char[] input) {\n-        StrTokenizer tok = (StrTokenizer)(TSV_TOKENIZER_PROTOTYPE.clone());\n+        StrTokenizer tok = getTSVClone();\n         tok.reset(input);\n         return tok;\n     }\n--- a/src/test/org/apache/commons/lang/text/StrTokenizerTest.java\n+++ b/src/test/org/apache/commons/lang/text/StrTokenizerTest.java\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n+\n package org.apache.commons.lang.text;\n \n import junit.framework.Test;\n \n /**\n  * Unit test for Tokenizer.\n- *\n+ * \n  * @author Matthew Inger\n  */\n public class StrTokenizerTest extends TestCase {\n-    \n+\n+    private static final String CSV_SIMPLE_FIXTURE = \"A,b,c\";\n+\n+    private static final String TSV_SIMPLE_FIXTURE = \"A\\tb\\tc\";\n+\n+    public static void main(String[] args) {\n+        TestRunner.run(suite());\n+    }\n+\n+    public static Test suite() {\n+        TestSuite suite = new TestSuite(StrTokenizerTest.class);\n+        suite.setName(\"TokenizerTest Tests\");\n+        return suite;\n+    }\n+\n     /**\n      * JUnit constructor.\n+     * \n      * @param name\n      */\n     public StrTokenizerTest(String name) {\n         super(name);\n     }\n \n-    public static Test suite() {\n-        TestSuite suite = new TestSuite(StrTokenizerTest.class);\n-        suite.setName(\"TokenizerTest Tests\");\n-        return suite;\n-    }\n-\n-\n-    public static void main(String[] args) {\n-        TestRunner.run(suite());\n-    }\n-\n-    //-----------------------------------------------------------------------\n+    private void checkClone(StrTokenizer tokenizer) {\n+        assertFalse(StrTokenizer.getCSVInstance() == tokenizer);\n+        assertFalse(StrTokenizer.getTSVInstance() == tokenizer);\n+    }\n+\n+    // -----------------------------------------------------------------------\n     public void test1() {\n \n         String input = \"a;b;c;\\\"d;\\\"\\\"e\\\";f; ; ;\";\n         tok.setQuoteChar('\"');\n         tok.setIgnoredMatcher(StrTokenizer.TRIM_MATCHER);\n         tok.setIgnoreEmptyTokens(false);\n-        String tokens [] = tok.getAllTokens();\n-\n-        String expected[] = new String[]\n-        {\n-            \"a\",\n-            \"b\",\n-            \"c\",\n-            \"d;\\\"e\",\n-            \"f\",\n-            \"\",\n-            \"\",\n-            \"\",\n-        };\n-\n-        assertTrue(tokens.length == expected.length);\n-        for (int i = 0; i < expected.length; i++) {\n-            assertTrue(\"token[\" + i + \"] was '\" + tokens[i]\n-                    + \"' but was expected to be '\" + expected[i]\n-                    + \"'\",\n-                    ObjectUtils.equals(expected[i], tokens[i]));\n-        }\n-\n-    }\n-\n+        String tokens[] = tok.getAllTokens();\n+\n+        String expected[] = new String[]{\"a\", \"b\", \"c\", \"d;\\\"e\", \"f\", \"\", \"\", \"\",};\n+\n+        assertTrue(tokens.length == expected.length);\n+        for (int i = 0; i < expected.length; i++) {\n+            assertTrue(\"token[\" + i + \"] was '\" + tokens[i] + \"' but was expected to be '\" + expected[i] + \"'\",\n+                    ObjectUtils.equals(expected[i], tokens[i]));\n+        }\n+\n+    }\n \n     public void test2() {\n \n         tok.setQuoteChar('\"');\n         tok.setIgnoredMatcher(StrTokenizer.NONE_MATCHER);\n         tok.setIgnoreEmptyTokens(false);\n-        String tokens [] = tok.getAllTokens();\n-\n-        String expected[] = new String[]\n-        {\n-            \"a\",\n-            \"b\",\n-            \"c \",\n-            \"d;\\\"e\",\n-            \"f\",\n-            \" \",\n-            \" \",\n-            \"\",\n-        };\n-\n-        assertTrue(tokens.length == expected.length);\n-        for (int i = 0; i < expected.length; i++) {\n-            assertTrue(\"token[\" + i + \"] was '\" + tokens[i]\n-                    + \"' but was expected to be '\" + expected[i]\n-                    + \"'\",\n-                    ObjectUtils.equals(expected[i], tokens[i]));\n-        }\n-\n-    }\n-\n+        String tokens[] = tok.getAllTokens();\n+\n+        String expected[] = new String[]{\"a\", \"b\", \"c \", \"d;\\\"e\", \"f\", \" \", \" \", \"\",};\n+\n+        assertTrue(tokens.length == expected.length);\n+        for (int i = 0; i < expected.length; i++) {\n+            assertTrue(\"token[\" + i + \"] was '\" + tokens[i] + \"' but was expected to be '\" + expected[i] + \"'\",\n+                    ObjectUtils.equals(expected[i], tokens[i]));\n+        }\n+\n+    }\n \n     public void test3() {\n \n         tok.setQuoteChar('\"');\n         tok.setIgnoredMatcher(StrTokenizer.NONE_MATCHER);\n         tok.setIgnoreEmptyTokens(false);\n-        String tokens [] = tok.getAllTokens();\n-\n-        String expected[] = new String[]\n-        {\n-            \"a\",\n-            \"b\",\n-            \" c\",\n-            \"d;\\\"e\",\n-            \"f\",\n-            \" \",\n-            \" \",\n-            \"\",\n-        };\n-\n-        assertTrue(tokens.length == expected.length);\n-        for (int i = 0; i < expected.length; i++) {\n-            assertTrue(\"token[\" + i + \"] was '\" + tokens[i]\n-                    + \"' but was expected to be '\" + expected[i]\n-                    + \"'\",\n-                    ObjectUtils.equals(expected[i], tokens[i]));\n-        }\n-\n-    }\n-\n+        String tokens[] = tok.getAllTokens();\n+\n+        String expected[] = new String[]{\"a\", \"b\", \" c\", \"d;\\\"e\", \"f\", \" \", \" \", \"\",};\n+\n+        assertTrue(tokens.length == expected.length);\n+        for (int i = 0; i < expected.length; i++) {\n+            assertTrue(\"token[\" + i + \"] was '\" + tokens[i] + \"' but was expected to be '\" + expected[i] + \"'\",\n+                    ObjectUtils.equals(expected[i], tokens[i]));\n+        }\n+\n+    }\n \n     public void test4() {\n \n         tok.setQuoteChar('\"');\n         tok.setIgnoredMatcher(StrTokenizer.TRIM_MATCHER);\n         tok.setIgnoreEmptyTokens(true);\n-        String tokens [] = tok.getAllTokens();\n-\n-        String expected[] = new String[]\n-        {\n-            \"a\",\n-            \"b\",\n-            \"c\",\n-            \"d;\\\"e\",\n-            \"f\",\n-        };\n-\n-        assertTrue(tokens.length == expected.length);\n-        for (int i = 0; i < expected.length; i++) {\n-            assertTrue(\"token[\" + i + \"] was '\" + tokens[i]\n-                    + \"' but was expected to be '\" + expected[i]\n-                    + \"'\",\n-                    ObjectUtils.equals(expected[i], tokens[i]));\n-        }\n-\n-    }\n-\n+        String tokens[] = tok.getAllTokens();\n+\n+        String expected[] = new String[]{\"a\", \"b\", \"c\", \"d;\\\"e\", \"f\",};\n+\n+        assertTrue(tokens.length == expected.length);\n+        for (int i = 0; i < expected.length; i++) {\n+            assertTrue(\"token[\" + i + \"] was '\" + tokens[i] + \"' but was expected to be '\" + expected[i] + \"'\",\n+                    ObjectUtils.equals(expected[i], tokens[i]));\n+        }\n+\n+    }\n \n     public void test5() {\n \n         tok.setIgnoredMatcher(StrTokenizer.TRIM_MATCHER);\n         tok.setIgnoreEmptyTokens(false);\n         tok.setEmptyTokenAsNull(true);\n-        String tokens [] = tok.getAllTokens();\n-\n-        String expected[] = new String[]\n-        {\n-            \"a\",\n-            \"b\",\n-            \"c\",\n-            \"d;\\\"e\",\n-            \"f\",\n-            null,\n-            null,\n-            null,\n-        };\n-\n-        assertTrue(tokens.length == expected.length);\n-        for (int i = 0; i < expected.length; i++) {\n-            assertTrue(\"token[\" + i + \"] was '\" + tokens[i]\n-                    + \"' but was expected to be '\" + expected[i]\n-                    + \"'\",\n-                    ObjectUtils.equals(expected[i], tokens[i]));\n-        }\n-\n-    }\n-\n+        String tokens[] = tok.getAllTokens();\n+\n+        String expected[] = new String[]{\"a\", \"b\", \"c\", \"d;\\\"e\", \"f\", null, null, null,};\n+\n+        assertTrue(tokens.length == expected.length);\n+        for (int i = 0; i < expected.length; i++) {\n+            assertTrue(\"token[\" + i + \"] was '\" + tokens[i] + \"' but was expected to be '\" + expected[i] + \"'\",\n+                    ObjectUtils.equals(expected[i], tokens[i]));\n+        }\n+\n+    }\n \n     public void test6() {\n \n         tok.setQuoteChar('\"');\n         tok.setIgnoredMatcher(StrTokenizer.TRIM_MATCHER);\n         tok.setIgnoreEmptyTokens(false);\n-//        tok.setTreatingEmptyAsNull(true);\n-        String tokens [] = tok.getAllTokens();\n-\n-        String expected[] = new String[]\n-        {\n-            \"a\",\n-            \"b\",\n-            \" c\",\n-            \"d;\\\"e\",\n-            \"f\",\n-            null,\n-            null,\n-            null,\n-        };\n+        // tok.setTreatingEmptyAsNull(true);\n+        String tokens[] = tok.getAllTokens();\n+\n+        String expected[] = new String[]{\"a\", \"b\", \" c\", \"d;\\\"e\", \"f\", null, null, null,};\n \n         int nextCount = 0;\n         while (tok.hasNext()) {\n \n         assertTrue(tokens.length == expected.length);\n \n-        assertTrue(\"could not cycle through entire token list\"\n-                + \" using the 'hasNext' and 'next' methods\",\n+        assertTrue(\"could not cycle through entire token list\" + \" using the 'hasNext' and 'next' methods\",\n                 nextCount == expected.length);\n \n-        assertTrue(\"could not cycle through entire token list\"\n-                + \" using the 'hasPrevious' and 'previous' methods\",\n+        assertTrue(\"could not cycle through entire token list\" + \" using the 'hasPrevious' and 'previous' methods\",\n                 prevCount == expected.length);\n \n     }\n-\n \n     public void test7() {\n \n         tok.setQuoteMatcher(StrTokenizer.DOUBLE_QUOTE_MATCHER);\n         tok.setIgnoredMatcher(StrTokenizer.NONE_MATCHER);\n         tok.setIgnoreEmptyTokens(false);\n-        String tokens [] = tok.getAllTokens();\n-\n-        String expected[] = new String[]\n-        {\n-            \"a\",\n-            \"\",\n-            \"\",\n-            \"b\",\n-            \"c\",\n-            \"d e\",\n-            \"f\",\n-            \"\",\n-        };\n-\n-        assertTrue(tokens.length == expected.length);\n-        for (int i = 0; i < expected.length; i++) {\n-            assertTrue(\"token[\" + i + \"] was '\" + tokens[i]\n-                    + \"' but was expected to be '\" + expected[i]\n-                    + \"'\",\n-                    ObjectUtils.equals(expected[i], tokens[i]));\n-        }\n-\n-    }\n-\n+        String tokens[] = tok.getAllTokens();\n+\n+        String expected[] = new String[]{\"a\", \"\", \"\", \"b\", \"c\", \"d e\", \"f\", \"\",};\n+\n+        assertTrue(tokens.length == expected.length);\n+        for (int i = 0; i < expected.length; i++) {\n+            assertTrue(\"token[\" + i + \"] was '\" + tokens[i] + \"' but was expected to be '\" + expected[i] + \"'\",\n+                    ObjectUtils.equals(expected[i], tokens[i]));\n+        }\n+\n+    }\n \n     public void test8() {\n \n         tok.setQuoteMatcher(StrTokenizer.DOUBLE_QUOTE_MATCHER);\n         tok.setIgnoredMatcher(StrTokenizer.NONE_MATCHER);\n         tok.setIgnoreEmptyTokens(true);\n-        String tokens [] = tok.getAllTokens();\n-\n-        String expected[] = new String[]\n-        {\n-            \"a\",\n-            \"b\",\n-            \"c\",\n-            \"d e\",\n-            \"f\",\n-        };\n-\n-        assertTrue(tokens.length == expected.length);\n-        for (int i = 0; i < expected.length; i++) {\n-            assertTrue(\"token[\" + i + \"] was '\" + tokens[i]\n-                    + \"' but was expected to be '\" + expected[i]\n-                    + \"'\",\n+        String tokens[] = tok.getAllTokens();\n+\n+        String expected[] = new String[]{\"a\", \"b\", \"c\", \"d e\", \"f\",};\n+\n+        assertTrue(tokens.length == expected.length);\n+        for (int i = 0; i < expected.length; i++) {\n+            assertTrue(\"token[\" + i + \"] was '\" + tokens[i] + \"' but was expected to be '\" + expected[i] + \"'\",\n                     ObjectUtils.equals(expected[i], tokens[i]));\n         }\n \n         assertEquals(\"b\", tok.next());\n         assertEquals(\"c\", tok.next());\n     }\n-    \n+\n     public void testBasic2() {\n         String input = \"a \\nb\\fc\";\n         StrTokenizer tok = new StrTokenizer(input);\n         assertEquals(\"b\", tok.next());\n         assertEquals(\"c\", tok.next());\n     }\n-    \n+\n     public void testBasic3() {\n         String input = \"a \\nb\\u0001\\fc\";\n         StrTokenizer tok = new StrTokenizer(input);\n         assertEquals(\"b\\u0001\", tok.next());\n         assertEquals(\"c\", tok.next());\n     }\n-    \n+\n     public void testBasic4() {\n         String input = \"a \\\"b\\\" c\";\n         StrTokenizer tok = new StrTokenizer(input);\n         assertEquals(\"\\\"b\\\"\", tok.next());\n         assertEquals(\"c\", tok.next());\n     }\n-    \n-    public void testBasicQuoted1() {\n-        String input = \"a \\\"b\\\" c\";\n-        StrTokenizer tok = new StrTokenizer(input, ' ', '\"');\n-        assertEquals(\"a\", tok.next());\n-        assertEquals(\"b\", tok.next());\n-        assertEquals(\"c\", tok.next());\n-    }\n-    \n+\n     public void testBasicDelim1() {\n         String input = \"a:b:c\";\n         StrTokenizer tok = new StrTokenizer(input, ':');\n         assertEquals(\"b\", tok.next());\n         assertEquals(\"c\", tok.next());\n     }\n-    \n+\n     public void testBasicDelim2() {\n         String input = \"a:b:c\";\n         StrTokenizer tok = new StrTokenizer(input, ',');\n         assertEquals(\"a:b:c\", tok.next());\n     }\n-    \n+\n     public void testBasicEmpty1() {\n         String input = \"a  b c\";\n         StrTokenizer tok = new StrTokenizer(input);\n         assertEquals(\"b\", tok.next());\n         assertEquals(\"c\", tok.next());\n     }\n-    \n+\n     public void testBasicEmpty2() {\n         String input = \"a  b c\";\n         StrTokenizer tok = new StrTokenizer(input);\n         assertEquals(\"b\", tok.next());\n         assertEquals(\"c\", tok.next());\n     }\n-    \n+\n+    public void testBasicQuoted1() {\n+        String input = \"a \\\"b\\\" c\";\n+        StrTokenizer tok = new StrTokenizer(input, ' ', '\"');\n+        assertEquals(\"a\", tok.next());\n+        assertEquals(\"b\", tok.next());\n+        assertEquals(\"c\", tok.next());\n+    }\n+\n+    public void testCSV(String data) {\n+        this.testXSVAbc(StrTokenizer.getCSVInstance(data));\n+        this.testXSVAbc(StrTokenizer.getCSVInstance(data.toCharArray()));\n+    }\n+\n+    public void testCSVEmpty() {\n+        this.testEmpty(StrTokenizer.getCSVInstance());\n+        this.testEmpty(StrTokenizer.getCSVInstance(\"\"));\n+    }\n+\n+    public void testCSVSimple() {\n+        this.testCSV(CSV_SIMPLE_FIXTURE);\n+    }\n+\n+    public void testCSVSimpleNeedsTrim() {\n+        this.testCSV(\"   \" + CSV_SIMPLE_FIXTURE);\n+        this.testCSV(\"   \\n\\t  \" + CSV_SIMPLE_FIXTURE);\n+        this.testCSV(\"   \\n  \" + CSV_SIMPLE_FIXTURE + \"\\n\\n\\r\");\n+    }\n+\n+    void testEmpty(StrTokenizer tokenizer) {\n+        this.checkClone(tokenizer);\n+        assertEquals(false, tokenizer.hasNext());\n+        assertEquals(false, tokenizer.hasPrevious());\n+        assertEquals(null, tokenizer.next());\n+        assertEquals(null, tokenizer.nextToken());\n+        assertEquals(0, tokenizer.size());\n+    }\n+\n     public void testGetContent() {\n         String input = \"a   b c \\\"d e\\\" f \";\n         StrTokenizer tok = new StrTokenizer(input);\n         assertSame(input, tok.getContent());\n-        \n+\n         tok = new StrTokenizer(input.toCharArray());\n         assertEquals(input, tok.getContent());\n+    }\n+\n+    public void testMatcher() {\n+        assertEquals(1, StrTokenizer.SPACE_MATCHER.isMatch(new char[]{' '}, 1, 0));\n+        assertEquals(0, StrTokenizer.SPACE_MATCHER.isMatch(new char[]{'\\n'}, 1, 0));\n+        assertEquals(0, StrTokenizer.SPACE_MATCHER.isMatch(new char[]{'\\u0001'}, 1, 0));\n+\n+        assertEquals(1, StrTokenizer.TRIM_MATCHER.isMatch(new char[]{' '}, 1, 0));\n+        assertEquals(1, StrTokenizer.TRIM_MATCHER.isMatch(new char[]{'\\n'}, 1, 0));\n+        assertEquals(1, StrTokenizer.TRIM_MATCHER.isMatch(new char[]{'\\u0001'}, 1, 0));\n+\n+        assertEquals(1, StrTokenizer.SPLIT_MATCHER.isMatch(new char[]{' '}, 1, 0));\n+        assertEquals(1, StrTokenizer.SPLIT_MATCHER.isMatch(new char[]{'\\n'}, 1, 0));\n+        assertEquals(0, StrTokenizer.SPLIT_MATCHER.isMatch(new char[]{'\\u0001'}, 1, 0));\n     }\n \n     public void testReset() {\n         assertEquals(\"f\", tok.next());\n         assertEquals(\"g\", tok.next());\n     }\n-    \n-    public void testMatcher() {\n-        assertEquals(1, StrTokenizer.SPACE_MATCHER.isMatch(new char[] {' '}, 1, 0));\n-        assertEquals(0, StrTokenizer.SPACE_MATCHER.isMatch(new char[] {'\\n'}, 1, 0));\n-        assertEquals(0, StrTokenizer.SPACE_MATCHER.isMatch(new char[] {'\\u0001'}, 1, 0));\n-        \n-        assertEquals(1, StrTokenizer.TRIM_MATCHER.isMatch(new char[] {' '}, 1, 0));\n-        assertEquals(1, StrTokenizer.TRIM_MATCHER.isMatch(new char[] {'\\n'}, 1, 0));\n-        assertEquals(1, StrTokenizer.TRIM_MATCHER.isMatch(new char[] {'\\u0001'}, 1, 0));\n-        \n-        assertEquals(1, StrTokenizer.SPLIT_MATCHER.isMatch(new char[] {' '}, 1, 0));\n-        assertEquals(1, StrTokenizer.SPLIT_MATCHER.isMatch(new char[] {'\\n'}, 1, 0));\n-        assertEquals(0, StrTokenizer.SPLIT_MATCHER.isMatch(new char[] {'\\u0001'}, 1, 0));\n-    }\n-    \n+\n+    public void testTSV() {\n+        this.testXSVAbc(StrTokenizer.getTSVInstance(TSV_SIMPLE_FIXTURE));\n+        this.testXSVAbc(StrTokenizer.getTSVInstance(TSV_SIMPLE_FIXTURE.toCharArray()));\n+    }\n+\n+    public void testTSVEmpty() {\n+        this.testEmpty(StrTokenizer.getCSVInstance());\n+        this.testEmpty(StrTokenizer.getCSVInstance(\"\"));\n+    }\n+\n+    void testXSVAbc(StrTokenizer tokenizer) {\n+        this.checkClone(tokenizer);\n+        assertEquals(-1, tokenizer.previousIndex());\n+        assertEquals(0, tokenizer.nextIndex());\n+        assertEquals(null, tokenizer.previousToken());\n+        assertEquals(\"A\", tokenizer.nextToken());\n+        assertEquals(1, tokenizer.nextIndex());\n+        assertEquals(\"b\", tokenizer.nextToken());\n+        assertEquals(2, tokenizer.nextIndex());\n+        assertEquals(\"c\", tokenizer.nextToken());\n+        assertEquals(3, tokenizer.nextIndex());\n+        assertEquals(null, tokenizer.nextToken());\n+        assertEquals(3, tokenizer.nextIndex());\n+        assertEquals(\"c\", tokenizer.previousToken());\n+        assertEquals(2, tokenizer.nextIndex());\n+        assertEquals(\"b\", tokenizer.previousToken());\n+        assertEquals(1, tokenizer.nextIndex());\n+        assertEquals(\"A\", tokenizer.previousToken());\n+        assertEquals(0, tokenizer.nextIndex());\n+        assertEquals(null, tokenizer.previousToken());\n+        assertEquals(0, tokenizer.nextIndex());\n+        assertEquals(-1, tokenizer.previousIndex());\n+        assertEquals(3, tokenizer.size());\n+    }\n+\n }", "timestamp": 1122858897, "metainfo": ""}