{"sha": "3f173de0a2383e9856b3640c4d331fefee668217", "log": "MATH-1008 Created package \"o.a.c.m.fitting.leastsquares\" to contain a modified version of the contents of (to-be-deprecated) \"o.a.c.m.optim.nonlinear.vector\", the main purpose being to provide a new \"fluent\" API (cf. \"withXxx\" methods).   Along the way, class \"LevenbergMarquardtOptimizer\" has been further cleaned     up (e.g. removing protected fields and deprecated methods and using local variables instead of instance fields). An additional constructor was necessary in \"BaseOptimizer\".   ", "commit": "\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/math3/fitting/leastsquares/AbstractLeastSquaresOptimizer.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import org.apache.commons.math3.exception.DimensionMismatchException;\n+import org.apache.commons.math3.exception.MathUnsupportedOperationException;\n+import org.apache.commons.math3.analysis.MultivariateVectorFunction;\n+import org.apache.commons.math3.analysis.MultivariateMatrixFunction;\n+import org.apache.commons.math3.linear.ArrayRealVector;\n+import org.apache.commons.math3.linear.RealMatrix;\n+import org.apache.commons.math3.linear.DiagonalMatrix;\n+import org.apache.commons.math3.linear.DecompositionSolver;\n+import org.apache.commons.math3.linear.MatrixUtils;\n+import org.apache.commons.math3.linear.QRDecomposition;\n+import org.apache.commons.math3.linear.EigenDecomposition;\n+import org.apache.commons.math3.optim.ConvergenceChecker;\n+import org.apache.commons.math3.optim.BaseOptimizer;\n+import org.apache.commons.math3.optim.PointVectorValuePair;\n+import org.apache.commons.math3.optim.OptimizationData;\n+import org.apache.commons.math3.util.FastMath;\n+\n+/**\n+ * Base class for implementing least-squares optimizers.\n+ * It provides methods for error estimation.\n+ *\n+ * @version $Id$\n+ * @since 3.3\n+ */\n+public abstract class AbstractLeastSquaresOptimizer\n+    extends BaseOptimizer<PointVectorValuePair> {\n+    /** Target values for the model function at optimum. */\n+    private final double[] target;\n+    /** Weight matrix. */\n+    private final RealMatrix weight;\n+    /** Model function. */\n+    private final MultivariateVectorFunction model;\n+    /** Jacobian of the model function. */\n+    private final MultivariateMatrixFunction jacobian;\n+    /** Square-root of the weight matrix. */\n+    private final RealMatrix weightSqrt;\n+    /** Initial guess. */\n+    private final double[] start;\n+\n+    /**\n+     * @param target Observations.\n+     * @param weight Weight of the observations.\n+     * For performance, no defensive copy is performed.\n+     * @param weightSqrt Square-root of the {@code weight} matrix.\n+     * If {@code null}, it will be computed; otherwise it is the caller's\n+     * responsibility that {@code weight} and {@code weightSqrt} are\n+     * consistent.\n+     * No defensive copy is performed.\n+     * @param model ModelFunction.\n+     * @param jacobian Jacobian of the model function.\n+     * @param checker Convergence checker.\n+     * @param start Initial guess.\n+     * @param maxEval Maximum number of evaluations of the model\n+     * function.\n+     * @param maxIter Maximum number of iterations.\n+     */\n+    protected AbstractLeastSquaresOptimizer(double[] target,\n+                                            RealMatrix weight,\n+                                            RealMatrix weightSqrt,\n+                                            MultivariateVectorFunction model,\n+                                            MultivariateMatrixFunction jacobian,\n+                                            ConvergenceChecker<PointVectorValuePair> checker,\n+                                            double[] start,\n+                                            int maxEval,\n+                                            int maxIter) {\n+        super(checker, maxEval, maxIter);\n+\n+        this.target = target;\n+        this.weight = weight;\n+        this.model = model;\n+        this.jacobian = jacobian;\n+        this.start = start;\n+\n+        this.weightSqrt = weightSqrt == null ?\n+            (weight == null ?\n+             null : squareRoot(weight)) : weightSqrt;\n+    }\n+\n+    /**\n+     * Gets the target values.\n+     *\n+     * @return the target values.\n+     */\n+    public double[] getTarget() {\n+        return target == null ? null : target.clone();\n+    }\n+\n+    /**\n+     * Gets the initial guess.\n+     *\n+     * @return the initial guess values.\n+     */\n+    public double[] getStart() {\n+        return start == null ? null : start.clone();\n+    }\n+\n+    /**\n+     * Gets the square-root of the weight matrix.\n+     *\n+     * @return the square-root of the weight matrix.\n+     */\n+    public RealMatrix getWeightSquareRoot() {\n+        return weightSqrt == null ? null : weightSqrt.copy();\n+    }\n+\n+    /**\n+     * Gets the model function.\n+     *\n+     * @return the model function.\n+     */\n+    public MultivariateVectorFunction getModel() {\n+        return model;\n+    }\n+\n+    /**\n+     * Gets the model function's Jacobian.\n+     *\n+     * @return the Jacobian.\n+     */\n+    public MultivariateMatrixFunction getJacobian() {\n+        return jacobian;\n+    }\n+\n+    /**\n+     * Get the covariance matrix of the optimized parameters.\n+     * <br/>\n+     * Note that this operation involves the inversion of the\n+     * <code>J<sup>T</sup>J</code> matrix, where {@code J} is the\n+     * Jacobian matrix.\n+     * The {@code threshold} parameter is a way for the caller to specify\n+     * that the result of this computation should be considered meaningless,\n+     * and thus trigger an exception.\n+     *\n+     * @param params Model parameters.\n+     * @param threshold Singularity threshold.\n+     * @return the covariance matrix.\n+     * @throws org.apache.commons.math3.linear.SingularMatrixException\n+     * if the covariance matrix cannot be computed (singular problem).\n+     */\n+    public double[][] computeCovariances(double[] params,\n+                                         double threshold) {\n+        // Set up the Jacobian.\n+        final RealMatrix j = computeWeightedJacobian(params);\n+\n+        // Compute transpose(J)J.\n+        final RealMatrix jTj = j.transpose().multiply(j);\n+\n+        // Compute the covariances matrix.\n+        final DecompositionSolver solver\n+            = new QRDecomposition(jTj, threshold).getSolver();\n+        return solver.getInverse().getData();\n+    }\n+\n+    /**\n+     * Computes an estimate of the standard deviation of the parameters. The\n+     * returned values are the square root of the diagonal coefficients of the\n+     * covariance matrix, {@code sd(a[i]) ~= sqrt(C[i][i])}, where {@code a[i]}\n+     * is the optimized value of the {@code i}-th parameter, and {@code C} is\n+     * the covariance matrix.\n+     *\n+     * @param params Model parameters.\n+     * @param covarianceSingularityThreshold Singularity threshold (see\n+     * {@link #computeCovariances(double[],double) computeCovariances}).\n+     * @return an estimate of the standard deviation of the optimized parameters\n+     * @throws org.apache.commons.math3.linear.SingularMatrixException\n+     * if the covariance matrix cannot be computed.\n+     */\n+    public double[] computeSigma(double[] params,\n+                                 double covarianceSingularityThreshold) {\n+        final int nC = params.length;\n+        final double[] sig = new double[nC];\n+        final double[][] cov = computeCovariances(params, covarianceSingularityThreshold);\n+        for (int i = 0; i < nC; ++i) {\n+            sig[i] = FastMath.sqrt(cov[i][i]);\n+        }\n+        return sig;\n+    }\n+\n+    /**\n+     * Gets the weight matrix of the observations.\n+     *\n+     * @return the weight matrix.\n+     */\n+    public RealMatrix getWeight() {\n+        return weight.copy();\n+    }\n+\n+    /**\n+     * Computes the normalized cost.\n+     * It is the square-root of the sum of squared of the residuals, divided\n+     * by the number of measurements.\n+     *\n+     * @param params Model function parameters.\n+     * @return the cost.\n+     */\n+    public double computeRMS(double[] params) {\n+        final double cost = computeCost(computeResiduals(getModel().value(params)));\n+        return FastMath.sqrt(cost * cost / target.length);\n+    }\n+\n+    /**\n+     * Calling this method will raise an exception.\n+     *\n+     * @param optData Obsolete.\n+     * @return nothing.\n+     * @throws MathUnsupportedOperationException if called.\n+     * @deprecated Do not use this method.\n+     */\n+    @Deprecated\n+    @Override\n+    public PointVectorValuePair optimize(OptimizationData... optData)\n+        throws MathUnsupportedOperationException {\n+        throw new MathUnsupportedOperationException();\n+    }\n+\n+    /**\n+     * Gets a reference to the corresponding field.\n+     * Altering it could jeopardize the consistency of this class.\n+     *\n+     * @return the reference.\n+     */\n+    protected double[] getTargetInternal() {\n+        return target;\n+    }\n+\n+    /**\n+     * Gets a reference to the corresponding field.\n+     * Altering it could jeopardize the consistency of this class.\n+     *\n+     * @return the reference.\n+     */\n+    protected RealMatrix getWeightInternal() {\n+        return weight;\n+    }\n+\n+    /**\n+     * Gets a reference to the corresponding field.\n+     * Altering it could jeopardize the consistency of this class.\n+     *\n+     * @return the reference.\n+     */\n+    protected RealMatrix getWeightSquareRootInternal() {\n+        return weightSqrt;\n+    }\n+\n+    /**\n+     * Computes the objective function value.\n+     * This method <em>must</em> be called by subclasses to enforce the\n+     * evaluation counter limit.\n+     *\n+     * @param params Point at which the objective function must be evaluated.\n+     * @return the objective function value at the specified point.\n+     * @throws org.apache.commons.math3.exception.TooManyEvaluationsException\n+     * if the maximal number of evaluations (of the model vector function) is\n+     * exceeded.\n+     */\n+    protected double[] computeObjectiveValue(double[] params) {\n+        super.incrementEvaluationCount();\n+        return model.value(params);\n+    }\n+\n+    /**\n+     * Computes the weighted Jacobian matrix.\n+     *\n+     * @param params Model parameters at which to compute the Jacobian.\n+     * @return the weighted Jacobian: W<sup>1/2</sup> J.\n+     * @throws DimensionMismatchException if the Jacobian dimension does not\n+     * match problem dimension.\n+     */\n+    protected RealMatrix computeWeightedJacobian(double[] params) {\n+        return weightSqrt.multiply(MatrixUtils.createRealMatrix(computeJacobian(params)));\n+    }\n+\n+    /**\n+     * Computes the Jacobian matrix.\n+     *\n+     * @param params Point at which the Jacobian must be evaluated.\n+     * @return the Jacobian at the specified point.\n+     */\n+    protected double[][] computeJacobian(final double[] params) {\n+        return jacobian.value(params);\n+    }\n+\n+    /**\n+     * Computes the cost.\n+     *\n+     * @param residuals Residuals.\n+     * @return the cost.\n+     * @see #computeResiduals(double[])\n+     */\n+    protected double computeCost(double[] residuals) {\n+        final ArrayRealVector r = new ArrayRealVector(residuals);\n+        return FastMath.sqrt(r.dotProduct(weight.operate(r)));\n+    }\n+\n+    /**\n+     * Computes the residuals.\n+     * The residual is the difference between the observed (target)\n+     * values and the model (objective function) value.\n+     * There is one residual for each element of the vector-valued\n+     * function.\n+     *\n+     * @param objectiveValue Value of the the objective function. This is\n+     * the value returned from a call to\n+     * {@link #computeObjectiveValue(double[]) computeObjectiveValue}\n+     * (whose array argument contains the model parameters).\n+     * @return the residuals.\n+     * @throws DimensionMismatchException if {@code params} has a wrong\n+     * length.\n+     */\n+    protected double[] computeResiduals(double[] objectiveValue) {\n+        if (objectiveValue.length != target.length) {\n+            throw new DimensionMismatchException(target.length,\n+                                                 objectiveValue.length);\n+        }\n+\n+        final double[] residuals = new double[target.length];\n+        for (int i = 0; i < target.length; i++) {\n+            residuals[i] = target[i] - objectiveValue[i];\n+        }\n+\n+        return residuals;\n+    }\n+\n+    /**\n+     * Computes the square-root of the weight matrix.\n+     *\n+     * @param m Symmetric, positive-definite (weight) matrix.\n+     * @return the square-root of the weight matrix.\n+     */\n+    private RealMatrix squareRoot(RealMatrix m) {\n+        if (m instanceof DiagonalMatrix) {\n+            final int dim = m.getRowDimension();\n+            final RealMatrix sqrtM = new DiagonalMatrix(dim);\n+            for (int i = 0; i < dim; i++) {\n+                sqrtM.setEntry(i, i, FastMath.sqrt(m.getEntry(i, i)));\n+            }\n+            return sqrtM;\n+        } else {\n+            final EigenDecomposition dec = new EigenDecomposition(m);\n+            return dec.getSquareRoot();\n+        }\n+    }\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/math3/fitting/leastsquares/GaussNewtonOptimizer.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import org.apache.commons.math3.analysis.MultivariateVectorFunction;\n+import org.apache.commons.math3.analysis.MultivariateMatrixFunction;\n+import org.apache.commons.math3.exception.DimensionMismatchException;\n+import org.apache.commons.math3.exception.ConvergenceException;\n+import org.apache.commons.math3.exception.NullArgumentException;\n+import org.apache.commons.math3.exception.MathInternalError;\n+import org.apache.commons.math3.exception.util.LocalizedFormats;\n+import org.apache.commons.math3.linear.ArrayRealVector;\n+import org.apache.commons.math3.linear.BlockRealMatrix;\n+import org.apache.commons.math3.linear.DecompositionSolver;\n+import org.apache.commons.math3.linear.LUDecomposition;\n+import org.apache.commons.math3.linear.QRDecomposition;\n+import org.apache.commons.math3.linear.RealMatrix;\n+import org.apache.commons.math3.linear.SingularMatrixException;\n+import org.apache.commons.math3.optim.ConvergenceChecker;\n+import org.apache.commons.math3.optim.PointVectorValuePair;\n+\n+/**\n+ * Gauss-Newton least-squares solver.\n+ *\n+ * <p>\n+ * This class solve a least-square problem by solving the normal equations\n+ * of the linearized problem at each iteration. Either LU decomposition or\n+ * QR decomposition can be used to solve the normal equations. LU decomposition\n+ * is faster but QR decomposition is more robust for difficult problems.\n+ * </p>\n+ *\n+ * @version $Id$\n+ * @since 3.3\n+ *\n+ */\n+public class GaussNewtonOptimizer extends AbstractLeastSquaresOptimizer\n+    implements WithTarget<GaussNewtonOptimizer>,\n+               WithWeight<GaussNewtonOptimizer>,\n+               WithModelAndJacobian<GaussNewtonOptimizer>,\n+               WithConvergenceChecker<GaussNewtonOptimizer>,\n+               WithStartPoint<GaussNewtonOptimizer>,\n+               WithMaxIterations<GaussNewtonOptimizer>,\n+               WithMaxEvaluations<GaussNewtonOptimizer> {\n+    /** Indicator for using LU decomposition. */\n+    private final boolean useLU;\n+\n+    /**\n+     * Constructor called by the various {@code withXxx} methods.\n+     *\n+     * @param target Observations.\n+     * @param weight Weight of the observations.\n+     * For performance, no defensive copy is performed.\n+     * @param weightSqrt Square-root of the {@code weight} matrix.\n+     * If {@code null}, it will be computed; otherwise it is the caller's\n+     * responsibility that {@code weight} and {@code weightSqrt} are\n+     * consistent.\n+     * No defensive copy is performed.\n+     * @param model ModelFunction.\n+     * @param jacobian Jacobian of the model function.\n+     * @param checker Convergence checker.\n+     * @param start Initial guess.\n+     * @param maxEval Maximum number of evaluations of the model\n+     * function.\n+     * @param maxIter Maximum number of iterations.\n+     * @param useLU Whether to use LU decomposition.\n+     */\n+    private GaussNewtonOptimizer(double[] target,\n+                                 RealMatrix weight,\n+                                 RealMatrix weightSqrt,\n+                                 MultivariateVectorFunction model,\n+                                 MultivariateMatrixFunction jacobian,\n+                                 ConvergenceChecker<PointVectorValuePair> checker,\n+                                 double[] start,\n+                                 int maxEval,\n+                                 int maxIter,\n+                                 boolean useLU) {\n+        super(target, weight, weightSqrt, model, jacobian, checker, start, maxEval, maxIter);\n+\n+        this.useLU = useLU;\n+    }\n+\n+    /**\n+     * Creates a bare-bones instance.\n+     * Several calls to {@code withXxx} methods are necessary to obtain\n+     * an object with all necessary fields set to sensible values.\n+     * <br/>\n+     * The default for the algorithm is to solve the normal equations\n+     * using LU decomposition.\n+     *\n+     * @return an instance of this class.\n+     */\n+    public static GaussNewtonOptimizer create() {\n+        return new GaussNewtonOptimizer(null, null, null, null, null, null, null,\n+                                        0, 0, true);\n+    }\n+\n+    /** {@inheritDoc} */\n+    public GaussNewtonOptimizer withTarget(double[] target) {\n+        return new GaussNewtonOptimizer(target,\n+                                        getWeightInternal(),\n+                                        getWeightSquareRootInternal(),\n+                                        getModel(),\n+                                        getJacobian(),\n+                                        getConvergenceChecker(),\n+                                        getStart(),\n+                                        getMaxEvaluations(),\n+                                        getMaxIterations(),\n+                                        useLU);\n+    }\n+\n+    /** {@inheritDoc} */\n+    public GaussNewtonOptimizer withWeight(RealMatrix weight) {\n+        return new GaussNewtonOptimizer(getTargetInternal(),\n+                                        weight,\n+                                        null,\n+                                        getModel(),\n+                                        getJacobian(),\n+                                        getConvergenceChecker(),\n+                                        getStart(),\n+                                        getMaxEvaluations(),\n+                                        getMaxIterations(),\n+                                        useLU);\n+    }\n+\n+    /** {@inheritDoc} */\n+    public GaussNewtonOptimizer withModelAndJacobian(MultivariateVectorFunction model,\n+                                                     MultivariateMatrixFunction jacobian) {\n+        return new GaussNewtonOptimizer(getTargetInternal(),\n+                                        getWeightInternal(),\n+                                        getWeightSquareRootInternal(),\n+                                        model,\n+                                        jacobian,\n+                                        getConvergenceChecker(),\n+                                        getStart(),\n+                                        getMaxEvaluations(),\n+                                        getMaxIterations(),\n+                                        useLU);\n+    }\n+\n+    /** {@inheritDoc} */\n+    public GaussNewtonOptimizer withConvergenceChecker(ConvergenceChecker<PointVectorValuePair> checker) {\n+        return new GaussNewtonOptimizer(getTarget(),\n+                                        getWeightInternal(),\n+                                        getWeightSquareRootInternal(),\n+                                        getModel(),\n+                                        getJacobian(),\n+                                        checker,\n+                                        getStart(),\n+                                        getMaxEvaluations(),\n+                                        getMaxIterations(),\n+                                        useLU);\n+    }\n+\n+    /** {@inheritDoc} */\n+    public GaussNewtonOptimizer withStartPoint(double[] start) {\n+        return new GaussNewtonOptimizer(getTarget(),\n+                                        getWeightInternal(),\n+                                        getWeightSquareRootInternal(),\n+                                        getModel(),\n+                                        getJacobian(),\n+                                        getConvergenceChecker(),\n+                                        start,\n+                                        getMaxEvaluations(),\n+                                        getMaxIterations(),\n+                                        useLU);\n+    }\n+\n+    /** {@inheritDoc} */\n+    public GaussNewtonOptimizer withMaxIterations(int maxIter) {\n+        return new GaussNewtonOptimizer(getTarget(),\n+                                        getWeightInternal(),\n+                                        getWeightSquareRootInternal(),\n+                                        getModel(),\n+                                        getJacobian(),\n+                                        getConvergenceChecker(),\n+                                        getStart(),\n+                                        getMaxEvaluations(),\n+                                        maxIter,\n+                                        useLU);\n+    }\n+\n+    /** {@inheritDoc} */\n+    public GaussNewtonOptimizer withMaxEvaluations(int maxEval) {\n+        return new GaussNewtonOptimizer(getTarget(),\n+                                        getWeightInternal(),\n+                                        getWeightSquareRootInternal(),\n+                                        getModel(),\n+                                        getJacobian(),\n+                                        getConvergenceChecker(),\n+                                        getStart(),\n+                                        maxEval,\n+                                        getMaxIterations(),\n+                                        useLU);\n+    }\n+\n+    /**\n+     * Creates a new instance.\n+     *\n+     * @param withLU Whether to use LU decomposition.\n+     * @return a new instance with all fields identical to this instance except\n+     * for the givens arguments.\n+     */\n+    public GaussNewtonOptimizer withLU(boolean withLU) {\n+        return new GaussNewtonOptimizer(getTarget(),\n+                                        getWeightInternal(),\n+                                        getWeightSquareRootInternal(),\n+                                        getModel(),\n+                                        getJacobian(),\n+                                        getConvergenceChecker(),\n+                                        getStart(),\n+                                        getMaxEvaluations(),\n+                                        getMaxIterations(),\n+                                        withLU);\n+    }\n+\n+    /** {@inheritDoc} */\n+    @Override\n+    public PointVectorValuePair doOptimize() {\n+        final ConvergenceChecker<PointVectorValuePair> checker\n+            = getConvergenceChecker();\n+\n+        // Computation will be useless without a checker (see \"for-loop\").\n+        if (checker == null) {\n+            throw new NullArgumentException();\n+        }\n+\n+        final double[] targetValues = getTarget();\n+        final int nR = targetValues.length; // Number of observed data.\n+\n+        final RealMatrix weightMatrix = getWeight();\n+        if (weightMatrix.getRowDimension() != nR) {\n+            throw new DimensionMismatchException(weightMatrix.getRowDimension(), nR);\n+        }\n+        if (weightMatrix.getColumnDimension() != nR) {\n+            throw new DimensionMismatchException(weightMatrix.getColumnDimension(), nR);\n+        }\n+\n+        // Diagonal of the weight matrix.\n+        final double[] residualsWeights = new double[nR];\n+        for (int i = 0; i < nR; i++) {\n+            residualsWeights[i] = weightMatrix.getEntry(i, i);\n+        }\n+\n+        final double[] currentPoint = getStart();\n+        final int nC = currentPoint.length;\n+\n+        // iterate until convergence is reached\n+        PointVectorValuePair current = null;\n+        for (boolean converged = false; !converged;) {\n+            incrementIterationCount();\n+\n+            // evaluate the objective function and its jacobian\n+            PointVectorValuePair previous = current;\n+            // Value of the objective function at \"currentPoint\".\n+            final double[] currentObjective = computeObjectiveValue(currentPoint);\n+            final double[] currentResiduals = computeResiduals(currentObjective);\n+            final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n+            current = new PointVectorValuePair(currentPoint, currentObjective);\n+\n+            // build the linear problem\n+            final double[]   b = new double[nC];\n+            final double[][] a = new double[nC][nC];\n+            for (int i = 0; i < nR; ++i) {\n+\n+                final double[] grad   = weightedJacobian.getRow(i);\n+                final double weight   = residualsWeights[i];\n+                final double residual = currentResiduals[i];\n+\n+                // compute the normal equation\n+                final double wr = weight * residual;\n+                for (int j = 0; j < nC; ++j) {\n+                    b[j] += wr * grad[j];\n+                }\n+\n+                // build the contribution matrix for measurement i\n+                for (int k = 0; k < nC; ++k) {\n+                    double[] ak = a[k];\n+                    double wgk = weight * grad[k];\n+                    for (int l = 0; l < nC; ++l) {\n+                        ak[l] += wgk * grad[l];\n+                    }\n+                }\n+            }\n+\n+            // Check convergence.\n+            if (previous != null) {\n+                converged = checker.converged(getIterations(), previous, current);\n+                if (converged) {\n+                    return current;\n+                }\n+            }\n+\n+            try {\n+                // solve the linearized least squares problem\n+                RealMatrix mA = new BlockRealMatrix(a);\n+                DecompositionSolver solver = useLU ?\n+                        new LUDecomposition(mA).getSolver() :\n+                        new QRDecomposition(mA).getSolver();\n+                final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n+                // update the estimated parameters\n+                for (int i = 0; i < nC; ++i) {\n+                    currentPoint[i] += dX[i];\n+                }\n+            } catch (SingularMatrixException e) {\n+                throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n+            }\n+        }\n+        // Must never happen.\n+        throw new MathInternalError();\n+    }\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/math3/fitting/leastsquares/LevenbergMarquardtOptimizer.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import java.util.Arrays;\n+import org.apache.commons.math3.analysis.MultivariateVectorFunction;\n+import org.apache.commons.math3.analysis.MultivariateMatrixFunction;\n+import org.apache.commons.math3.linear.RealMatrix;\n+import org.apache.commons.math3.exception.ConvergenceException;\n+import org.apache.commons.math3.exception.util.LocalizedFormats;\n+import org.apache.commons.math3.optim.ConvergenceChecker;\n+import org.apache.commons.math3.optim.PointVectorValuePair;\n+import org.apache.commons.math3.util.Precision;\n+import org.apache.commons.math3.util.FastMath;\n+\n+\n+/**\n+ * This class solves a least-squares problem using the Levenberg-Marquardt\n+ * algorithm.\n+ *\n+ * <p>This implementation <em>should</em> work even for over-determined systems\n+ * (i.e. systems having more point than equations). Over-determined systems\n+ * are solved by ignoring the point which have the smallest impact according\n+ * to their jacobian column norm. Only the rank of the matrix and some loop bounds\n+ * are changed to implement this.</p>\n+ *\n+ * <p>The resolution engine is a simple translation of the MINPACK <a\n+ * href=\"http://www.netlib.org/minpack/lmder.f\">lmder</a> routine with minor\n+ * changes. The changes include the over-determined resolution, the use of\n+ * inherited convergence checker and the Q.R. decomposition which has been\n+ * rewritten following the algorithm described in the\n+ * P. Lascaux and R. Theodor book <i>Analyse num&eacute;rique matricielle\n+ * appliqu&eacute;e &agrave; l'art de l'ing&eacute;nieur</i>, Masson 1986.</p>\n+ * <p>The authors of the original fortran version are:\n+ * <ul>\n+ * <li>Argonne National Laboratory. MINPACK project. March 1980</li>\n+ * <li>Burton S. Garbow</li>\n+ * <li>Kenneth E. Hillstrom</li>\n+ * <li>Jorge J. More</li>\n+ * </ul>\n+ * The redistribution policy for MINPACK is available <a\n+ * href=\"http://www.netlib.org/minpack/disclaimer\">here</a>, for convenience, it\n+ * is reproduced below.</p>\n+ *\n+ * <table border=\"0\" width=\"80%\" cellpadding=\"10\" align=\"center\" bgcolor=\"#E0E0E0\">\n+ * <tr><td>\n+ *    Minpack Copyright Notice (1999) University of Chicago.\n+ *    All rights reserved\n+ * </td></tr>\n+ * <tr><td>\n+ * Redistribution and use in source and binary forms, with or without\n+ * modification, are permitted provided that the following conditions\n+ * are met:\n+ * <ol>\n+ *  <li>Redistributions of source code must retain the above copyright\n+ *      notice, this list of conditions and the following disclaimer.</li>\n+ * <li>Redistributions in binary form must reproduce the above\n+ *     copyright notice, this list of conditions and the following\n+ *     disclaimer in the documentation and/or other materials provided\n+ *     with the distribution.</li>\n+ * <li>The end-user documentation included with the redistribution, if any,\n+ *     must include the following acknowledgment:\n+ *     <code>This product includes software developed by the University of\n+ *           Chicago, as Operator of Argonne National Laboratory.</code>\n+ *     Alternately, this acknowledgment may appear in the software itself,\n+ *     if and wherever such third-party acknowledgments normally appear.</li>\n+ * <li><strong>WARRANTY DISCLAIMER. THE SOFTWARE IS SUPPLIED \"AS IS\"\n+ *     WITHOUT WARRANTY OF ANY KIND. THE COPYRIGHT HOLDER, THE\n+ *     UNITED STATES, THE UNITED STATES DEPARTMENT OF ENERGY, AND\n+ *     THEIR EMPLOYEES: (1) DISCLAIM ANY WARRANTIES, EXPRESS OR\n+ *     IMPLIED, INCLUDING BUT NOT LIMITED TO ANY IMPLIED WARRANTIES\n+ *     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE\n+ *     OR NON-INFRINGEMENT, (2) DO NOT ASSUME ANY LEGAL LIABILITY\n+ *     OR RESPONSIBILITY FOR THE ACCURACY, COMPLETENESS, OR\n+ *     USEFULNESS OF THE SOFTWARE, (3) DO NOT REPRESENT THAT USE OF\n+ *     THE SOFTWARE WOULD NOT INFRINGE PRIVATELY OWNED RIGHTS, (4)\n+ *     DO NOT WARRANT THAT THE SOFTWARE WILL FUNCTION\n+ *     UNINTERRUPTED, THAT IT IS ERROR-FREE OR THAT ANY ERRORS WILL\n+ *     BE CORRECTED.</strong></li>\n+ * <li><strong>LIMITATION OF LIABILITY. IN NO EVENT WILL THE COPYRIGHT\n+ *     HOLDER, THE UNITED STATES, THE UNITED STATES DEPARTMENT OF\n+ *     ENERGY, OR THEIR EMPLOYEES: BE LIABLE FOR ANY INDIRECT,\n+ *     INCIDENTAL, CONSEQUENTIAL, SPECIAL OR PUNITIVE DAMAGES OF\n+ *     ANY KIND OR NATURE, INCLUDING BUT NOT LIMITED TO LOSS OF\n+ *     PROFITS OR LOSS OF DATA, FOR ANY REASON WHATSOEVER, WHETHER\n+ *     SUCH LIABILITY IS ASSERTED ON THE BASIS OF CONTRACT, TORT\n+ *     (INCLUDING NEGLIGENCE OR STRICT LIABILITY), OR OTHERWISE,\n+ *     EVEN IF ANY OF SAID PARTIES HAS BEEN WARNED OF THE\n+ *     POSSIBILITY OF SUCH LOSS OR DAMAGES.</strong></li>\n+ * <ol></td></tr>\n+ * </table>\n+ *\n+ * @version $Id$\n+ * @since 2.0\n+ */\n+public class LevenbergMarquardtOptimizer extends AbstractLeastSquaresOptimizer\n+    implements WithTarget<LevenbergMarquardtOptimizer>,\n+               WithWeight<LevenbergMarquardtOptimizer>,\n+               WithModelAndJacobian<LevenbergMarquardtOptimizer>,\n+               WithConvergenceChecker<LevenbergMarquardtOptimizer>,\n+               WithStartPoint<LevenbergMarquardtOptimizer>,\n+               WithMaxIterations<LevenbergMarquardtOptimizer>,\n+               WithMaxEvaluations<LevenbergMarquardtOptimizer> {\n+    /** Twice the \"epsilon machine\". */\n+    private static final double TWO_EPS = 2 * Precision.EPSILON;\n+    /** Positive input variable used in determining the initial step bound. */\n+    private final double initialStepBoundFactor;\n+    /** Desired relative error in the sum of squares. */\n+    private final double costRelativeTolerance;\n+    /**  Desired relative error in the approximate solution parameters. */\n+    private final double parRelativeTolerance;\n+    /** Desired max cosine on the orthogonality between the function vector\n+     * and the columns of the jacobian. */\n+    private final double orthoTolerance;\n+    /** Threshold for QR ranking. */\n+    private final double qrRankingThreshold;\n+    /** Levenberg-Marquardt parameter. */\n+    private double lmPar;\n+    /** Parameters evolution direction associated with lmPar. */\n+    private double[] lmDir;\n+\n+    /**\n+     * Constructor called by the various {@code withXxx} methods.\n+     *\n+     * @param target Observations.\n+     * @param weight Weight of the observations.\n+     * For performance, no defensive copy is performed.\n+     * @param weightSqrt Square-root of the {@code weight} matrix.\n+     * If {@code null}, it will be computed; otherwise it is the caller's\n+     * responsibility that {@code weight} and {@code weightSqrt} are\n+     * consistent.\n+     * No defensive copy is performed.\n+     * @param model ModelFunction.\n+     * @param jacobian Jacobian of the model function.\n+     * @param checker Convergence checker.\n+     * @param start Initial guess.\n+     * @param maxEval Maximum number of evaluations of the model\n+     * function.\n+     * @param maxIter Maximum number of iterations.\n+     * @param initialStepBoundFactor Positive input variable used in\n+     * determining the initial step bound. This bound is set to the\n+     * product of initialStepBoundFactor and the euclidean norm of\n+     * {@code diag * x} if non-zero, or else to {@code initialStepBoundFactor}\n+     * itself. In most cases factor should lie in the interval\n+     * {@code (0.1, 100.0)}. {@code 100} is a generally recommended value.\n+     * @param costRelativeTolerance Desired relative error in the sum of\n+     * squares.\n+     * @param parRelativeTolerance Desired relative error in the approximate\n+     * solution parameters.\n+     * @param orthoTolerance Desired max cosine on the orthogonality between\n+     * the function vector and the columns of the Jacobian.\n+     * @param threshold Desired threshold for QR ranking. If the squared norm\n+     * of a column vector is smaller or equal to this threshold during QR\n+     * decomposition, it is considered to be a zero vector and hence the rank\n+     * of the matrix is reduced.\n+     */\n+    private LevenbergMarquardtOptimizer(double[] target,\n+                                        RealMatrix weight,\n+                                        RealMatrix weightSqrt,\n+                                        MultivariateVectorFunction model,\n+                                        MultivariateMatrixFunction jacobian,\n+                                        ConvergenceChecker<PointVectorValuePair> checker,\n+                                        double[] start,\n+                                        int maxEval,\n+                                        int maxIter,\n+                                        double initialStepBoundFactor,\n+                                        double costRelativeTolerance,\n+                                        double parRelativeTolerance,\n+                                        double orthoTolerance,\n+                                        double threshold) {\n+        super(target, weight, weightSqrt, model, jacobian, checker, start, maxEval, maxIter);\n+\n+        this.initialStepBoundFactor = initialStepBoundFactor;\n+        this.costRelativeTolerance = costRelativeTolerance;\n+        this.parRelativeTolerance = parRelativeTolerance;\n+        this.orthoTolerance = orthoTolerance;\n+        this.qrRankingThreshold = threshold;\n+    }\n+\n+    /**\n+     * Creates a bare-bones instance.\n+     * Several calls to {@code withXxx} methods are necessary to obtain\n+     * an object with all necessary fields set to sensible values.\n+     * <br/>\n+     * The default values for the algorithm settings are:\n+     * <ul>\n+     *  <li>Initial step bound factor: 100</li>\n+     *  <li>Cost relative tolerance: 1e-10</li>\n+     *  <li>Parameters relative tolerance: 1e-10</li>\n+     *  <li>Orthogonality tolerance: 1e-10</li>\n+     *  <li>QR ranking threshold: {@link Precision#SAFE_MIN}</li>\n+     * </ul>\n+     *\n+     * @return an instance of this class.\n+     */\n+    public static LevenbergMarquardtOptimizer create() {\n+        return new LevenbergMarquardtOptimizer(null, null, null, null, null, null, null,\n+                                               0, 0, 100, 1e-10, 1e-10, 1e-10,\n+                                               Precision.SAFE_MIN);\n+    }\n+\n+    /** {@inheritDoc} */\n+    public LevenbergMarquardtOptimizer withTarget(double[] target) {\n+        return new LevenbergMarquardtOptimizer(target,\n+                                               getWeightInternal(),\n+                                               getWeightSquareRootInternal(),\n+                                               getModel(),\n+                                               getJacobian(),\n+                                               getConvergenceChecker(),\n+                                               getStart(),\n+                                               getMaxEvaluations(),\n+                                               getMaxIterations(),\n+                                               initialStepBoundFactor,\n+                                               costRelativeTolerance,\n+                                               parRelativeTolerance,\n+                                               orthoTolerance,\n+                                               qrRankingThreshold);\n+    }\n+\n+    /** {@inheritDoc} */\n+    public LevenbergMarquardtOptimizer withWeight(RealMatrix weight) {\n+        return new LevenbergMarquardtOptimizer(getTargetInternal(),\n+                                               weight,\n+                                               null,\n+                                               getModel(),\n+                                               getJacobian(),\n+                                               getConvergenceChecker(),\n+                                               getStart(),\n+                                               getMaxEvaluations(),\n+                                               getMaxIterations(),\n+                                               initialStepBoundFactor,\n+                                               costRelativeTolerance,\n+                                               parRelativeTolerance,\n+                                               orthoTolerance,\n+                                               qrRankingThreshold);\n+    }\n+\n+    /** {@inheritDoc} */\n+    public LevenbergMarquardtOptimizer withModelAndJacobian(MultivariateVectorFunction model,\n+                                                            MultivariateMatrixFunction jacobian) {\n+        return new LevenbergMarquardtOptimizer(getTargetInternal(),\n+                                               getWeightInternal(),\n+                                               getWeightSquareRootInternal(),\n+                                               model,\n+                                               jacobian,\n+                                               getConvergenceChecker(),\n+                                               getStart(),\n+                                               getMaxEvaluations(),\n+                                               getMaxIterations(),\n+                                               initialStepBoundFactor,\n+                                               costRelativeTolerance,\n+                                               parRelativeTolerance,\n+                                               orthoTolerance,\n+                                               qrRankingThreshold);\n+    }\n+\n+    /** {@inheritDoc} */\n+    public LevenbergMarquardtOptimizer withConvergenceChecker(ConvergenceChecker<PointVectorValuePair> checker) {\n+        return new LevenbergMarquardtOptimizer(getTarget(),\n+                                               getWeightInternal(),\n+                                               getWeightSquareRootInternal(),\n+                                               getModel(),\n+                                               getJacobian(),\n+                                               checker,\n+                                               getStart(),\n+                                               getMaxEvaluations(),\n+                                               getMaxIterations(),\n+                                               initialStepBoundFactor,\n+                                               costRelativeTolerance,\n+                                               parRelativeTolerance,\n+                                               orthoTolerance,\n+                                               qrRankingThreshold);\n+    }\n+\n+    /** {@inheritDoc} */\n+    public LevenbergMarquardtOptimizer withStartPoint(double[] start) {\n+        return new LevenbergMarquardtOptimizer(getTarget(),\n+                                               getWeightInternal(),\n+                                               getWeightSquareRootInternal(),\n+                                               getModel(),\n+                                               getJacobian(),\n+                                               getConvergenceChecker(),\n+                                               start,\n+                                               getMaxEvaluations(),\n+                                               getMaxIterations(),\n+                                               initialStepBoundFactor,\n+                                               costRelativeTolerance,\n+                                               parRelativeTolerance,\n+                                               orthoTolerance,\n+                                               qrRankingThreshold);\n+    }\n+\n+    /** {@inheritDoc} */\n+    public LevenbergMarquardtOptimizer withMaxIterations(int maxIter) {\n+        return new LevenbergMarquardtOptimizer(getTarget(),\n+                                               getWeightInternal(),\n+                                               getWeightSquareRootInternal(),\n+                                               getModel(),\n+                                               getJacobian(),\n+                                               getConvergenceChecker(),\n+                                               getStart(),\n+                                               getMaxEvaluations(),\n+                                               maxIter,\n+                                               initialStepBoundFactor,\n+                                               costRelativeTolerance,\n+                                               parRelativeTolerance,\n+                                               orthoTolerance,\n+                                               qrRankingThreshold);\n+    }\n+\n+    /** {@inheritDoc} */\n+    public LevenbergMarquardtOptimizer withMaxEvaluations(int maxEval) {\n+        return new LevenbergMarquardtOptimizer(getTarget(),\n+                                               getWeightInternal(),\n+                                               getWeightSquareRootInternal(),\n+                                               getModel(),\n+                                               getJacobian(),\n+                                               getConvergenceChecker(),\n+                                               getStart(),\n+                                               maxEval,\n+                                               getMaxIterations(),\n+                                               initialStepBoundFactor,\n+                                               costRelativeTolerance,\n+                                               parRelativeTolerance,\n+                                               orthoTolerance,\n+                                               qrRankingThreshold);\n+    }\n+\n+    /**\n+     * Creates a new instance.\n+     *\n+     * @param initStepBoundFactor Positive input variable used in\n+     * determining the initial step bound. This bound is set to the\n+     * product of initialStepBoundFactor and the euclidean norm of\n+     * {@code diag * x} if non-zero, or else to {@code initialStepBoundFactor}\n+     * itself. In most cases factor should lie in the interval\n+     * {@code (0.1, 100.0)}. {@code 100} is a generally recommended value.\n+     * @param costRelTol Desired relative error in the sum of squares.\n+     * @param parRelTol Desired relative error in the approximate solution\n+     * parameters.\n+     * @param orthoTol Desired max cosine on the orthogonality between\n+     * the function vector and the columns of the Jacobian.\n+     * @param threshold Desired threshold for QR ranking. If the squared norm\n+     * of a column vector is smaller or equal to this threshold during QR\n+     * decomposition, it is considered to be a zero vector and hence the rank\n+     * of the matrix is reduced.\n+     * @return a new instance with all fields identical to this instance except\n+     * for the givens arguments.\n+     */\n+    public LevenbergMarquardtOptimizer withTuningParameters(double initStepBoundFactor,\n+                                                            double costRelTol,\n+                                                            double parRelTol,\n+                                                            double orthoTol,\n+                                                            double threshold) {\n+        return new LevenbergMarquardtOptimizer(getTarget(),\n+                                               getWeightInternal(),\n+                                               getWeightSquareRootInternal(),\n+                                               getModel(),\n+                                               getJacobian(),\n+                                               getConvergenceChecker(),\n+                                               getStart(),\n+                                               getMaxEvaluations(),\n+                                               getMaxIterations(),\n+                                               initStepBoundFactor,\n+                                               costRelTol,\n+                                               parRelTol,\n+                                               orthoTol,\n+                                               threshold);\n+    }\n+\n+    /**\n+     * Gets the value of a tuning parameter.\n+     * @see #withTuningParameters(double,double,double,double,double)\n+     *\n+     * @return the parameter's value.\n+     */\n+    public double getInitialStepBoundFactor() {\n+        return initialStepBoundFactor;\n+    }\n+\n+    /**\n+     * Gets the value of a tuning parameter.\n+     * @see #withTuningParameters(double,double,double,double,double)\n+     *\n+     * @return the parameter's value.\n+     */\n+    public double getCostRelativeTolerance() {\n+        return costRelativeTolerance;\n+    }\n+\n+    /**\n+     * Gets the value of a tuning parameter.\n+     * @see #withTuningParameters(double,double,double,double,double)\n+     *\n+     * @return the parameter's value.\n+     */\n+    public double getParRelativeTolerance() {\n+        return parRelativeTolerance;\n+    }\n+\n+    /**\n+     * Gets the value of a tuning parameter.\n+     * @see #withTuningParameters(double,double,double,double,double)\n+     *\n+     * @return the parameter's value.\n+     */\n+    public double getOrthoTolerance() {\n+        return orthoTolerance;\n+    }\n+\n+    /**\n+     * Gets the value of a tuning parameter.\n+     * @see #withTuningParameters(double,double,double,double,double)\n+     *\n+     * @return the parameter's value.\n+     */\n+    public double getRankingThreshold() {\n+        return qrRankingThreshold;\n+    }\n+\n+    /** {@inheritDoc} */\n+    @Override\n+    protected PointVectorValuePair doOptimize() {\n+        final int nR = getTarget().length; // Number of observed data.\n+        final double[] currentPoint = getStart();\n+        final int nC = currentPoint.length; // Number of parameters.\n+\n+        // arrays shared with the other private methods\n+        final int solvedCols  = FastMath.min(nR, nC);\n+        lmDir = new double[nC];\n+        lmPar = 0;\n+\n+        // local point\n+        double   delta   = 0;\n+        double   xNorm   = 0;\n+        double[] diag    = new double[nC];\n+        double[] oldX    = new double[nC];\n+        double[] oldRes  = new double[nR];\n+        double[] oldObj  = new double[nR];\n+        double[] qtf     = new double[nR];\n+        double[] work1   = new double[nC];\n+        double[] work2   = new double[nC];\n+        double[] work3   = new double[nC];\n+\n+        final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n+\n+        // Evaluate the function at the starting point and calculate its norm.\n+        double[] currentObjective = computeObjectiveValue(currentPoint);\n+        double[] currentResiduals = computeResiduals(currentObjective);\n+        PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n+        double currentCost = computeCost(currentResiduals);\n+\n+        // Outer loop.\n+        boolean firstIteration = true;\n+        final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n+        while (true) {\n+            incrementIterationCount();\n+\n+            final PointVectorValuePair previous = current;\n+\n+            // QR decomposition of the jacobian matrix\n+            final InternalData internalData = qrDecomposition(computeWeightedJacobian(currentPoint),\n+                                                              solvedCols);\n+            final double[][] weightedJacobian = internalData.weightedJacobian;\n+            final int[] permutation = internalData.permutation;\n+            final double[] diagR = internalData.diagR;\n+            final double[] jacNorm = internalData.jacNorm;\n+\n+            double[] weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n+            for (int i = 0; i < nR; i++) {\n+                qtf[i] = weightedResidual[i];\n+            }\n+\n+            // compute Qt.res\n+            qTy(qtf, internalData);\n+\n+            // now we don't need Q anymore,\n+            // so let jacobian contain the R matrix with its diagonal elements\n+            for (int k = 0; k < solvedCols; ++k) {\n+                int pk = permutation[k];\n+                weightedJacobian[k][pk] = diagR[pk];\n+            }\n+\n+            if (firstIteration) {\n+                // scale the point according to the norms of the columns\n+                // of the initial jacobian\n+                xNorm = 0;\n+                for (int k = 0; k < nC; ++k) {\n+                    double dk = jacNorm[k];\n+                    if (dk == 0) {\n+                        dk = 1.0;\n+                    }\n+                    double xk = dk * currentPoint[k];\n+                    xNorm  += xk * xk;\n+                    diag[k] = dk;\n+                }\n+                xNorm = FastMath.sqrt(xNorm);\n+\n+                // initialize the step bound delta\n+                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n+            }\n+\n+            // check orthogonality between function vector and jacobian columns\n+            double maxCosine = 0;\n+            if (currentCost != 0) {\n+                for (int j = 0; j < solvedCols; ++j) {\n+                    int    pj = permutation[j];\n+                    double s  = jacNorm[pj];\n+                    if (s != 0) {\n+                        double sum = 0;\n+                        for (int i = 0; i <= j; ++i) {\n+                            sum += weightedJacobian[i][pj] * qtf[i];\n+                        }\n+                        maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n+                    }\n+                }\n+            }\n+            if (maxCosine <= orthoTolerance) {\n+                // Convergence has been reached.\n+                return current;\n+            }\n+\n+            // rescale if necessary\n+            for (int j = 0; j < nC; ++j) {\n+                diag[j] = FastMath.max(diag[j], jacNorm[j]);\n+            }\n+\n+            // Inner loop.\n+            for (double ratio = 0; ratio < 1.0e-4;) {\n+\n+                // save the state\n+                for (int j = 0; j < solvedCols; ++j) {\n+                    int pj = permutation[j];\n+                    oldX[pj] = currentPoint[pj];\n+                }\n+                final double previousCost = currentCost;\n+                double[] tmpVec = weightedResidual;\n+                weightedResidual = oldRes;\n+                oldRes    = tmpVec;\n+                tmpVec    = currentObjective;\n+                currentObjective = oldObj;\n+                oldObj    = tmpVec;\n+\n+                // determine the Levenberg-Marquardt parameter\n+                determineLMParameter(qtf, delta, diag,\n+                                     internalData, solvedCols,\n+                                     work1, work2, work3);\n+\n+                // compute the new point and the norm of the evolution direction\n+                double lmNorm = 0;\n+                for (int j = 0; j < solvedCols; ++j) {\n+                    int pj = permutation[j];\n+                    lmDir[pj] = -lmDir[pj];\n+                    currentPoint[pj] = oldX[pj] + lmDir[pj];\n+                    double s = diag[pj] * lmDir[pj];\n+                    lmNorm  += s * s;\n+                }\n+                lmNorm = FastMath.sqrt(lmNorm);\n+                // on the first iteration, adjust the initial step bound.\n+                if (firstIteration) {\n+                    delta = FastMath.min(delta, lmNorm);\n+                }\n+\n+                // Evaluate the function at x + p and calculate its norm.\n+                currentObjective = computeObjectiveValue(currentPoint);\n+                currentResiduals = computeResiduals(currentObjective);\n+                current = new PointVectorValuePair(currentPoint, currentObjective);\n+                currentCost = computeCost(currentResiduals);\n+\n+                // compute the scaled actual reduction\n+                double actRed = -1.0;\n+                if (0.1 * currentCost < previousCost) {\n+                    double r = currentCost / previousCost;\n+                    actRed = 1.0 - r * r;\n+                }\n+\n+                // compute the scaled predicted reduction\n+                // and the scaled directional derivative\n+                for (int j = 0; j < solvedCols; ++j) {\n+                    int pj = permutation[j];\n+                    double dirJ = lmDir[pj];\n+                    work1[j] = 0;\n+                    for (int i = 0; i <= j; ++i) {\n+                        work1[i] += weightedJacobian[i][pj] * dirJ;\n+                    }\n+                }\n+                double coeff1 = 0;\n+                for (int j = 0; j < solvedCols; ++j) {\n+                    coeff1 += work1[j] * work1[j];\n+                }\n+                double pc2 = previousCost * previousCost;\n+                coeff1 = coeff1 / pc2;\n+                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n+                double preRed = coeff1 + 2 * coeff2;\n+                double dirDer = -(coeff1 + coeff2);\n+\n+                // ratio of the actual to the predicted reduction\n+                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n+\n+                // update the step bound\n+                if (ratio <= 0.25) {\n+                    double tmp =\n+                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n+                        if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n+                            tmp = 0.1;\n+                        }\n+                        delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n+                        lmPar /= tmp;\n+                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n+                    delta = 2 * lmNorm;\n+                    lmPar *= 0.5;\n+                }\n+\n+                // test for successful iteration.\n+                if (ratio >= 1.0e-4) {\n+                    // successful iteration, update the norm\n+                    firstIteration = false;\n+                    xNorm = 0;\n+                    for (int k = 0; k < nC; ++k) {\n+                        double xK = diag[k] * currentPoint[k];\n+                        xNorm += xK * xK;\n+                    }\n+                    xNorm = FastMath.sqrt(xNorm);\n+\n+                    // tests for convergence.\n+                    if (checker != null && checker.converged(getIterations(), previous, current)) {\n+                        return current;\n+                    }\n+                } else {\n+                    // failed iteration, reset the previous values\n+                    currentCost = previousCost;\n+                    for (int j = 0; j < solvedCols; ++j) {\n+                        int pj = permutation[j];\n+                        currentPoint[pj] = oldX[pj];\n+                    }\n+                    tmpVec    = weightedResidual;\n+                    weightedResidual = oldRes;\n+                    oldRes    = tmpVec;\n+                    tmpVec    = currentObjective;\n+                    currentObjective = oldObj;\n+                    oldObj    = tmpVec;\n+                    // Reset \"current\" to previous values.\n+                    current = new PointVectorValuePair(currentPoint, currentObjective);\n+                }\n+\n+                // Default convergence criteria.\n+                if ((FastMath.abs(actRed) <= costRelativeTolerance &&\n+                     preRed <= costRelativeTolerance &&\n+                     ratio <= 2.0) ||\n+                    delta <= parRelativeTolerance * xNorm) {\n+                    return current;\n+                }\n+\n+                // tests for termination and stringent tolerances\n+                if (FastMath.abs(actRed) <= TWO_EPS &&\n+                    preRed <= TWO_EPS &&\n+                    ratio <= 2.0) {\n+                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n+                                                   costRelativeTolerance);\n+                } else if (delta <= TWO_EPS * xNorm) {\n+                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n+                                                   parRelativeTolerance);\n+                } else if (maxCosine <= TWO_EPS) {\n+                    throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n+                                                   orthoTolerance);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Holds internal data.\n+     * This structure was created so that all optimizer fields can be \"final\".\n+     * Code should be further refactored in order to not pass around arguments\n+     * that will modified in-place (cf. \"work\" arrays).\n+     */\n+    private static class InternalData {\n+        /** Weighted Jacobian. */\n+        final double[][] weightedJacobian;\n+        /** Columns permutation array. */\n+        final int[] permutation;\n+        /** Rank of the Jacobian matrix. */\n+        final int rank;\n+        /** Diagonal elements of the R matrix in the QR decomposition. */\n+        final double[] diagR;\n+        /** Norms of the columns of the jacobian matrix. */\n+        final double[] jacNorm;\n+        /** Coefficients of the Householder transforms vectors. */\n+        final double[] beta;\n+\n+        /**\n+         * @param weightedJacobian Weighted Jacobian.\n+         * @param permutation Columns permutation array.\n+         * @param rank Rank of the Jacobian matrix.\n+         * @param diagR Diagonal elements of the R matrix in the QR decomposition.\n+         * @param jacNorm Norms of the columns of the jacobian matrix.\n+         * @param beta Coefficients of the Householder transforms vectors.\n+         */\n+        InternalData(double[][] weightedJacobian,\n+                     int[] permutation,\n+                     int rank,\n+                     double[] diagR,\n+                     double[] jacNorm,\n+                     double[] beta) {\n+            this.weightedJacobian = weightedJacobian;\n+            this.permutation = permutation;\n+            this.rank = rank;\n+            this.diagR = diagR;\n+            this.jacNorm = jacNorm;\n+            this.beta = beta;\n+        }\n+    }\n+\n+    /**\n+     * Determines the Levenberg-Marquardt parameter.\n+     *\n+     * <p>This implementation is a translation in Java of the MINPACK\n+     * <a href=\"http://www.netlib.org/minpack/lmpar.f\">lmpar</a>\n+     * routine.</p>\n+     * <p>This method sets the lmPar and lmDir attributes.</p>\n+     * <p>The authors of the original fortran function are:</p>\n+     * <ul>\n+     *   <li>Argonne National Laboratory. MINPACK project. March 1980</li>\n+     *   <li>Burton  S. Garbow</li>\n+     *   <li>Kenneth E. Hillstrom</li>\n+     *   <li>Jorge   J. More</li>\n+     * </ul>\n+     * <p>Luc Maisonobe did the Java translation.</p>\n+     *\n+     * @param qy Array containing qTy.\n+     * @param delta Upper bound on the euclidean norm of diagR * lmDir.\n+     * @param diag Diagonal matrix.\n+     * @param internalData Data (modified in-place in this method).\n+     * @param solvedCols Number of solved point.\n+     * @param work1 work array\n+     * @param work2 work array\n+     * @param work3 work array\n+     */\n+    private void determineLMParameter(double[] qy, double delta, double[] diag,\n+                                      InternalData internalData, int solvedCols,\n+                                      double[] work1, double[] work2, double[] work3) {\n+        final double[][] weightedJacobian = internalData.weightedJacobian;\n+        final int[] permutation = internalData.permutation;\n+        final int rank = internalData.rank;\n+        final double[] diagR = internalData.diagR;\n+\n+        final int nC = weightedJacobian[0].length;\n+\n+        // compute and store in x the gauss-newton direction, if the\n+        // jacobian is rank-deficient, obtain a least squares solution\n+        for (int j = 0; j < rank; ++j) {\n+            lmDir[permutation[j]] = qy[j];\n+        }\n+        for (int j = rank; j < nC; ++j) {\n+            lmDir[permutation[j]] = 0;\n+        }\n+        for (int k = rank - 1; k >= 0; --k) {\n+            int pk = permutation[k];\n+            double ypk = lmDir[pk] / diagR[pk];\n+            for (int i = 0; i < k; ++i) {\n+                lmDir[permutation[i]] -= ypk * weightedJacobian[i][pk];\n+            }\n+            lmDir[pk] = ypk;\n+        }\n+\n+        // evaluate the function at the origin, and test\n+        // for acceptance of the Gauss-Newton direction\n+        double dxNorm = 0;\n+        for (int j = 0; j < solvedCols; ++j) {\n+            int pj = permutation[j];\n+            double s = diag[pj] * lmDir[pj];\n+            work1[pj] = s;\n+            dxNorm += s * s;\n+        }\n+        dxNorm = FastMath.sqrt(dxNorm);\n+        double fp = dxNorm - delta;\n+        if (fp <= 0.1 * delta) {\n+            lmPar = 0;\n+            return;\n+        }\n+\n+        // if the jacobian is not rank deficient, the Newton step provides\n+        // a lower bound, parl, for the zero of the function,\n+        // otherwise set this bound to zero\n+        double sum2;\n+        double parl = 0;\n+        if (rank == solvedCols) {\n+            for (int j = 0; j < solvedCols; ++j) {\n+                int pj = permutation[j];\n+                work1[pj] *= diag[pj] / dxNorm;\n+            }\n+            sum2 = 0;\n+            for (int j = 0; j < solvedCols; ++j) {\n+                int pj = permutation[j];\n+                double sum = 0;\n+                for (int i = 0; i < j; ++i) {\n+                    sum += weightedJacobian[i][pj] * work1[permutation[i]];\n+                }\n+                double s = (work1[pj] - sum) / diagR[pj];\n+                work1[pj] = s;\n+                sum2 += s * s;\n+            }\n+            parl = fp / (delta * sum2);\n+        }\n+\n+        // calculate an upper bound, paru, for the zero of the function\n+        sum2 = 0;\n+        for (int j = 0; j < solvedCols; ++j) {\n+            int pj = permutation[j];\n+            double sum = 0;\n+            for (int i = 0; i <= j; ++i) {\n+                sum += weightedJacobian[i][pj] * qy[i];\n+            }\n+            sum /= diag[pj];\n+            sum2 += sum * sum;\n+        }\n+        double gNorm = FastMath.sqrt(sum2);\n+        double paru = gNorm / delta;\n+        if (paru == 0) {\n+            paru = Precision.SAFE_MIN / FastMath.min(delta, 0.1);\n+        }\n+\n+        // if the input par lies outside of the interval (parl,paru),\n+        // set par to the closer endpoint\n+        lmPar = FastMath.min(paru, FastMath.max(lmPar, parl));\n+        if (lmPar == 0) {\n+            lmPar = gNorm / dxNorm;\n+        }\n+\n+        for (int countdown = 10; countdown >= 0; --countdown) {\n+\n+            // evaluate the function at the current value of lmPar\n+            if (lmPar == 0) {\n+                lmPar = FastMath.max(Precision.SAFE_MIN, 0.001 * paru);\n+            }\n+            double sPar = FastMath.sqrt(lmPar);\n+            for (int j = 0; j < solvedCols; ++j) {\n+                int pj = permutation[j];\n+                work1[pj] = sPar * diag[pj];\n+            }\n+            determineLMDirection(qy, work1, work2, internalData, solvedCols, work3);\n+\n+            dxNorm = 0;\n+            for (int j = 0; j < solvedCols; ++j) {\n+                int pj = permutation[j];\n+                double s = diag[pj] * lmDir[pj];\n+                work3[pj] = s;\n+                dxNorm += s * s;\n+            }\n+            dxNorm = FastMath.sqrt(dxNorm);\n+            double previousFP = fp;\n+            fp = dxNorm - delta;\n+\n+            // if the function is small enough, accept the current value\n+            // of lmPar, also test for the exceptional cases where parl is zero\n+            if (FastMath.abs(fp) <= 0.1 * delta ||\n+                (parl == 0 &&\n+                 fp <= previousFP &&\n+                 previousFP < 0)) {\n+                return;\n+            }\n+\n+            // compute the Newton correction\n+            for (int j = 0; j < solvedCols; ++j) {\n+                int pj = permutation[j];\n+                work1[pj] = work3[pj] * diag[pj] / dxNorm;\n+            }\n+            for (int j = 0; j < solvedCols; ++j) {\n+                int pj = permutation[j];\n+                work1[pj] /= work2[j];\n+                double tmp = work1[pj];\n+                for (int i = j + 1; i < solvedCols; ++i) {\n+                    work1[permutation[i]] -= weightedJacobian[i][pj] * tmp;\n+                }\n+            }\n+            sum2 = 0;\n+            for (int j = 0; j < solvedCols; ++j) {\n+                double s = work1[permutation[j]];\n+                sum2 += s * s;\n+            }\n+            double correction = fp / (delta * sum2);\n+\n+            // depending on the sign of the function, update parl or paru.\n+            if (fp > 0) {\n+                parl = FastMath.max(parl, lmPar);\n+            } else if (fp < 0) {\n+                paru = FastMath.min(paru, lmPar);\n+            }\n+\n+            // compute an improved estimate for lmPar\n+            lmPar = FastMath.max(parl, lmPar + correction);\n+        }\n+\n+        return;\n+    }\n+\n+    /**\n+     * Solve a*x = b and d*x = 0 in the least squares sense.\n+     * <p>This implementation is a translation in Java of the MINPACK\n+     * <a href=\"http://www.netlib.org/minpack/qrsolv.f\">qrsolv</a>\n+     * routine.</p>\n+     * <p>This method sets the lmDir and lmDiag attributes.</p>\n+     * <p>The authors of the original fortran function are:</p>\n+     * <ul>\n+     *   <li>Argonne National Laboratory. MINPACK project. March 1980</li>\n+     *   <li>Burton  S. Garbow</li>\n+     *   <li>Kenneth E. Hillstrom</li>\n+     *   <li>Jorge   J. More</li>\n+     * </ul>\n+     * <p>Luc Maisonobe did the Java translation.</p>\n+     *\n+     * @param qy array containing qTy\n+     * @param diag diagonal matrix\n+     * @param lmDiag diagonal elements associated with lmDir\n+     * @param internalData Data (modified in-place in this method).\n+     * @param solvedCols Number of sloved point.\n+     * @param work work array\n+     */\n+    private void determineLMDirection(double[] qy, double[] diag,\n+                                      double[] lmDiag,\n+                                      InternalData internalData,\n+                                      int solvedCols,\n+                                      double[] work) {\n+        final int[] permutation = internalData.permutation;\n+        final double[][] weightedJacobian = internalData.weightedJacobian;\n+        final double[] diagR = internalData.diagR;\n+\n+        // copy R and Qty to preserve input and initialize s\n+        //  in particular, save the diagonal elements of R in lmDir\n+        for (int j = 0; j < solvedCols; ++j) {\n+            int pj = permutation[j];\n+            for (int i = j + 1; i < solvedCols; ++i) {\n+                weightedJacobian[i][pj] = weightedJacobian[j][permutation[i]];\n+            }\n+            lmDir[j] = diagR[pj];\n+            work[j]  = qy[j];\n+        }\n+\n+        // eliminate the diagonal matrix d using a Givens rotation\n+        for (int j = 0; j < solvedCols; ++j) {\n+\n+            // prepare the row of d to be eliminated, locating the\n+            // diagonal element using p from the Q.R. factorization\n+            int pj = permutation[j];\n+            double dpj = diag[pj];\n+            if (dpj != 0) {\n+                Arrays.fill(lmDiag, j + 1, lmDiag.length, 0);\n+            }\n+            lmDiag[j] = dpj;\n+\n+            //  the transformations to eliminate the row of d\n+            // modify only a single element of Qty\n+            // beyond the first n, which is initially zero.\n+            double qtbpj = 0;\n+            for (int k = j; k < solvedCols; ++k) {\n+                int pk = permutation[k];\n+\n+                // determine a Givens rotation which eliminates the\n+                // appropriate element in the current row of d\n+                if (lmDiag[k] != 0) {\n+\n+                    final double sin;\n+                    final double cos;\n+                    double rkk = weightedJacobian[k][pk];\n+                    if (FastMath.abs(rkk) < FastMath.abs(lmDiag[k])) {\n+                        final double cotan = rkk / lmDiag[k];\n+                        sin   = 1.0 / FastMath.sqrt(1.0 + cotan * cotan);\n+                        cos   = sin * cotan;\n+                    } else {\n+                        final double tan = lmDiag[k] / rkk;\n+                        cos = 1.0 / FastMath.sqrt(1.0 + tan * tan);\n+                        sin = cos * tan;\n+                    }\n+\n+                    // compute the modified diagonal element of R and\n+                    // the modified element of (Qty,0)\n+                    weightedJacobian[k][pk] = cos * rkk + sin * lmDiag[k];\n+                    final double temp = cos * work[k] + sin * qtbpj;\n+                    qtbpj = -sin * work[k] + cos * qtbpj;\n+                    work[k] = temp;\n+\n+                    // accumulate the tranformation in the row of s\n+                    for (int i = k + 1; i < solvedCols; ++i) {\n+                        double rik = weightedJacobian[i][pk];\n+                        final double temp2 = cos * rik + sin * lmDiag[i];\n+                        lmDiag[i] = -sin * rik + cos * lmDiag[i];\n+                        weightedJacobian[i][pk] = temp2;\n+                    }\n+                }\n+            }\n+\n+            // store the diagonal element of s and restore\n+            // the corresponding diagonal element of R\n+            lmDiag[j] = weightedJacobian[j][permutation[j]];\n+            weightedJacobian[j][permutation[j]] = lmDir[j];\n+        }\n+\n+        // solve the triangular system for z, if the system is\n+        // singular, then obtain a least squares solution\n+        int nSing = solvedCols;\n+        for (int j = 0; j < solvedCols; ++j) {\n+            if ((lmDiag[j] == 0) && (nSing == solvedCols)) {\n+                nSing = j;\n+            }\n+            if (nSing < solvedCols) {\n+                work[j] = 0;\n+            }\n+        }\n+        if (nSing > 0) {\n+            for (int j = nSing - 1; j >= 0; --j) {\n+                int pj = permutation[j];\n+                double sum = 0;\n+                for (int i = j + 1; i < nSing; ++i) {\n+                    sum += weightedJacobian[i][pj] * work[i];\n+                }\n+                work[j] = (work[j] - sum) / lmDiag[j];\n+            }\n+        }\n+\n+        // permute the components of z back to components of lmDir\n+        for (int j = 0; j < lmDir.length; ++j) {\n+            lmDir[permutation[j]] = work[j];\n+        }\n+    }\n+\n+    /**\n+     * Decompose a matrix A as A.P = Q.R using Householder transforms.\n+     * <p>As suggested in the P. Lascaux and R. Theodor book\n+     * <i>Analyse num&eacute;rique matricielle appliqu&eacute;e &agrave;\n+     * l'art de l'ing&eacute;nieur</i> (Masson, 1986), instead of representing\n+     * the Householder transforms with u<sub>k</sub> unit vectors such that:\n+     * <pre>\n+     * H<sub>k</sub> = I - 2u<sub>k</sub>.u<sub>k</sub><sup>t</sup>\n+     * </pre>\n+     * we use <sub>k</sub> non-unit vectors such that:\n+     * <pre>\n+     * H<sub>k</sub> = I - beta<sub>k</sub>v<sub>k</sub>.v<sub>k</sub><sup>t</sup>\n+     * </pre>\n+     * where v<sub>k</sub> = a<sub>k</sub> - alpha<sub>k</sub> e<sub>k</sub>.\n+     * The beta<sub>k</sub> coefficients are provided upon exit as recomputing\n+     * them from the v<sub>k</sub> vectors would be costly.</p>\n+     * <p>This decomposition handles rank deficient cases since the tranformations\n+     * are performed in non-increasing columns norms order thanks to columns\n+     * pivoting. The diagonal elements of the R matrix are therefore also in\n+     * non-increasing absolute values order.</p>\n+     *\n+     * @param jacobian Weighted Jacobian matrix at the current point.\n+     * @param solvedCols Number of solved point.\n+     * @return data used in other methods of this class.\n+     * @throws ConvergenceException if the decomposition cannot be performed.\n+     */\n+    private InternalData qrDecomposition(RealMatrix jacobian,\n+                                         int solvedCols) throws ConvergenceException {\n+        // Code in this class assumes that the weighted Jacobian is -(W^(1/2) J),\n+        // hence the multiplication by -1.\n+        final double[][] weightedJacobian = jacobian.scalarMultiply(-1).getData();\n+\n+        final int nR = weightedJacobian.length;\n+        final int nC = weightedJacobian[0].length;\n+\n+        final int[] permutation = new int[nC];\n+        final double[] diagR = new double[nC];\n+        final double[] jacNorm = new double[nC];\n+        final double[] beta = new double[nC];\n+\n+        // initializations\n+        for (int k = 0; k < nC; ++k) {\n+            permutation[k] = k;\n+            double norm2 = 0;\n+            for (int i = 0; i < nR; ++i) {\n+                double akk = weightedJacobian[i][k];\n+                norm2 += akk * akk;\n+            }\n+            jacNorm[k] = FastMath.sqrt(norm2);\n+        }\n+\n+        // transform the matrix column after column\n+        for (int k = 0; k < nC; ++k) {\n+\n+            // select the column with the greatest norm on active components\n+            int nextColumn = -1;\n+            double ak2 = Double.NEGATIVE_INFINITY;\n+            for (int i = k; i < nC; ++i) {\n+                double norm2 = 0;\n+                for (int j = k; j < nR; ++j) {\n+                    double aki = weightedJacobian[j][permutation[i]];\n+                    norm2 += aki * aki;\n+                }\n+                if (Double.isInfinite(norm2) || Double.isNaN(norm2)) {\n+                    throw new ConvergenceException(LocalizedFormats.UNABLE_TO_PERFORM_QR_DECOMPOSITION_ON_JACOBIAN,\n+                                                   nR, nC);\n+                }\n+                if (norm2 > ak2) {\n+                    nextColumn = i;\n+                    ak2        = norm2;\n+                }\n+            }\n+            if (ak2 <= qrRankingThreshold) {\n+                return new InternalData(weightedJacobian, permutation, k, diagR, jacNorm, beta);\n+            }\n+            int pk = permutation[nextColumn];\n+            permutation[nextColumn] = permutation[k];\n+            permutation[k] = pk;\n+\n+            // choose alpha such that Hk.u = alpha ek\n+            double akk = weightedJacobian[k][pk];\n+            double alpha = (akk > 0) ? -FastMath.sqrt(ak2) : FastMath.sqrt(ak2);\n+            double betak = 1.0 / (ak2 - akk * alpha);\n+            beta[pk] = betak;\n+\n+            // transform the current column\n+            diagR[pk] = alpha;\n+            weightedJacobian[k][pk] -= alpha;\n+\n+            // transform the remaining columns\n+            for (int dk = nC - 1 - k; dk > 0; --dk) {\n+                double gamma = 0;\n+                for (int j = k; j < nR; ++j) {\n+                    gamma += weightedJacobian[j][pk] * weightedJacobian[j][permutation[k + dk]];\n+                }\n+                gamma *= betak;\n+                for (int j = k; j < nR; ++j) {\n+                    weightedJacobian[j][permutation[k + dk]] -= gamma * weightedJacobian[j][pk];\n+                }\n+            }\n+        }\n+\n+        return new InternalData(weightedJacobian, permutation, solvedCols, diagR, jacNorm, beta);\n+    }\n+\n+    /**\n+     * Compute the product Qt.y for some Q.R. decomposition.\n+     *\n+     * @param y vector to multiply (will be overwritten with the result)\n+     * @param internalData Data.\n+     */\n+    private void qTy(double[] y,\n+                     InternalData internalData) {\n+        final double[][] weightedJacobian = internalData.weightedJacobian;\n+        final int[] permutation = internalData.permutation;\n+        final double[] beta = internalData.beta;\n+\n+        final int nR = weightedJacobian.length;\n+        final int nC = weightedJacobian[0].length;\n+\n+        for (int k = 0; k < nC; ++k) {\n+            int pk = permutation[k];\n+            double gamma = 0;\n+            for (int i = k; i < nR; ++i) {\n+                gamma += weightedJacobian[i][pk] * y[i];\n+            }\n+            gamma *= beta[pk];\n+            for (int i = k; i < nR; ++i) {\n+                y[i] -= gamma * weightedJacobian[i][pk];\n+            }\n+        }\n+    }\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/math3/fitting/leastsquares/WithConvergenceChecker.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import org.apache.commons.math3.optim.ConvergenceChecker;\n+import org.apache.commons.math3.optim.PointVectorValuePair;\n+\n+/**\n+ * Interface for \"fluent-API\" that advertizes a capability of the optimizer.\n+ *\n+ * @param <T> Concrete optimizer implementation.\n+ *\n+ * @version $Id$\n+ * @since 3.3\n+ */\n+public interface WithConvergenceChecker<T> {\n+    /**\n+     * Creates a new instance with the specified parameter.\n+     *\n+     * @param checker Convergence checker.\n+     * @return a new optimizer instance with all fields identical to this\n+     * instance except for the given argument.\n+     */\n+    T withConvergenceChecker(ConvergenceChecker<PointVectorValuePair> checker);\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/math3/fitting/leastsquares/WithMaxEvaluations.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+/**\n+ * Interface for \"fluent-API\" that advertizes a capability of the optimizer.\n+ *\n+ * @param <T> Concrete optimizer implementation.\n+ *\n+ * @version $Id$\n+ * @since 3.3\n+ */\n+public interface WithMaxEvaluations<T> {\n+    /**\n+     * Creates a new instance with the specified parameter.\n+     *\n+     * @param maxEval Maximum number of evaluations of the model function.\n+     * @return a new optimizer instance with all fields identical to this\n+     * instance except for the given argument.\n+     */\n+    T withMaxEvaluations(int maxEval);\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/math3/fitting/leastsquares/WithMaxIterations.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+/**\n+ * Interface for \"fluent-API\" that advertizes a capability of the optimizer.\n+ *\n+ * @param <T> Concrete optimizer implementation.\n+ *\n+ * @version $Id$\n+ * @since 3.3\n+ */\n+public interface WithMaxIterations<T> {\n+    /**\n+     * Creates a new instance with the specified parameter.\n+     *\n+     * @param maxIter Maximum number of iterations.\n+     * @return a new optimizer instance with all fields identical to this\n+     * instance except for the given argument.\n+     */\n+    T withMaxIterations(int maxIter);\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/math3/fitting/leastsquares/WithModelAndJacobian.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import org.apache.commons.math3.analysis.MultivariateVectorFunction;\n+import org.apache.commons.math3.analysis.MultivariateMatrixFunction;\n+\n+/**\n+ * Interface for \"fluent-API\" that advertizes a capability of the optimizer.\n+ *\n+ * @param <T> Concrete optimizer implementation.\n+ *\n+ * @version $Id$\n+ * @since 3.3\n+ */\n+public interface WithModelAndJacobian<T> {\n+    /**\n+     * Creates a new instance with the specified parameters.\n+     *\n+     * @param model ModelFunction.\n+     * @param jacobian Jacobian of the model function.\n+     * @return a new optimizer instance with all fields identical to this\n+     * instance except for the given arguments.\n+     */\n+    T withModelAndJacobian(MultivariateVectorFunction model,\n+                           MultivariateMatrixFunction jacobian);\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/math3/fitting/leastsquares/WithStartPoint.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+/**\n+ * Interface for \"fluent-API\" that advertizes a capability of the optimizer.\n+ *\n+ * @param <T> Concrete optimizer implementation.\n+ *\n+ * @version $Id$\n+ * @since 3.3\n+ */\n+public interface WithStartPoint<T> {\n+    /**\n+     * Creates a new instance with the specified parameter.\n+     *\n+     * @param start Initial guess for the parameters of the model function.\n+     * @return a new optimizer instance with all fields identical to this\n+     * instance except for the given argument.\n+     */\n+    T withStartPoint(double[] start);\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/math3/fitting/leastsquares/WithTarget.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+/**\n+ * Interface for \"fluent-API\" that advertizes a capability of the optimizer.\n+ *\n+ * @param <T> Concrete optimizer implementation.\n+ *\n+ * @version $Id$\n+ * @since 3.3\n+ */\n+public interface WithTarget<T> {\n+    /**\n+     * Creates a new instance with the specified parameter.\n+     *\n+     * @param target Objective points of the model function.\n+     * @return a new optimizer instance with all fields identical to this\n+     * instance except for the given argument.\n+     */\n+    T withTarget(double[] target);\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/math3/fitting/leastsquares/WithWeight.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import org.apache.commons.math3.linear.RealMatrix;\n+\n+/**\n+ * Interface for \"fluent-API\" that advertizes a capability of the optimizer.\n+ *\n+ * @param <T> Concrete optimizer implementation.\n+ *\n+ * @version $Id$\n+ * @since 3.3\n+ */\n+public interface WithWeight<T> {\n+    /**\n+     * Creates a new instance with the specified parameter.\n+     *\n+     * @param weight Weight matrix of the observations.\n+     * @return a new optimizer instance with all fields identical to this\n+     * instance except for the given argument.\n+     */\n+    T withWeight(RealMatrix weight);\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/math3/fitting/leastsquares/package-info.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+/**\n+ * This package provides algorithms that minimize the residuals\n+ * between observations and model values.\n+ * The {@link org.apache.commons.math3.fitting.leastsquares.AbstractLeastSquaresOptimizer\n+ * non-linear least-squares optimizers} minimize the distance (called <em>cost</em> or\n+ * <em>&chi;<sup>2</sup></em>) between model and observations.\n+ *\n+ * <br/>\n+ * Algorithms in this category need access to a <em>model function</em>\n+ * (represented by a {@link org.apache.commons.math3.analysis.MultivariateVectorFunction\n+ * MultivariateVectorFunction}).\n+ * Such a model predicts a set of values which the algorithm tries to match with a set\n+ * of given set of {@link WithTarget observed values}.\n+ * <br/>\n+ * The algorithms implemented in this package also require that the user specifies the\n+ * Jacobian matrix of the model (represented by a\n+ * {@link org.apache.commons.math3.analysis.MultivariateMatrixFunction\n+ * MultivariateMatrixFunction}).\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n--- a/src/main/java/org/apache/commons/math3/optim/BaseOptimizer.java\n+++ b/src/main/java/org/apache/commons/math3/optim/BaseOptimizer.java\n      * @param checker Convergence checker.\n      */\n     protected BaseOptimizer(ConvergenceChecker<PAIR> checker) {\n+        this(checker, 0, Integer.MAX_VALUE);\n+    }\n+\n+    /**\n+     * @param checker Convergence checker.\n+     * @param maxEval Maximum number of objective function evaluations.\n+     * @param maxIter Maximum number of algorithm iterations.\n+     */\n+    protected BaseOptimizer(ConvergenceChecker<PAIR> checker,\n+                            int maxEval,\n+                            int maxIter) {\n         this.checker = checker;\n \n-        evaluations = new Incrementor(0, new MaxEvalCallback());\n-        iterations = new Incrementor(Integer.MAX_VALUE, new MaxIterCallback());\n+        evaluations = new Incrementor(maxEval, new MaxEvalCallback());\n+        iterations = new Incrementor(maxIter, new MaxIterCallback());\n     }\n \n     /**\n     }\n \n     /**\n+     * Performs the optimization.\n+     *\n+     * @return a point/value pair that satifies the convergence criteria.\n+     * @throws TooManyEvaluationsException if the maximal number of\n+     * evaluations is exceeded.\n+     * @throws TooManyIterationsException if the maximal number of\n+     * iterations is exceeded.\n+     */\n+    public PAIR optimize()\n+        throws TooManyEvaluationsException,\n+               TooManyIterationsException {\n+        // Reset counters.\n+        evaluations.resetCount();\n+        iterations.resetCount();\n+        // Perform optimization.\n+        return doOptimize();\n+    }\n+\n+    /**\n      * Performs the bulk of the optimization algorithm.\n      *\n      * @return the point/value pair giving the optimal value of the\n--- /dev/null\n+++ b/src/test/java/org/apache/commons/math3/fitting/leastsquares/AbstractLeastSquaresOptimizerAbstractTest.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import org.apache.commons.math3.analysis.MultivariateVectorFunction;\n+import org.apache.commons.math3.analysis.MultivariateMatrixFunction;\n+import org.apache.commons.math3.exception.ConvergenceException;\n+import org.apache.commons.math3.exception.DimensionMismatchException;\n+import org.apache.commons.math3.exception.NumberIsTooSmallException;\n+import org.apache.commons.math3.geometry.euclidean.twod.Vector2D;\n+import org.apache.commons.math3.linear.BlockRealMatrix;\n+import org.apache.commons.math3.linear.RealMatrix;\n+import org.apache.commons.math3.linear.DiagonalMatrix;\n+import org.apache.commons.math3.optim.PointVectorValuePair;\n+import org.apache.commons.math3.util.FastMath;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+/**\n+ * Some of the unit tests are re-implementations of the MINPACK\n+ * <a href=\"http://www.netlib.org/minpack/ex/file17\">file17</a> and\n+ * <a href=\"http://www.netlib.org/minpack/ex/file22\">file22</a> test files.\n+ * The redistribution policy for MINPACK is available\n+ * <a href=\"http://www.netlib.org/minpack/disclaimer\">here</a>.\n+ *\n+ * <T> Concrete implementation of an optimizer.\n+ *\n+ * @version $Id$\n+ */\n+public abstract class AbstractLeastSquaresOptimizerAbstractTest<T extends AbstractLeastSquaresOptimizer &\n+                                                                          WithTarget<T> &\n+                                                                          WithWeight<T> &\n+                                                                          WithModelAndJacobian<T> &\n+                                                                          WithConvergenceChecker<T> &\n+                                                                          WithStartPoint<T> &\n+                                                                          WithMaxIterations<T> &\n+                                                                          WithMaxEvaluations<T>> {\n+    /**\n+     * @return a concrete optimizer.\n+     */\n+    public abstract T createOptimizer();\n+\n+    /**\n+     * @return the default number of allowed iterations (which will be\n+     * used when not specified otherwise).\n+     */\n+    public abstract int getMaxIterations();\n+\n+    @Test\n+    public void testGetIterations() {\n+        T optim = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withTarget(new double[] { 1 })\n+            .withWeight(new DiagonalMatrix(new double[] { 1 }))\n+            .withStartPoint(new double[] { 3 })\n+            .withModelAndJacobian(new MultivariateVectorFunction() {\n+                    public double[] value(double[] point) {\n+                        return new double[] {\n+                            FastMath.pow(point[0], 4)\n+                        };\n+                    }},\n+                new MultivariateMatrixFunction() {\n+                    public double[][] value(double[] point) {\n+                        return new double[][] {\n+                            { 0.25 * FastMath.pow(point[0], 3) }\n+                        };\n+                    }\n+                });\n+\n+        optim.optimize();\n+        Assert.assertTrue(optim.getIterations() > 0);\n+    }\n+\n+    @Test\n+    public void testTrivial() {\n+        LinearProblem problem\n+            = new LinearProblem(new double[][] { { 2 } },\n+                                new double[] { 3 });\n+        T optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(problem.getTarget())\n+            .withWeight(new DiagonalMatrix(new double[] { 1 }))\n+            .withStartPoint(new double[] { 0 });\n+\n+        PointVectorValuePair optimum = optimizer.optimize();\n+\n+        Assert.assertEquals(0, optimizer.computeRMS(optimum.getPoint()), 1e-10);\n+        Assert.assertEquals(1.5, optimum.getPoint()[0], 1e-10);\n+        Assert.assertEquals(3.0, optimum.getValue()[0], 1e-10);\n+    }\n+\n+    @Test\n+    public void testQRColumnsPermutation() {\n+        LinearProblem problem\n+            = new LinearProblem(new double[][] { { 1, -1 }, { 0, 2 }, { 1, -2 } },\n+                                new double[] { 4, 6, 1 });\n+\n+        T optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(problem.getTarget())\n+            .withWeight(new DiagonalMatrix(new double[] { 1, 1, 1 }))\n+            .withStartPoint(new double[] { 0, 0 });\n+\n+        PointVectorValuePair optimum = optimizer.optimize();\n+\n+        Assert.assertEquals(0, optimizer.computeRMS(optimum.getPoint()), 1e-10);\n+        Assert.assertEquals(7, optimum.getPoint()[0], 1e-10);\n+        Assert.assertEquals(3, optimum.getPoint()[1], 1e-10);\n+        Assert.assertEquals(4, optimum.getValue()[0], 1e-10);\n+        Assert.assertEquals(6, optimum.getValue()[1], 1e-10);\n+        Assert.assertEquals(1, optimum.getValue()[2], 1e-10);\n+    }\n+\n+    @Test\n+    public void testNoDependency() {\n+        LinearProblem problem = new LinearProblem(new double[][] {\n+                { 2, 0, 0, 0, 0, 0 },\n+                { 0, 2, 0, 0, 0, 0 },\n+                { 0, 0, 2, 0, 0, 0 },\n+                { 0, 0, 0, 2, 0, 0 },\n+                { 0, 0, 0, 0, 2, 0 },\n+                { 0, 0, 0, 0, 0, 2 }\n+        }, new double[] { 0, 1.1, 2.2, 3.3, 4.4, 5.5 });\n+        T optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(problem.getTarget())\n+            .withWeight(new DiagonalMatrix(new double[] { 1, 1, 1, 1, 1, 1 }))\n+            .withStartPoint(new double[] { 0, 0, 0, 0, 0, 0 });\n+\n+        double[] optimum = optimizer.optimize().getPoint();\n+        Assert.assertEquals(0, optimizer.computeRMS(optimum), 1e-10);\n+        for (int i = 0; i < problem.target.length; ++i) {\n+            Assert.assertEquals(0.55 * i, optimum[i], 1e-10);\n+        }\n+    }\n+\n+    @Test\n+    public void testOneSet() {\n+        LinearProblem problem = new LinearProblem(new double[][] {\n+                {  1,  0, 0 },\n+                { -1,  1, 0 },\n+                {  0, -1, 1 }\n+        }, new double[] { 1, 1, 1});\n+\n+        T optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(problem.getTarget())\n+            .withWeight(new DiagonalMatrix(new double[] { 1, 1, 1 }))\n+            .withStartPoint(new double[] { 0, 0, 0 });\n+\n+        double[] optimum = optimizer.optimize().getPoint();\n+        Assert.assertEquals(0, optimizer.computeRMS(optimum), 1e-10);\n+        Assert.assertEquals(1, optimum[0], 1e-10);\n+        Assert.assertEquals(2, optimum[1], 1e-10);\n+        Assert.assertEquals(3, optimum[2], 1e-10);\n+    }\n+\n+    @Test\n+    public void testTwoSets() {\n+        double epsilon = 1e-7;\n+        LinearProblem problem = new LinearProblem(new double[][] {\n+                {  2,  1,   0,  4,       0, 0 },\n+                { -4, -2,   3, -7,       0, 0 },\n+                {  4,  1,  -2,  8,       0, 0 },\n+                {  0, -3, -12, -1,       0, 0 },\n+                {  0,  0,   0,  0, epsilon, 1 },\n+                {  0,  0,   0,  0,       1, 1 }\n+        }, new double[] { 2, -9, 2, 2, 1 + epsilon * epsilon, 2});\n+\n+        T optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(problem.getTarget())\n+            .withWeight(new DiagonalMatrix(new double[] { 1, 1, 1, 1, 1, 1 }))\n+            .withStartPoint(new double[] { 0, 0, 0, 0, 0, 0 });\n+\n+        double[] optimum = optimizer.optimize().getPoint();\n+\n+        Assert.assertEquals(0, optimizer.computeRMS(optimum), 1e-10);\n+        Assert.assertEquals(3, optimum[0], 1e-10);\n+        Assert.assertEquals(4, optimum[1], 1e-10);\n+        Assert.assertEquals(-1, optimum[2], 1e-10);\n+        Assert.assertEquals(-2, optimum[3], 1e-10);\n+        Assert.assertEquals(1 + epsilon, optimum[4], 1e-10);\n+        Assert.assertEquals(1 - epsilon, optimum[5], 1e-10);\n+    }\n+\n+    @Test(expected=ConvergenceException.class)\n+    public void testNonInvertible() throws Exception {\n+        LinearProblem problem = new LinearProblem(new double[][] {\n+                {  1, 2, -3 },\n+                {  2, 1,  3 },\n+                { -3, 0, -9 }\n+        }, new double[] { 1, 1, 1 });\n+\n+        T optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(problem.getTarget())\n+            .withWeight(new DiagonalMatrix(new double[] { 1, 1, 1 }))\n+            .withStartPoint(new double[] { 0, 0, 0 });\n+\n+        optimizer.optimize();\n+    }\n+\n+    @Test\n+    public void testIllConditioned() {\n+        LinearProblem problem1 = new LinearProblem(new double[][] {\n+                { 10, 7,  8,  7 },\n+                {  7, 5,  6,  5 },\n+                {  8, 6, 10,  9 },\n+                {  7, 5,  9, 10 }\n+        }, new double[] { 32, 23, 33, 31 });\n+        T optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withModelAndJacobian(problem1.getModelFunction(),\n+                                  problem1.getModelFunctionJacobian())\n+            .withTarget(problem1.getTarget())\n+            .withWeight(new DiagonalMatrix(new double[] { 1, 1, 1, 1 }))\n+            .withStartPoint(new double[] { 0, 1, 2, 3 });\n+\n+        double[] optimum = optimizer.optimize().getPoint();\n+\n+        Assert.assertEquals(0, optimizer.computeRMS(optimum), 1e-10);\n+        Assert.assertEquals(1, optimum[0], 1e-10);\n+        Assert.assertEquals(1, optimum[1], 1e-10);\n+        Assert.assertEquals(1, optimum[2], 1e-10);\n+        Assert.assertEquals(1, optimum[3], 1e-10);\n+\n+        LinearProblem problem2 = new LinearProblem(new double[][] {\n+                { 10.00, 7.00, 8.10, 7.20 },\n+                {  7.08, 5.04, 6.00, 5.00 },\n+                {  8.00, 5.98, 9.89, 9.00 },\n+                {  6.99, 4.99, 9.00, 9.98 }\n+        }, new double[] { 32, 23, 33, 31 });\n+\n+        optimizer = optimizer\n+            .withModelAndJacobian(problem2.getModelFunction(),\n+                                  problem2.getModelFunctionJacobian())\n+            .withTarget(problem2.getTarget());\n+\n+        optimum = optimizer.optimize().getPoint();\n+\n+        Assert.assertEquals(0, optimizer.computeRMS(optimum), 1e-10);\n+        Assert.assertEquals(-81, optimum[0], 1e-8);\n+        Assert.assertEquals(137, optimum[1], 1e-8);\n+        Assert.assertEquals(-34, optimum[2], 1e-8);\n+        Assert.assertEquals( 22, optimum[3], 1e-8);\n+    }\n+\n+    @Test\n+    public void testMoreEstimatedParametersSimple() {\n+        LinearProblem problem = new LinearProblem(new double[][] {\n+                { 3, 2,  0, 0 },\n+                { 0, 1, -1, 1 },\n+                { 2, 0,  1, 0 }\n+        }, new double[] { 7, 3, 5 });\n+\n+        T optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(problem.getTarget())\n+            .withWeight(new DiagonalMatrix(new double[] { 1, 1, 1 }))\n+            .withStartPoint(new double[] { 7, 6, 5, 4 });\n+\n+        double[] optimum = optimizer.optimize().getPoint();\n+        Assert.assertEquals(0, optimizer.computeRMS(optimum), 1e-10);\n+    }\n+\n+    @Test\n+    public void testMoreEstimatedParametersUnsorted() {\n+        LinearProblem problem = new LinearProblem(new double[][] {\n+                { 1, 1,  0,  0, 0,  0 },\n+                { 0, 0,  1,  1, 1,  0 },\n+                { 0, 0,  0,  0, 1, -1 },\n+                { 0, 0, -1,  1, 0,  1 },\n+                { 0, 0,  0, -1, 1,  0 }\n+       }, new double[] { 3, 12, -1, 7, 1 });\n+\n+        T optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(problem.getTarget())\n+            .withWeight(new DiagonalMatrix(new double[] { 1, 1, 1, 1, 1 }))\n+            .withStartPoint(new double[] { 2, 2, 2, 2, 2, 2 });\n+\n+        double[] optimum = optimizer.optimize().getPoint();\n+\n+        Assert.assertEquals(0, optimizer.computeRMS(optimum), 1e-10);\n+        Assert.assertEquals(3, optimum[2], 1e-10);\n+        Assert.assertEquals(4, optimum[3], 1e-10);\n+        Assert.assertEquals(5, optimum[4], 1e-10);\n+        Assert.assertEquals(6, optimum[5], 1e-10);\n+    }\n+\n+    @Test\n+    public void testRedundantEquations() {\n+        LinearProblem problem = new LinearProblem(new double[][] {\n+                { 1,  1 },\n+                { 1, -1 },\n+                { 1,  3 }\n+        }, new double[] { 3, 1, 5 });\n+\n+        T optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(problem.getTarget())\n+            .withWeight(new DiagonalMatrix(new double[] { 1, 1, 1 }))\n+            .withStartPoint(new double[] { 1, 1 });\n+\n+        double[] optimum = optimizer.optimize().getPoint();\n+\n+        Assert.assertEquals(0, optimizer.computeRMS(optimum), 1e-10);\n+        Assert.assertEquals(2, optimum[0], 1e-10);\n+        Assert.assertEquals(1, optimum[1], 1e-10);\n+    }\n+\n+    @Test\n+    public void testInconsistentEquations() {\n+        LinearProblem problem = new LinearProblem(new double[][] {\n+                { 1,  1 },\n+                { 1, -1 },\n+                { 1,  3 }\n+        }, new double[] { 3, 1, 4 });\n+\n+        T optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(problem.getTarget())\n+            .withWeight(new DiagonalMatrix(new double[] { 1, 1, 1 }))\n+            .withStartPoint(new double[] { 1, 1 });\n+\n+        double[] optimum = optimizer.optimize().getPoint();\n+\n+        Assert.assertTrue(optimizer.computeRMS(optimum) > 0.1);\n+    }\n+\n+    @Test(expected=DimensionMismatchException.class)\n+    public void testInconsistentSizes1() {\n+        LinearProblem problem\n+            = new LinearProblem(new double[][] { { 1, 0 },\n+                                                 { 0, 1 } },\n+                                new double[] { -1, 1 });\n+        T optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(problem.getTarget())\n+            .withWeight(new DiagonalMatrix(new double[] { 1, 1 }))\n+            .withStartPoint(new double[] { 0, 0 });\n+\n+        double[] optimum = optimizer.optimize().getPoint();\n+\n+        Assert.assertEquals(0, optimizer.computeRMS(optimum), 1e-10);\n+        Assert.assertEquals(-1, optimum[0], 1e-10);\n+        Assert.assertEquals(1, optimum[1], 1e-10);\n+\n+        optimizer.withWeight(new DiagonalMatrix(new double[] { 1 })).optimize();\n+    }\n+\n+    @Test(expected=DimensionMismatchException.class)\n+    public void testInconsistentSizes2() {\n+        LinearProblem problem\n+            = new LinearProblem(new double[][] { { 1, 0 }, { 0, 1 } },\n+                                new double[] { -1, 1 });\n+\n+        T optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(problem.getTarget())\n+            .withWeight(new DiagonalMatrix(new double[] { 1, 1 }))\n+            .withStartPoint(new double[] { 0, 0 });\n+\n+        double[] optimum = optimizer.optimize().getPoint();\n+\n+        Assert.assertEquals(0, optimizer.computeRMS(optimum), 1e-10);\n+        Assert.assertEquals(-1, optimum[0], 1e-10);\n+        Assert.assertEquals(1, optimum[1], 1e-10);\n+\n+        optimizer\n+            .withTarget(new double[] { 1 })\n+            .withWeight(new DiagonalMatrix(new double[] { 1 }))\n+            .optimize();\n+    }\n+\n+    @Test\n+    public void testCircleFitting() {\n+        CircleVectorial circle = new CircleVectorial();\n+        circle.addPoint( 30,  68);\n+        circle.addPoint( 50,  -6);\n+        circle.addPoint(110, -20);\n+        circle.addPoint( 35,  15);\n+        circle.addPoint( 45,  97);\n+\n+        T optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withModelAndJacobian(circle.getModelFunction(),\n+                                  circle.getModelFunctionJacobian())\n+            .withTarget(new double[] { 0, 0, 0, 0, 0 })\n+            .withWeight(new DiagonalMatrix(new double[] { 1, 1, 1, 1, 1 }))\n+            .withStartPoint(new double[] { 98.680, 47.345 });\n+\n+        double[] optimum = optimizer.optimize().getPoint();\n+        Assert.assertTrue(optimizer.getEvaluations() < 10);\n+\n+        double rms = optimizer.computeRMS(optimum);\n+        Assert.assertEquals(1.768262623567235,  FastMath.sqrt(circle.getN()) * rms, 1e-10);\n+\n+        Vector2D center = new Vector2D(optimum[0], optimum[1]);\n+        Assert.assertEquals(69.96016176931406, circle.getRadius(center), 1e-6);\n+        Assert.assertEquals(96.07590211815305, center.getX(), 1e-6);\n+        Assert.assertEquals(48.13516790438953, center.getY(), 1e-6);\n+\n+        double[][] cov = optimizer.computeCovariances(optimum, 1e-14);\n+        Assert.assertEquals(1.839, cov[0][0], 0.001);\n+        Assert.assertEquals(0.731, cov[0][1], 0.001);\n+        Assert.assertEquals(cov[0][1], cov[1][0], 1e-14);\n+        Assert.assertEquals(0.786, cov[1][1], 0.001);\n+\n+        // add perfect measurements and check errors are reduced\n+        double  r = circle.getRadius(center);\n+        for (double d= 0; d < 2 * FastMath.PI; d += 0.01) {\n+            circle.addPoint(center.getX() + r * FastMath.cos(d), center.getY() + r * FastMath.sin(d));\n+        }\n+\n+        double[] target = new double[circle.getN()];\n+        Arrays.fill(target, 0);\n+        double[] weights = new double[circle.getN()];\n+        Arrays.fill(weights, 2);\n+        optimizer = optimizer.withTarget(target).withWeight(new DiagonalMatrix(weights));\n+        optimum = optimizer.optimize().getPoint();\n+\n+        cov = optimizer.computeCovariances(optimum, 1e-14);\n+        Assert.assertEquals(0.0016, cov[0][0], 0.001);\n+        Assert.assertEquals(3.2e-7, cov[0][1], 1e-9);\n+        Assert.assertEquals(cov[0][1], cov[1][0], 1e-14);\n+        Assert.assertEquals(0.0016, cov[1][1], 0.001);\n+    }\n+\n+    @Test\n+    public void testCircleFittingBadInit() {\n+        CircleVectorial circle = new CircleVectorial();\n+        double[][] points = circlePoints;\n+        double[] target = new double[points.length];\n+        Arrays.fill(target, 0);\n+        double[] weights = new double[points.length];\n+        Arrays.fill(weights, 2);\n+        for (int i = 0; i < points.length; ++i) {\n+            circle.addPoint(points[i][0], points[i][1]);\n+        }\n+        T optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withModelAndJacobian(circle.getModelFunction(),\n+                                  circle.getModelFunctionJacobian())\n+            .withTarget(target)\n+            .withWeight(new DiagonalMatrix(weights))\n+            .withStartPoint(new double[] { -12, -12 });\n+\n+        double[] optimum = optimizer.optimize().getPoint();\n+\n+        Vector2D center = new Vector2D(optimum[0], optimum[1]);\n+        Assert.assertTrue(optimizer.getEvaluations() < 25);\n+        Assert.assertEquals( 0.043, optimizer.computeRMS(optimum), 1e-3);\n+        Assert.assertEquals( 0.292235,  circle.getRadius(center), 1e-6);\n+        Assert.assertEquals(-0.151738,  center.getX(), 1e-6);\n+        Assert.assertEquals( 0.2075001, center.getY(), 1e-6);\n+    }\n+\n+    @Test\n+    public void testCircleFittingGoodInit() {\n+        CircleVectorial circle = new CircleVectorial();\n+        double[][] points = circlePoints;\n+        double[] target = new double[points.length];\n+        Arrays.fill(target, 0);\n+        double[] weights = new double[points.length];\n+        Arrays.fill(weights, 2);\n+        for (int i = 0; i < points.length; ++i) {\n+            circle.addPoint(points[i][0], points[i][1]);\n+        }\n+        T optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withModelAndJacobian(circle.getModelFunction(),\n+                                  circle.getModelFunctionJacobian())\n+            .withTarget(target)\n+            .withWeight(new DiagonalMatrix(weights))\n+            .withStartPoint(new double[] { 0, 0 });\n+\n+        double[] optimum = optimizer.optimize().getPoint();\n+\n+        Assert.assertEquals(-0.1517383071957963, optimum[0], 1e-6);\n+        Assert.assertEquals(0.2074999736353867,  optimum[1], 1e-6);\n+        Assert.assertEquals(0.04268731682389561, optimizer.computeRMS(optimum), 1e-8);\n+    }\n+\n+    private final double[][] circlePoints = new double[][] {\n+        {-0.312967,  0.072366}, {-0.339248,  0.132965}, {-0.379780,  0.202724},\n+        {-0.390426,  0.260487}, {-0.361212,  0.328325}, {-0.346039,  0.392619},\n+        {-0.280579,  0.444306}, {-0.216035,  0.470009}, {-0.149127,  0.493832},\n+        {-0.075133,  0.483271}, {-0.007759,  0.452680}, { 0.060071,  0.410235},\n+        { 0.103037,  0.341076}, { 0.118438,  0.273884}, { 0.131293,  0.192201},\n+        { 0.115869,  0.129797}, { 0.072223,  0.058396}, { 0.022884,  0.000718},\n+        {-0.053355, -0.020405}, {-0.123584, -0.032451}, {-0.216248, -0.032862},\n+        {-0.278592, -0.005008}, {-0.337655,  0.056658}, {-0.385899,  0.112526},\n+        {-0.405517,  0.186957}, {-0.415374,  0.262071}, {-0.387482,  0.343398},\n+        {-0.347322,  0.397943}, {-0.287623,  0.458425}, {-0.223502,  0.475513},\n+        {-0.135352,  0.478186}, {-0.061221,  0.483371}, { 0.003711,  0.422737},\n+        { 0.065054,  0.375830}, { 0.108108,  0.297099}, { 0.123882,  0.222850},\n+        { 0.117729,  0.134382}, { 0.085195,  0.056820}, { 0.029800, -0.019138},\n+        {-0.027520, -0.072374}, {-0.102268, -0.091555}, {-0.200299, -0.106578},\n+        {-0.292731, -0.091473}, {-0.356288, -0.051108}, {-0.420561,  0.014926},\n+        {-0.471036,  0.074716}, {-0.488638,  0.182508}, {-0.485990,  0.254068},\n+        {-0.463943,  0.338438}, {-0.406453,  0.404704}, {-0.334287,  0.466119},\n+        {-0.254244,  0.503188}, {-0.161548,  0.495769}, {-0.075733,  0.495560},\n+        { 0.001375,  0.434937}, { 0.082787,  0.385806}, { 0.115490,  0.323807},\n+        { 0.141089,  0.223450}, { 0.138693,  0.131703}, { 0.126415,  0.049174},\n+        { 0.066518, -0.010217}, {-0.005184, -0.070647}, {-0.080985, -0.103635},\n+        {-0.177377, -0.116887}, {-0.260628, -0.100258}, {-0.335756, -0.056251},\n+        {-0.405195, -0.000895}, {-0.444937,  0.085456}, {-0.484357,  0.175597},\n+        {-0.472453,  0.248681}, {-0.438580,  0.347463}, {-0.402304,  0.422428},\n+        {-0.326777,  0.479438}, {-0.247797,  0.505581}, {-0.152676,  0.519380},\n+        {-0.071754,  0.516264}, { 0.015942,  0.472802}, { 0.076608,  0.419077},\n+        { 0.127673,  0.330264}, { 0.159951,  0.262150}, { 0.153530,  0.172681},\n+        { 0.140653,  0.089229}, { 0.078666,  0.024981}, { 0.023807, -0.037022},\n+        {-0.048837, -0.077056}, {-0.127729, -0.075338}, {-0.221271, -0.067526}\n+    };\n+\n+    public void doTestStRD(final StatisticalReferenceDataset dataset,\n+                           final double errParams,\n+                           final double errParamsSd) {\n+        final double[] w = new double[dataset.getNumObservations()];\n+        Arrays.fill(w, 1);\n+\n+        final double[][] data = dataset.getData();\n+        final double[] initial = dataset.getStartingPoint(0);\n+        final StatisticalReferenceDataset.LeastSquaresProblem problem = dataset.getLeastSquaresProblem();\n+\n+        final T optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(getMaxIterations())\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(data[1])\n+            .withWeight(new DiagonalMatrix(w))\n+            .withStartPoint(initial);\n+\n+        final double[] actual = optimizer.optimize().getPoint();\n+        for (int i = 0; i < actual.length; i++) {\n+            double expected = dataset.getParameter(i);\n+            double delta = FastMath.abs(errParams * expected);\n+            Assert.assertEquals(dataset.getName() + \", param #\" + i,\n+                                expected, actual[i], delta);\n+        }\n+    }\n+\n+    @Test\n+    public void testKirby2() throws IOException {\n+        doTestStRD(StatisticalReferenceDatasetFactory.createKirby2(), 1E-7, 1E-7);\n+    }\n+\n+    @Test\n+    public void testHahn1() throws IOException {\n+        doTestStRD(StatisticalReferenceDatasetFactory.createHahn1(), 1E-7, 1E-4);\n+    }\n+\n+    static class LinearProblem {\n+        private final RealMatrix factors;\n+        private final double[] target;\n+\n+        public LinearProblem(double[][] factors, double[] target) {\n+            this.factors = new BlockRealMatrix(factors);\n+            this.target  = target;\n+        }\n+\n+        public double[] getTarget() {\n+            return target;\n+        }\n+\n+        public MultivariateVectorFunction getModelFunction() {\n+            return new MultivariateVectorFunction() {\n+                public double[] value(double[] params) {\n+                    return factors.operate(params);\n+                }\n+            };\n+        }\n+\n+        public MultivariateMatrixFunction getModelFunctionJacobian() {\n+            return new MultivariateMatrixFunction() {\n+                public double[][] value(double[] params) {\n+                    return factors.getData();\n+                }\n+            };\n+        }\n+    }\n+}\n--- /dev/null\n+++ b/src/test/java/org/apache/commons/math3/fitting/leastsquares/AbstractLeastSquaresOptimizerTest.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with this\n+ * work for additional information regarding copyright ownership. The ASF\n+ * licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ * http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law\n+ * or agreed to in writing, software distributed under the License is\n+ * distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the specific language\n+ * governing permissions and limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import org.apache.commons.math3.optim.PointVectorValuePair;\n+import org.apache.commons.math3.linear.DiagonalMatrix;\n+import org.apache.commons.math3.util.FastMath;\n+import org.junit.Test;\n+import org.junit.Assert;\n+/**\n+ * The only features tested here are utility methods defined\n+ * in {@link AbstractLeastSquaresOptimizer} that compute the\n+ * chi-square and parameters standard-deviations.\n+ */\n+public class AbstractLeastSquaresOptimizerTest {\n+    @Test\n+    public void testComputeCost() throws IOException {\n+        final StatisticalReferenceDataset dataset\n+            = StatisticalReferenceDatasetFactory.createKirby2();\n+        final double[] a = dataset.getParameters();\n+        final double[] y = dataset.getData()[1];\n+        final double[] w = new double[y.length];\n+        Arrays.fill(w, 1d);\n+\n+        StatisticalReferenceDataset.LeastSquaresProblem problem\n+            = dataset.getLeastSquaresProblem();\n+\n+        final LevenbergMarquardtOptimizer optim = LevenbergMarquardtOptimizer.create()\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(y)\n+            .withWeight(new DiagonalMatrix(w))\n+            .withStartPoint(a);\n+\n+        final double expected = dataset.getResidualSumOfSquares();\n+        final double cost = optim.computeCost(optim.computeResiduals(optim.getModel().value(optim.getStart())));\n+        final double actual = cost * cost;\n+        Assert.assertEquals(dataset.getName(), expected, actual, 1e-11 * expected);\n+    }\n+\n+    @Test\n+    public void testComputeRMS() throws IOException {\n+        final StatisticalReferenceDataset dataset\n+            = StatisticalReferenceDatasetFactory.createKirby2();\n+        final double[] a = dataset.getParameters();\n+        final double[] y = dataset.getData()[1];\n+        final double[] w = new double[y.length];\n+        Arrays.fill(w, 1d);\n+\n+        StatisticalReferenceDataset.LeastSquaresProblem problem\n+            = dataset.getLeastSquaresProblem();\n+\n+        final LevenbergMarquardtOptimizer optim = LevenbergMarquardtOptimizer.create()\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(y)\n+            .withWeight(new DiagonalMatrix(w))\n+            .withStartPoint(a);\n+\n+        final double expected = FastMath.sqrt(dataset.getResidualSumOfSquares() /\n+                                              dataset.getNumObservations());\n+        final double actual = optim.computeRMS(optim.getStart());\n+        Assert.assertEquals(dataset.getName(), expected, actual, 1e-11 * expected);\n+    }\n+\n+    @Test\n+    public void testComputeSigma() throws IOException {\n+        final StatisticalReferenceDataset dataset\n+            = StatisticalReferenceDatasetFactory.createKirby2();\n+        final double[] a = dataset.getParameters();\n+        final double[] y = dataset.getData()[1];\n+        final double[] w = new double[y.length];\n+        Arrays.fill(w, 1d);\n+\n+        StatisticalReferenceDataset.LeastSquaresProblem problem\n+            = dataset.getLeastSquaresProblem();\n+\n+        final LevenbergMarquardtOptimizer optim = LevenbergMarquardtOptimizer.create()\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(y)\n+            .withWeight(new DiagonalMatrix(w))\n+            .withStartPoint(a);\n+\n+        final double[] expected = dataset.getParametersStandardDeviations();\n+\n+        final double cost = optim.computeCost(optim.computeResiduals(optim.getModel().value(optim.getStart())));\n+        final double[] sig = optim.computeSigma(optim.getStart(), 1e-14);\n+        final int dof = y.length - a.length;\n+        for (int i = 0; i < sig.length; i++) {\n+            final double actual = FastMath.sqrt(cost * cost / dof) * sig[i];\n+            Assert.assertEquals(dataset.getName() + \", parameter #\" + i,\n+                                expected[i], actual, 1e-6 * expected[i]);\n+        }\n+    }\n+}\n--- /dev/null\n+++ b/src/test/java/org/apache/commons/math3/fitting/leastsquares/AbstractLeastSquaresOptimizerTestValidation.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with this\n+ * work for additional information regarding copyright ownership. The ASF\n+ * licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ * http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law\n+ * or agreed to in writing, software distributed under the License is\n+ * distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the specific language\n+ * governing permissions and limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.ArrayList;\n+import java.awt.geom.Point2D;\n+import org.apache.commons.math3.optim.PointVectorValuePair;\n+import org.apache.commons.math3.stat.descriptive.SummaryStatistics;\n+import org.apache.commons.math3.stat.descriptive.StatisticalSummary;\n+import org.apache.commons.math3.linear.DiagonalMatrix;\n+import org.apache.commons.math3.util.FastMath;\n+import org.junit.Test;\n+import org.junit.Assert;\n+\n+/**\n+ * This class demonstrates the main functionality of the\n+ * {@link AbstractLeastSquaresOptimizer}, common to the\n+ * optimizer implementations in package\n+ * {@link org.apache.commons.math3.fitting.leastsquares}.\n+ * <br/>\n+ * Not enabled by default, as the class name does not end with \"Test\".\n+ * <br/>\n+ * Invoke by running\n+ * <pre><code>\n+ *  mvn test -Dtest=AbstractLeastSquaresOptimizerTestValidation\n+ * </code></pre>\n+ * or by running\n+ * <pre><code>\n+ *  mvn test -Dtest=AbstractLeastSquaresOptimizerTestValidation -DargLine=\"-DmcRuns=1234 -server\"\n+ * </code></pre>\n+ */\n+public class AbstractLeastSquaresOptimizerTestValidation {\n+    /** Number of runs. */\n+    private static final int MONTE_CARLO_RUNS = Integer.parseInt(System.getProperty(\"mcRuns\",\n+                                                                                    \"100\"));\n+\n+    /**\n+     * Using a Monte-Carlo procedure, this test checks the error estimations\n+     * as provided by the square-root of the diagonal elements of the\n+     * covariance matrix.\n+     * <br/>\n+     * The test generates sets of observations, each sampled from\n+     * a Gaussian distribution.\n+     * <br/>\n+     * The optimization problem solved is defined in class\n+     * {@link StraightLineProblem}.\n+     * <br/>\n+     * The output (on stdout) will be a table summarizing the distribution\n+     * of parameters generated by the Monte-Carlo process and by the direct\n+     * estimation provided by the diagonal elements of the covariance matrix.\n+     */\n+    @Test\n+    public void testParametersErrorMonteCarloObservations() {\n+        // Error on the observations.\n+        final double yError = 15;\n+\n+        // True values of the parameters.\n+        final double slope = 123.456;\n+        final double offset = -98.765;\n+\n+        // Samples generator.\n+        final RandomStraightLinePointGenerator lineGenerator\n+            = new RandomStraightLinePointGenerator(slope, offset,\n+                                                   yError,\n+                                                   -1e3, 1e4,\n+                                                   138577L);\n+\n+        // Number of observations.\n+        final int numObs = 100; // XXX Should be a command-line option.\n+        // number of parameters.\n+        final int numParams = 2;\n+\n+        // Parameters found for each of Monte-Carlo run.\n+        final SummaryStatistics[] paramsFoundByDirectSolution = new SummaryStatistics[numParams];\n+        // Sigma estimations (square-root of the diagonal elements of the\n+        // covariance matrix), for each Monte-Carlo run.\n+        final SummaryStatistics[] sigmaEstimate = new SummaryStatistics[numParams];\n+\n+        // Initialize statistics accumulators.\n+        for (int i = 0; i < numParams; i++) {\n+            paramsFoundByDirectSolution[i] = new SummaryStatistics();\n+            sigmaEstimate[i] = new SummaryStatistics();\n+        }\n+\n+        final double[] init = { slope, offset };\n+\n+        // Monte-Carlo (generates many sets of observations).\n+        final int mcRepeat = MONTE_CARLO_RUNS;\n+        int mcCount = 0;\n+        while (mcCount < mcRepeat) {\n+            // Observations.\n+            final Point2D.Double[] obs = lineGenerator.generate(numObs);\n+\n+            final StraightLineProblem problem = new StraightLineProblem(yError);\n+            for (int i = 0; i < numObs; i++) {\n+                final Point2D.Double p = obs[i];\n+                problem.addPoint(p.x, p.y);\n+            }\n+\n+            // Direct solution (using simple regression).\n+            final double[] regress = problem.solve();\n+\n+            // Estimation of the standard deviation (diagonal elements of the\n+            // covariance matrix).\n+            // Dummy optimizer (to compute the covariance matrix).\n+            final AbstractLeastSquaresOptimizer optim = LevenbergMarquardtOptimizer.create()\n+                .withModelAndJacobian(problem.getModelFunction(),\n+                                      problem.getModelFunctionJacobian())\n+                .withTarget(problem.target())\n+                .withWeight(new DiagonalMatrix(problem.weight()));\n+\n+            final double[] sigma = optim.computeSigma(init, 1e-14);\n+\n+            // Accumulate statistics.\n+            for (int i = 0; i < numParams; i++) {\n+                paramsFoundByDirectSolution[i].addValue(regress[i]);\n+                sigmaEstimate[i].addValue(sigma[i]);\n+            }\n+\n+            // Next Monte-Carlo.\n+            ++mcCount;\n+        }\n+\n+        // Print statistics.\n+        final String line = \"--------------------------------------------------------------\";\n+        System.out.println(\"                 True value       Mean        Std deviation\");\n+        for (int i = 0; i < numParams; i++) {\n+            System.out.println(line);\n+            System.out.println(\"Parameter #\" + i);\n+\n+            StatisticalSummary s = paramsFoundByDirectSolution[i].getSummary();\n+            System.out.printf(\"              %+.6e   %+.6e   %+.6e\\n\",\n+                              init[i],\n+                              s.getMean(),\n+                              s.getStandardDeviation());\n+\n+            s = sigmaEstimate[i].getSummary();\n+            System.out.printf(\"sigma: %+.6e (%+.6e)\\n\",\n+                              s.getMean(),\n+                              s.getStandardDeviation());\n+        }\n+        System.out.println(line);\n+\n+        // Check the error estimation.\n+        for (int i = 0; i < numParams; i++) {\n+            Assert.assertEquals(paramsFoundByDirectSolution[i].getSummary().getStandardDeviation(),\n+                                sigmaEstimate[i].getSummary().getMean(),\n+                                8e-2);\n+        }\n+    }\n+\n+    /**\n+     * In this test, the set of observations is fixed.\n+     * Using a Monte-Carlo procedure, it generates sets of parameters,\n+     * and determine the parameter change that will result in the\n+     * normalized chi-square becoming larger by one than the value from\n+     * the best fit solution.\n+     * <br/>\n+     * The optimization problem solved is defined in class\n+     * {@link StraightLineProblem}.\n+     * <br/>\n+     * The output (on stdout) will be a list of lines containing:\n+     * <ul>\n+     *  <li>slope of the straight line,</li>\n+     *  <li>intercept of the straight line,</li>\n+     *  <li>chi-square of the solution defined by the above two values.</li>\n+     * </ul>\n+     * The output is separated into two blocks (with a blank line between\n+     * them); the first block will contain all parameter sets for which\n+     * {@code chi2 < chi2_b + 1}\n+     * and the second block, all sets for which\n+     * {@code chi2 >= chi2_b + 1}\n+     * where {@code chi2_b} is the lowest chi-square (corresponding to the\n+     * best solution).\n+     */\n+    @Test\n+    public void testParametersErrorMonteCarloParameters() {\n+        // Error on the observations.\n+        final double yError = 15;\n+\n+        // True values of the parameters.\n+        final double slope = 123.456;\n+        final double offset = -98.765;\n+\n+        // Samples generator.\n+        final RandomStraightLinePointGenerator lineGenerator\n+            = new RandomStraightLinePointGenerator(slope, offset,\n+                                                   yError,\n+                                                   -1e3, 1e4,\n+                                                   13839013L);\n+\n+        // Number of observations.\n+        final int numObs = 10;\n+        // number of parameters.\n+        final int numParams = 2;\n+\n+        // Create a single set of observations.\n+        final Point2D.Double[] obs = lineGenerator.generate(numObs);\n+\n+        final StraightLineProblem problem = new StraightLineProblem(yError);\n+        for (int i = 0; i < numObs; i++) {\n+            final Point2D.Double p = obs[i];\n+            problem.addPoint(p.x, p.y);\n+        }\n+\n+        // Direct solution (using simple regression).\n+        final double[] regress = problem.solve();\n+\n+        // Dummy optimizer (to compute the chi-square).\n+        final AbstractLeastSquaresOptimizer optim = LevenbergMarquardtOptimizer.create()\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(problem.target())\n+            .withWeight(new DiagonalMatrix(problem.weight()));\n+\n+        final double[] init = { slope, offset };\n+        // Get chi-square of the best parameters set for the given set of\n+        // observations.\n+        final double bestChi2N = getChi2N(optim, problem, regress);\n+        final double[] sigma = optim.computeSigma(regress, 1e-14);\n+\n+        // Monte-Carlo (generates a grid of parameters).\n+        final int mcRepeat = MONTE_CARLO_RUNS;\n+        final int gridSize = (int) FastMath.sqrt(mcRepeat);\n+\n+        // Parameters found for each of Monte-Carlo run.\n+        // Index 0 = slope\n+        // Index 1 = offset\n+        // Index 2 = normalized chi2\n+        final List<double[]> paramsAndChi2 = new ArrayList<double[]>(gridSize * gridSize);\n+\n+        final double slopeRange = 10 * sigma[0];\n+        final double offsetRange = 10 * sigma[1];\n+        final double minSlope = slope - 0.5 * slopeRange;\n+        final double minOffset = offset - 0.5 * offsetRange;\n+        final double deltaSlope =  slopeRange/ gridSize;\n+        final double deltaOffset = offsetRange / gridSize;\n+        for (int i = 0; i < gridSize; i++) {\n+            final double s = minSlope + i * deltaSlope;\n+            for (int j = 0; j < gridSize; j++) {\n+                final double o = minOffset + j * deltaOffset;\n+                final double chi2N = getChi2N(optim, problem, new double[] {s, o});\n+\n+                paramsAndChi2.add(new double[] {s, o, chi2N});\n+            }\n+        }\n+\n+        // Output (for use with \"gnuplot\").\n+\n+        // Some info.\n+\n+        // For plotting separately sets of parameters that have a large chi2.\n+        final double chi2NPlusOne = bestChi2N + 1;\n+        int numLarger = 0;\n+\n+        final String lineFmt = \"%+.10e %+.10e   %.8e\\n\";\n+\n+        // Point with smallest chi-square.\n+        System.out.printf(lineFmt, regress[0], regress[1], bestChi2N);\n+        System.out.println(); // Empty line.\n+\n+        // Points within the confidence interval.\n+        for (double[] d : paramsAndChi2) {\n+            if (d[2] <= chi2NPlusOne) {\n+                System.out.printf(lineFmt, d[0], d[1], d[2]);\n+            }\n+        }\n+        System.out.println(); // Empty line.\n+\n+        // Points outside the confidence interval.\n+        for (double[] d : paramsAndChi2) {\n+            if (d[2] > chi2NPlusOne) {\n+                ++numLarger;\n+                System.out.printf(lineFmt, d[0], d[1], d[2]);\n+            }\n+        }\n+        System.out.println(); // Empty line.\n+\n+        System.out.println(\"# sigma=\" + Arrays.toString(sigma));\n+        System.out.println(\"# \" + numLarger + \" sets filtered out\");\n+    }\n+\n+    /**\n+     * @return the normalized chi-square.\n+     */\n+    private double getChi2N(AbstractLeastSquaresOptimizer optim,\n+                            StraightLineProblem problem,\n+                            double[] params) {\n+        final double[] t = problem.target();\n+        final double[] w = problem.weight();\n+        final double cost = optim.computeCost(optim.computeResiduals(optim.getModel().value(params)));\n+        return cost * cost / (t.length - params.length);\n+    }\n+}\n+\n--- /dev/null\n+++ b/src/test/java/org/apache/commons/math3/fitting/leastsquares/CircleProblem.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import java.util.ArrayList;\n+import org.apache.commons.math3.analysis.MultivariateVectorFunction;\n+import org.apache.commons.math3.analysis.MultivariateMatrixFunction;\n+import org.apache.commons.math3.util.MathUtils;\n+import org.apache.commons.math3.util.FastMath;\n+\n+/**\n+ * Class that models a circle.\n+ * The parameters of problem are:\n+ * <ul>\n+ *  <li>the x-coordinate of the circle center,</li>\n+ *  <li>the y-coordinate of the circle center,</li>\n+ *  <li>the radius of the circle.</li>\n+ * </ul>\n+ * The model functions are:\n+ * <ul>\n+ *  <li>for each triplet (cx, cy, r), the (x, y) coordinates of a point on the\n+ *   corresponding circle.</li>\n+ * </ul>\n+ */\n+class CircleProblem {\n+    /** Cloud of points assumed to be fitted by a circle. */\n+    private final ArrayList<double[]> points;\n+    /** Error on the x-coordinate of the points. */\n+    private final double xSigma;\n+    /** Error on the y-coordinate of the points. */\n+    private final double ySigma;\n+    /** Number of points on the circumference (when searching which\n+        model point is closest to a given \"observation\". */\n+    private final int resolution;\n+\n+    /**\n+     * @param xError Assumed error for the x-coordinate of the circle points.\n+     * @param yError Assumed error for the y-coordinate of the circle points.\n+     * @param searchResolution Number of points to try when searching the one\n+     * that is closest to a given \"observed\" point.\n+     */\n+    public CircleProblem(double xError,\n+                         double yError,\n+                         int searchResolution) {\n+        points = new ArrayList<double[]>();\n+        xSigma = xError;\n+        ySigma = yError;\n+        resolution = searchResolution;\n+    }\n+\n+    /**\n+     * @param xError Assumed error for the x-coordinate of the circle points.\n+     * @param yError Assumed error for the y-coordinate of the circle points.\n+     */\n+    public CircleProblem(double xError,\n+                         double yError) {\n+        this(xError, yError, 500);\n+    }\n+\n+    public void addPoint(double px, double py) {\n+        points.add(new double[] { px, py });\n+    }\n+\n+    public double[] target() {\n+        final double[] t = new double[points.size() * 2];\n+        for (int i = 0; i < points.size(); i++) {\n+            final double[] p = points.get(i);\n+            final int index = i * 2;\n+            t[index] = p[0];\n+            t[index + 1] = p[1];\n+        }\n+\n+        return t;\n+    }\n+\n+    public double[] weight() {\n+        final double wX = 1 / (xSigma * xSigma);\n+        final double wY = 1 / (ySigma * ySigma);\n+        final double[] w = new double[points.size() * 2];\n+        for (int i = 0; i < points.size(); i++) {\n+            final int index = i * 2;\n+            w[index] = wX;\n+            w[index + 1] = wY;\n+        }\n+\n+        return w;\n+    }\n+\n+    public MultivariateVectorFunction getModelFunction() {\n+        return new MultivariateVectorFunction() {\n+            public double[] value(double[] params) {\n+                final double cx = params[0];\n+                final double cy = params[1];\n+                final double r = params[2];\n+\n+                final double[] model = new double[points.size() * 2];\n+\n+                final double deltaTheta = MathUtils.TWO_PI / resolution;\n+                for (int i = 0; i < points.size(); i++) {\n+                    final double[] p = points.get(i);\n+                    final double px = p[0];\n+                    final double py = p[1];\n+\n+                    double bestX = 0;\n+                    double bestY = 0;\n+                    double dMin = Double.POSITIVE_INFINITY;\n+\n+                    // Find the angle for which the circle passes closest to the\n+                    // current point (using a resolution of 100 points along the\n+                    // circumference).\n+                    for (double theta = 0; theta <= MathUtils.TWO_PI; theta += deltaTheta) {\n+                        final double currentX = cx + r * FastMath.cos(theta);\n+                        final double currentY = cy + r * FastMath.sin(theta);\n+                        final double dX = currentX - px;\n+                        final double dY = currentY - py;\n+                        final double d = dX * dX + dY * dY;\n+                        if (d < dMin) {\n+                            dMin = d;\n+                            bestX = currentX;\n+                            bestY = currentY;\n+                        }\n+                    }\n+\n+                    final int index = i * 2;\n+                    model[index] = bestX;\n+                    model[index + 1] = bestY;\n+                }\n+\n+                return model;\n+            }\n+        };\n+    }\n+\n+    public MultivariateMatrixFunction getModelFunctionJacobian() {\n+        return new MultivariateMatrixFunction() {\n+            public double[][] value(double[] point) {\n+                return jacobian(point);\n+            }\n+        };\n+    }\n+\n+    private double[][] jacobian(double[] params) {\n+        final double[][] jacobian = new double[points.size() * 2][3];\n+\n+        for (int i = 0; i < points.size(); i++) {\n+            final int index = i * 2;\n+            // Partial derivative wrt x-coordinate of center. \n+            jacobian[index][0] = 1;\n+            jacobian[index + 1][0] = 0;\n+            // Partial derivative wrt y-coordinate of center.\n+            jacobian[index][1] = 0;\n+            jacobian[index + 1][1] = 1;\n+            // Partial derivative wrt radius.\n+            final double[] p = points.get(i);\n+            jacobian[index][2] = (p[0] - params[0]) / params[2];\n+            jacobian[index + 1][2] = (p[1] - params[1]) / params[2];\n+        }\n+\n+        return jacobian;\n+    }\n+}\n--- /dev/null\n+++ b/src/test/java/org/apache/commons/math3/fitting/leastsquares/CircleVectorial.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import java.util.ArrayList;\n+import org.apache.commons.math3.analysis.MultivariateVectorFunction;\n+import org.apache.commons.math3.analysis.MultivariateMatrixFunction;\n+import org.apache.commons.math3.geometry.euclidean.twod.Vector2D;\n+\n+/**\n+ * Class used in the tests.\n+ */\n+class CircleVectorial {\n+    private ArrayList<Vector2D> points;\n+\n+    public CircleVectorial() {\n+        points  = new ArrayList<Vector2D>();\n+    }\n+\n+    public void addPoint(double px, double py) {\n+        points.add(new Vector2D(px, py));\n+    }\n+\n+    public int getN() {\n+        return points.size();\n+    }\n+\n+    public double getRadius(Vector2D center) {\n+        double r = 0;\n+        for (Vector2D point : points) {\n+            r += point.distance(center);\n+        }\n+        return r / points.size();\n+    }\n+\n+    public MultivariateVectorFunction getModelFunction() {\n+        return new MultivariateVectorFunction() {\n+            public double[] value(double[] params) {\n+                Vector2D center = new Vector2D(params[0], params[1]);\n+                double radius = getRadius(center);\n+                double[] residuals = new double[points.size()];\n+                for (int i = 0; i < residuals.length; i++) {\n+                    residuals[i] = points.get(i).distance(center) - radius;\n+                }\n+                \n+                return residuals;\n+            }\n+        };\n+    }\n+\n+    public MultivariateMatrixFunction getModelFunctionJacobian() {\n+        return new MultivariateMatrixFunction() {\n+            public double[][] value(double[] params) {\n+                final int n = points.size();\n+                final Vector2D center = new Vector2D(params[0], params[1]);\n+\n+                double dRdX = 0;\n+                double dRdY = 0;\n+                for (Vector2D pk : points) {\n+                    double dk = pk.distance(center);\n+                    dRdX += (center.getX() - pk.getX()) / dk;\n+                    dRdY += (center.getY() - pk.getY()) / dk;\n+                }\n+                dRdX /= n;\n+                dRdY /= n;\n+\n+                // Jacobian of the radius residuals.\n+                double[][] jacobian = new double[n][2];\n+                for (int i = 0; i < n; i++) {\n+                    final Vector2D pi = points.get(i);\n+                    final double di = pi.distance(center);\n+                    jacobian[i][0] = (center.getX() - pi.getX()) / di - dRdX;\n+                    jacobian[i][1] = (center.getY() - pi.getY()) / di - dRdY;\n+                }\n+\n+                return jacobian;\n+            }\n+        };\n+    }\n+}\n--- /dev/null\n+++ b/src/test/java/org/apache/commons/math3/fitting/leastsquares/GaussNewtonOptimizerTest.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import java.io.IOException;\n+import org.apache.commons.math3.exception.ConvergenceException;\n+import org.apache.commons.math3.exception.TooManyEvaluationsException;\n+import org.apache.commons.math3.exception.MathUnsupportedOperationException;\n+import org.apache.commons.math3.optim.SimpleVectorValueChecker;\n+import org.apache.commons.math3.linear.DiagonalMatrix;\n+import org.junit.Test;\n+\n+/**\n+ * <p>Some of the unit tests are re-implementations of the MINPACK <a\n+ * href=\"http://www.netlib.org/minpack/ex/file17\">file17</a> and <a\n+ * href=\"http://www.netlib.org/minpack/ex/file22\">file22</a> test files.\n+ * The redistribution policy for MINPACK is available <a\n+ * href=\"http://www.netlib.org/minpack/disclaimer\">here</a>/\n+ *\n+ * @version $Id$\n+ */\n+public class GaussNewtonOptimizerTest\n+    extends AbstractLeastSquaresOptimizerAbstractTest<GaussNewtonOptimizer> {\n+    @Override\n+    public GaussNewtonOptimizer createOptimizer() {\n+        return GaussNewtonOptimizer.create()\n+            .withConvergenceChecker(new SimpleVectorValueChecker(1e-6, 1e-6));\n+    }\n+\n+    @Override\n+    public int getMaxIterations() {\n+        return 1000;\n+    }\n+\n+    @Override\n+    @Test(expected=ConvergenceException.class)\n+    public void testMoreEstimatedParametersSimple() {\n+        /*\n+         * Exception is expected with this optimizer\n+         */\n+        super.testMoreEstimatedParametersSimple();\n+    }\n+\n+    @Override\n+    @Test(expected=ConvergenceException.class)\n+    public void testMoreEstimatedParametersUnsorted() {\n+        /*\n+         * Exception is expected with this optimizer\n+         */\n+        super.testMoreEstimatedParametersUnsorted();\n+    }\n+\n+    @Test(expected=TooManyEvaluationsException.class)\n+    public void testMaxEvaluations() throws Exception {\n+        CircleVectorial circle = new CircleVectorial();\n+        circle.addPoint( 30.0,  68.0);\n+        circle.addPoint( 50.0,  -6.0);\n+        circle.addPoint(110.0, -20.0);\n+        circle.addPoint( 35.0,  15.0);\n+        circle.addPoint( 45.0,  97.0);\n+\n+        GaussNewtonOptimizer optimizer = createOptimizer()\n+            .withConvergenceChecker(new SimpleVectorValueChecker(1e-30, 1e-30))\n+            .withMaxIterations(Integer.MAX_VALUE)\n+            .withMaxEvaluations(100)\n+            .withModelAndJacobian(circle.getModelFunction(),\n+                                  circle.getModelFunctionJacobian())\n+            .withTarget(new double[] { 0, 0, 0, 0, 0 })\n+            .withWeight(new DiagonalMatrix(new double[] { 1, 1, 1, 1, 1 }))\n+            .withStartPoint(new double[] { 98.680, 47.345 });\n+\n+        optimizer.optimize();\n+    }\n+\n+    @Override\n+    @Test(expected=ConvergenceException.class)\n+    public void testCircleFittingBadInit() {\n+        /*\n+         * This test does not converge with this optimizer.\n+         */\n+        super.testCircleFittingBadInit();\n+    }\n+\n+    @Override\n+    @Test(expected=ConvergenceException.class)\n+    public void testHahn1()\n+        throws IOException {\n+        /*\n+         * TODO This test leads to a singular problem with the Gauss-Newton\n+         * optimizer. This should be inquired.\n+         */\n+        super.testHahn1();\n+    }\n+}\n--- /dev/null\n+++ b/src/test/java/org/apache/commons/math3/fitting/leastsquares/LevenbergMarquardtOptimizerTest.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.commons.math3.optim.PointVectorValuePair;\n+import org.apache.commons.math3.analysis.MultivariateVectorFunction;\n+import org.apache.commons.math3.analysis.MultivariateMatrixFunction;\n+import org.apache.commons.math3.exception.ConvergenceException;\n+import org.apache.commons.math3.exception.DimensionMismatchException;\n+import org.apache.commons.math3.exception.TooManyEvaluationsException;\n+import org.apache.commons.math3.exception.MathUnsupportedOperationException;\n+import org.apache.commons.math3.geometry.euclidean.twod.Vector2D;\n+import org.apache.commons.math3.linear.SingularMatrixException;\n+import org.apache.commons.math3.linear.DiagonalMatrix;\n+import org.apache.commons.math3.util.FastMath;\n+import org.apache.commons.math3.util.Precision;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.junit.Ignore;\n+\n+/**\n+ * <p>Some of the unit tests are re-implementations of the MINPACK <a\n+ * href=\"http://www.netlib.org/minpack/ex/file17\">file17</a> and <a\n+ * href=\"http://www.netlib.org/minpack/ex/file22\">file22</a> test files.\n+ * The redistribution policy for MINPACK is available <a\n+ * href=\"http://www.netlib.org/minpack/disclaimer\">here</a>.\n+ *\n+ * @version $Id$\n+ */\n+public class LevenbergMarquardtOptimizerTest\n+    extends AbstractLeastSquaresOptimizerAbstractTest<LevenbergMarquardtOptimizer> {\n+    @Override\n+    public LevenbergMarquardtOptimizer createOptimizer() {\n+        return LevenbergMarquardtOptimizer.create();\n+    }\n+\n+    @Override\n+    public int getMaxIterations() {\n+        return 25;\n+    }\n+\n+    @Override\n+    @Test(expected=SingularMatrixException.class)\n+    public void testNonInvertible() {\n+        /*\n+         * Overrides the method from parent class, since the default singularity\n+         * threshold (1e-14) does not trigger the expected exception.\n+         */\n+        LinearProblem problem = new LinearProblem(new double[][] {\n+                {  1, 2, -3 },\n+                {  2, 1,  3 },\n+                { -3, 0, -9 }\n+        }, new double[] { 1, 1, 1 });\n+\n+        final LevenbergMarquardtOptimizer optimizer = createOptimizer()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(20)\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(problem.getTarget())\n+            .withWeight(new DiagonalMatrix(new double[] { 1, 1, 1 }))\n+            .withStartPoint(new double[] { 0, 0, 0 });\n+\n+        final double[] optimum = optimizer.optimize().getPoint();\n+        Assert.assertTrue(FastMath.sqrt(optimizer.getTarget().length) * optimizer.computeRMS(optimum) > 0.6);\n+\n+        optimizer.computeCovariances(optimum, 1.5e-14);\n+    }\n+\n+    @Test\n+    public void testControlParameters() {\n+        CircleVectorial circle = new CircleVectorial();\n+        circle.addPoint( 30.0,  68.0);\n+        circle.addPoint( 50.0,  -6.0);\n+        circle.addPoint(110.0, -20.0);\n+        circle.addPoint( 35.0,  15.0);\n+        circle.addPoint( 45.0,  97.0);\n+        checkEstimate(circle.getModelFunction(),\n+                      circle.getModelFunctionJacobian(),\n+                      0.1, 10, 1.0e-14, 1.0e-16, 1.0e-10, false);\n+        checkEstimate(circle.getModelFunction(),\n+                      circle.getModelFunctionJacobian(),\n+                      0.1, 10, 1.0e-15, 1.0e-17, 1.0e-10, true);\n+        checkEstimate(circle.getModelFunction(),\n+                      circle.getModelFunctionJacobian(),\n+                      0.1,  5, 1.0e-15, 1.0e-16, 1.0e-10, true);\n+        circle.addPoint(300, -300);\n+        checkEstimate(circle.getModelFunction(),\n+                      circle.getModelFunctionJacobian(),\n+                      0.1, 20, 1.0e-18, 1.0e-16, 1.0e-10, true);\n+    }\n+\n+    private void checkEstimate(MultivariateVectorFunction problem,\n+                               MultivariateMatrixFunction problemJacobian,\n+                               double initialStepBoundFactor, int maxCostEval,\n+                               double costRelativeTolerance, double parRelativeTolerance,\n+                               double orthoTolerance, boolean shouldFail) {\n+        try {\n+            final LevenbergMarquardtOptimizer optimizer = LevenbergMarquardtOptimizer.create()\n+                .withTuningParameters(initialStepBoundFactor,\n+                                      costRelativeTolerance,\n+                                      parRelativeTolerance,\n+                                      orthoTolerance,\n+                                      Precision.SAFE_MIN)\n+                .withMaxEvaluations(maxCostEval)\n+                .withMaxIterations(100)\n+                .withModelAndJacobian(problem, problemJacobian)\n+                .withTarget(new double[] { 0, 0, 0, 0, 0 })\n+                .withWeight(new DiagonalMatrix(new double[] { 1, 1, 1, 1, 1 }))\n+                .withStartPoint(new double[] { 98.680, 47.345 });\n+\n+            optimizer.optimize();\n+\n+            Assert.assertTrue(!shouldFail);\n+        } catch (DimensionMismatchException ee) {\n+            Assert.assertTrue(shouldFail);\n+        } catch (TooManyEvaluationsException ee) {\n+            Assert.assertTrue(shouldFail);\n+        }\n+    }\n+\n+    /**\n+     * Non-linear test case: fitting of decay curve (from Chapter 8 of\n+     * Bevington's textbook, \"Data reduction and analysis for the physical sciences\").\n+     * XXX The expected (\"reference\") values may not be accurate and the tolerance too\n+     * relaxed for this test to be currently really useful (the issue is under\n+     * investigation).\n+     */\n+    @Test\n+    public void testBevington() {\n+        final double[][] dataPoints = {\n+            // column 1 = times\n+            { 15, 30, 45, 60, 75, 90, 105, 120, 135, 150,\n+              165, 180, 195, 210, 225, 240, 255, 270, 285, 300,\n+              315, 330, 345, 360, 375, 390, 405, 420, 435, 450,\n+              465, 480, 495, 510, 525, 540, 555, 570, 585, 600,\n+              615, 630, 645, 660, 675, 690, 705, 720, 735, 750,\n+              765, 780, 795, 810, 825, 840, 855, 870, 885, },\n+            // column 2 = measured counts\n+            { 775, 479, 380, 302, 185, 157, 137, 119, 110, 89,\n+              74, 61, 66, 68, 48, 54, 51, 46, 55, 29,\n+              28, 37, 49, 26, 35, 29, 31, 24, 25, 35,\n+              24, 30, 26, 28, 21, 18, 20, 27, 17, 17,\n+              14, 17, 24, 11, 22, 17, 12, 10, 13, 16,\n+              9, 9, 14, 21, 17, 13, 12, 18, 10, },\n+        };\n+\n+        final BevingtonProblem problem = new BevingtonProblem();\n+\n+        final int len = dataPoints[0].length;\n+        final double[] weights = new double[len];\n+        for (int i = 0; i < len; i++) {\n+            problem.addPoint(dataPoints[0][i],\n+                             dataPoints[1][i]);\n+\n+            weights[i] = 1 / dataPoints[1][i];\n+        }\n+\n+        final LevenbergMarquardtOptimizer optimizer = LevenbergMarquardtOptimizer.create()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(20)\n+            .withModelAndJacobian(problem.getModelFunction(),\n+                                  problem.getModelFunctionJacobian())\n+            .withTarget(dataPoints[1])\n+            .withWeight(new DiagonalMatrix(weights))\n+            .withStartPoint(new double[] { 10, 900, 80, 27, 225 });\n+\n+        final PointVectorValuePair optimum = optimizer.optimize();\n+        final double[] solution = optimum.getPoint();\n+        final double[] expectedSolution = { 10.4, 958.3, 131.4, 33.9, 205.0 };\n+\n+        final double[][] covarMatrix = optimizer.computeCovariances(solution, 1e-14);\n+        final double[][] expectedCovarMatrix = {\n+            { 3.38, -3.69, 27.98, -2.34, -49.24 },\n+            { -3.69, 2492.26, 81.89, -69.21, -8.9 },\n+            { 27.98, 81.89, 468.99, -44.22, -615.44 },\n+            { -2.34, -69.21, -44.22, 6.39, 53.80 },\n+            { -49.24, -8.9, -615.44, 53.8, 929.45 }\n+        };\n+\n+        final int numParams = expectedSolution.length;\n+\n+        // Check that the computed solution is within the reference error range.\n+        for (int i = 0; i < numParams; i++) {\n+            final double error = FastMath.sqrt(expectedCovarMatrix[i][i]);\n+            Assert.assertEquals(\"Parameter \" + i, expectedSolution[i], solution[i], error);\n+        }\n+\n+        // Check that each entry of the computed covariance matrix is within 10%\n+        // of the reference matrix entry.\n+        for (int i = 0; i < numParams; i++) {\n+            for (int j = 0; j < numParams; j++) {\n+                Assert.assertEquals(\"Covariance matrix [\" + i + \"][\" + j + \"]\",\n+                                    expectedCovarMatrix[i][j],\n+                                    covarMatrix[i][j],\n+                                    FastMath.abs(0.1 * expectedCovarMatrix[i][j]));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void testCircleFitting2() {\n+        final double xCenter = 123.456;\n+        final double yCenter = 654.321;\n+        final double xSigma = 10;\n+        final double ySigma = 15;\n+        final double radius = 111.111;\n+        // The test is extremely sensitive to the seed.\n+        final long seed = 59421061L;\n+        final RandomCirclePointGenerator factory\n+            = new RandomCirclePointGenerator(xCenter, yCenter, radius,\n+                                             xSigma, ySigma,\n+                                             seed);\n+        final CircleProblem circle = new CircleProblem(xSigma, ySigma);\n+\n+        final int numPoints = 10;\n+        for (Vector2D p : factory.generate(numPoints)) {\n+            circle.addPoint(p.getX(), p.getY());\n+        }\n+\n+        // First guess for the center's coordinates and radius.\n+        final double[] init = { 90, 659, 115 };\n+\n+        final LevenbergMarquardtOptimizer optimizer = LevenbergMarquardtOptimizer.create()\n+            .withMaxEvaluations(100)\n+            .withMaxIterations(50)\n+            .withModelAndJacobian(circle.getModelFunction(),\n+                                  circle.getModelFunctionJacobian())\n+            .withTarget(circle.target())\n+            .withWeight(new DiagonalMatrix(circle.weight()))\n+            .withStartPoint(init);\n+\n+        final PointVectorValuePair optimum = optimizer.optimize();\n+        final double[] paramFound = optimum.getPoint();\n+\n+        // Retrieve errors estimation.\n+        final double[] asymptoticStandardErrorFound = optimizer.computeSigma(paramFound, 1e-14);\n+\n+        // Check that the parameters are found within the assumed error bars.\n+        Assert.assertEquals(xCenter, paramFound[0], asymptoticStandardErrorFound[0]);\n+        Assert.assertEquals(yCenter, paramFound[1], asymptoticStandardErrorFound[1]);\n+        Assert.assertEquals(radius, paramFound[2], asymptoticStandardErrorFound[2]);\n+    }\n+\n+    private static class QuadraticProblem {\n+        private List<Double> x;\n+        private List<Double> y;\n+\n+        public QuadraticProblem() {\n+            x = new ArrayList<Double>();\n+            y = new ArrayList<Double>();\n+        }\n+\n+        public void addPoint(double x, double y) {\n+            this.x.add(x);\n+            this.y.add(y);\n+        }\n+\n+        public MultivariateVectorFunction getModelFunction() {\n+            return new MultivariateVectorFunction() {\n+                public double[] value(double[] variables) {\n+                    double[] values = new double[x.size()];\n+                    for (int i = 0; i < values.length; ++i) {\n+                        values[i] = (variables[0] * x.get(i) + variables[1]) * x.get(i) + variables[2];\n+                    }\n+                    return values;\n+                }\n+            };\n+        }\n+\n+        public MultivariateMatrixFunction getModelFunctionJacobian() {\n+            return new MultivariateMatrixFunction() {\n+                public double[][] value(double[] params) {                    \n+                    double[][] jacobian = new double[x.size()][3];\n+                    for (int i = 0; i < jacobian.length; ++i) {\n+                        jacobian[i][0] = x.get(i) * x.get(i);\n+                        jacobian[i][1] = x.get(i);\n+                        jacobian[i][2] = 1.0;\n+                    }\n+                    return jacobian;\n+                }\n+            };\n+        }\n+    }\n+\n+    private static class BevingtonProblem {\n+        private List<Double> time;\n+        private List<Double> count;\n+\n+        public BevingtonProblem() {\n+            time = new ArrayList<Double>();\n+            count = new ArrayList<Double>();\n+        }\n+\n+        public void addPoint(double t, double c) {\n+            time.add(t);\n+            count.add(c);\n+        }\n+\n+        public MultivariateVectorFunction getModelFunction() {\n+            return new MultivariateVectorFunction() {\n+                public double[] value(double[] params) {\n+                    double[] values = new double[time.size()];\n+                    for (int i = 0; i < values.length; ++i) {\n+                        final double t = time.get(i);\n+                        values[i] = params[0] +\n+                            params[1] * Math.exp(-t / params[3]) +\n+                            params[2] * Math.exp(-t / params[4]);\n+                    }\n+                    return values;\n+                }\n+            };\n+        }\n+\n+        public MultivariateMatrixFunction getModelFunctionJacobian() {\n+            return new MultivariateMatrixFunction() {\n+                public double[][] value(double[] params) {\n+                    double[][] jacobian = new double[time.size()][5];\n+\n+                    for (int i = 0; i < jacobian.length; ++i) {\n+                        final double t = time.get(i);\n+                        jacobian[i][0] = 1;\n+\n+                        final double p3 =  params[3];\n+                        final double p4 =  params[4];\n+                        final double tOp3 = t / p3;\n+                        final double tOp4 = t / p4;\n+                        jacobian[i][1] = Math.exp(-tOp3);\n+                        jacobian[i][2] = Math.exp(-tOp4);\n+                        jacobian[i][3] = params[1] * Math.exp(-tOp3) * tOp3 / p3;\n+                        jacobian[i][4] = params[2] * Math.exp(-tOp4) * tOp4 / p4;\n+                    }\n+                    return jacobian;\n+                }\n+            };\n+        }\n+    }\n+}\n--- /dev/null\n+++ b/src/test/java/org/apache/commons/math3/fitting/leastsquares/MinpackTest.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import org.apache.commons.math3.exception.TooManyEvaluationsException;\n+import org.apache.commons.math3.analysis.MultivariateVectorFunction;\n+import org.apache.commons.math3.analysis.MultivariateMatrixFunction;\n+import org.apache.commons.math3.optim.PointVectorValuePair;\n+import org.apache.commons.math3.linear.DiagonalMatrix;\n+import org.apache.commons.math3.util.FastMath;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+/**\n+ * <p>Some of the unit tests are re-implementations of the MINPACK <a\n+ * href=\"http://www.netlib.org/minpack/ex/file17\">file17</a> and <a\n+ * href=\"http://www.netlib.org/minpack/ex/file22\">file22</a> test files.\n+ * The redistribution policy for MINPACK is available <a\n+ * href=\"http://www.netlib.org/minpack/disclaimer\">here</a>, for\n+ * convenience, it is reproduced below.</p>\n+\n+ * <table border=\"0\" width=\"80%\" cellpadding=\"10\" align=\"center\" bgcolor=\"#E0E0E0\">\n+ * <tr><td>\n+ *    Minpack Copyright Notice (1999) University of Chicago.\n+ *    All rights reserved\n+ * </td></tr>\n+ * <tr><td>\n+ * Redistribution and use in source and binary forms, with or without\n+ * modification, are permitted provided that the following conditions\n+ * are met:\n+ * <ol>\n+ *  <li>Redistributions of source code must retain the above copyright\n+ *      notice, this list of conditions and the following disclaimer.</li>\n+ * <li>Redistributions in binary form must reproduce the above\n+ *     copyright notice, this list of conditions and the following\n+ *     disclaimer in the documentation and/or other materials provided\n+ *     with the distribution.</li>\n+ * <li>The end-user documentation included with the redistribution, if any,\n+ *     must include the following acknowledgment:\n+ *     <code>This product includes software developed by the University of\n+ *           Chicago, as Operator of Argonne National Laboratory.</code>\n+ *     Alternately, this acknowledgment may appear in the software itself,\n+ *     if and wherever such third-party acknowledgments normally appear.</li>\n+ * <li><strong>WARRANTY DISCLAIMER. THE SOFTWARE IS SUPPLIED \"AS IS\"\n+ *     WITHOUT WARRANTY OF ANY KIND. THE COPYRIGHT HOLDER, THE\n+ *     UNITED STATES, THE UNITED STATES DEPARTMENT OF ENERGY, AND\n+ *     THEIR EMPLOYEES: (1) DISCLAIM ANY WARRANTIES, EXPRESS OR\n+ *     IMPLIED, INCLUDING BUT NOT LIMITED TO ANY IMPLIED WARRANTIES\n+ *     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE\n+ *     OR NON-INFRINGEMENT, (2) DO NOT ASSUME ANY LEGAL LIABILITY\n+ *     OR RESPONSIBILITY FOR THE ACCURACY, COMPLETENESS, OR\n+ *     USEFULNESS OF THE SOFTWARE, (3) DO NOT REPRESENT THAT USE OF\n+ *     THE SOFTWARE WOULD NOT INFRINGE PRIVATELY OWNED RIGHTS, (4)\n+ *     DO NOT WARRANT THAT THE SOFTWARE WILL FUNCTION\n+ *     UNINTERRUPTED, THAT IT IS ERROR-FREE OR THAT ANY ERRORS WILL\n+ *     BE CORRECTED.</strong></li>\n+ * <li><strong>LIMITATION OF LIABILITY. IN NO EVENT WILL THE COPYRIGHT\n+ *     HOLDER, THE UNITED STATES, THE UNITED STATES DEPARTMENT OF\n+ *     ENERGY, OR THEIR EMPLOYEES: BE LIABLE FOR ANY INDIRECT,\n+ *     INCIDENTAL, CONSEQUENTIAL, SPECIAL OR PUNITIVE DAMAGES OF\n+ *     ANY KIND OR NATURE, INCLUDING BUT NOT LIMITED TO LOSS OF\n+ *     PROFITS OR LOSS OF DATA, FOR ANY REASON WHATSOEVER, WHETHER\n+ *     SUCH LIABILITY IS ASSERTED ON THE BASIS OF CONTRACT, TORT\n+ *     (INCLUDING NEGLIGENCE OR STRICT LIABILITY), OR OTHERWISE,\n+ *     EVEN IF ANY OF SAID PARTIES HAS BEEN WARNED OF THE\n+ *     POSSIBILITY OF SUCH LOSS OR DAMAGES.</strong></li>\n+ * <ol></td></tr>\n+ * </table>\n+\n+ * @author Argonne National Laboratory. MINPACK project. March 1980 (original fortran minpack tests)\n+ * @author Burton S. Garbow (original fortran minpack tests)\n+ * @author Kenneth E. Hillstrom (original fortran minpack tests)\n+ * @author Jorge J. More (original fortran minpack tests)\n+ * @author Luc Maisonobe (non-minpack tests and minpack tests Java translation)\n+ */\n+public class MinpackTest {\n+\n+    @Test\n+    public void testMinpackLinearFullRank() {\n+        minpackTest(new LinearFullRankFunction(10, 5, 1.0,\n+                                               5.0, 2.23606797749979), false);\n+        minpackTest(new LinearFullRankFunction(50, 5, 1.0,\n+                                               8.06225774829855, 6.70820393249937), false);\n+    }\n+\n+    @Test\n+    public void testMinpackLinearRank1() {\n+        minpackTest(new LinearRank1Function(10, 5, 1.0,\n+                                            291.521868819476, 1.4638501094228), false);\n+        minpackTest(new LinearRank1Function(50, 5, 1.0,\n+                                            3101.60039334535, 3.48263016573496), false);\n+    }\n+\n+    @Test\n+    public void testMinpackLinearRank1ZeroColsAndRows() {\n+        minpackTest(new LinearRank1ZeroColsAndRowsFunction(10, 5, 1.0), false);\n+        minpackTest(new LinearRank1ZeroColsAndRowsFunction(50, 5, 1.0), false);\n+    }\n+\n+    @Test\n+    public void testMinpackRosenbrok() {\n+        minpackTest(new RosenbrockFunction(new double[] { -1.2, 1.0 },\n+                                           FastMath.sqrt(24.2)), false);\n+        minpackTest(new RosenbrockFunction(new double[] { -12.0, 10.0 },\n+                                           FastMath.sqrt(1795769.0)), false);\n+        minpackTest(new RosenbrockFunction(new double[] { -120.0, 100.0 },\n+                                           11.0 * FastMath.sqrt(169000121.0)), false);\n+    }\n+\n+    @Test\n+    public void testMinpackHelicalValley() {\n+        minpackTest(new HelicalValleyFunction(new double[] { -1.0, 0.0, 0.0 },\n+                                              50.0), false);\n+        minpackTest(new HelicalValleyFunction(new double[] { -10.0, 0.0, 0.0 },\n+                                              102.95630140987), false);\n+        minpackTest(new HelicalValleyFunction(new double[] { -100.0, 0.0, 0.0},\n+                                              991.261822123701), false);\n+    }\n+\n+    @Test\n+    public void testMinpackPowellSingular() {\n+        minpackTest(new PowellSingularFunction(new double[] { 3.0, -1.0, 0.0, 1.0 },\n+                                               14.6628782986152), false);\n+        minpackTest(new PowellSingularFunction(new double[] { 30.0, -10.0, 0.0, 10.0 },\n+                                               1270.9838708654), false);\n+        minpackTest(new PowellSingularFunction(new double[] { 300.0, -100.0, 0.0, 100.0 },\n+                                               126887.903284750), false);\n+    }\n+\n+    @Test\n+    public void testMinpackFreudensteinRoth() {\n+        minpackTest(new FreudensteinRothFunction(new double[] { 0.5, -2.0 },\n+                                                 20.0124960961895, 6.99887517584575,\n+                                                 new double[] {\n+                                                     11.4124844654993,\n+                                                     -0.896827913731509\n+                                                 }), false);\n+        minpackTest(new FreudensteinRothFunction(new double[] { 5.0, -20.0 },\n+                                                 12432.833948863, 6.9988751744895,\n+                                                 new double[] {\n+                                                     11.41300466147456,\n+                                                     -0.896796038685959\n+                                                 }), false);\n+        minpackTest(new FreudensteinRothFunction(new double[] { 50.0, -200.0 },\n+                                                 11426454.595762, 6.99887517242903,\n+                                                 new double[] {\n+                                                     11.412781785788564,\n+                                                     -0.8968051074920405\n+                                                 }), false);\n+    }\n+\n+    @Test\n+    public void testMinpackBard() {\n+        minpackTest(new BardFunction(1.0, 6.45613629515967, 0.0906359603390466,\n+                                     new double[] {\n+                                         0.0824105765758334,\n+                                         1.1330366534715,\n+                                         2.34369463894115\n+                                     }), false);\n+        minpackTest(new BardFunction(10.0, 36.1418531596785, 4.17476870138539,\n+                                     new double[] {\n+                                         0.840666673818329,\n+                                         -158848033.259565,\n+                                         -164378671.653535\n+                                     }), false);\n+        minpackTest(new BardFunction(100.0, 384.114678637399, 4.17476870135969,\n+                                     new double[] {\n+                                         0.840666673867645,\n+                                         -158946167.205518,\n+                                         -164464906.857771\n+                                     }), false);\n+    }\n+\n+    @Test\n+    public void testMinpackKowalikOsborne() {\n+        minpackTest(new KowalikOsborneFunction(new double[] { 0.25, 0.39, 0.415, 0.39 },\n+                                               0.0728915102882945,\n+                                               0.017535837721129,\n+                                               new double[] {\n+                                                   0.192807810476249,\n+                                                   0.191262653354071,\n+                                                   0.123052801046931,\n+                                                   0.136053221150517\n+                                               }), false);\n+        minpackTest(new KowalikOsborneFunction(new double[] { 2.5, 3.9, 4.15, 3.9 },\n+                                               2.97937007555202,\n+                                               0.032052192917937,\n+                                               new double[] {\n+                                                   728675.473768287,\n+                                                   -14.0758803129393,\n+                                                   -32977797.7841797,\n+                                                   -20571594.1977912\n+                                               }), false);\n+        minpackTest(new KowalikOsborneFunction(new double[] { 25.0, 39.0, 41.5, 39.0 },\n+                                               29.9590617016037,\n+                                               0.0175364017658228,\n+                                               new double[] {\n+                                                   0.192948328597594,\n+                                                   0.188053165007911,\n+                                                   0.122430604321144,\n+                                                   0.134575665392506\n+                                               }), false);\n+    }\n+\n+    @Test\n+    public void testMinpackMeyer() {\n+        minpackTest(new MeyerFunction(new double[] { 0.02, 4000.0, 250.0 },\n+                                      41153.4665543031, 9.37794514651874,\n+                                      new double[] {\n+                                          0.00560963647102661,\n+                                          6181.34634628659,\n+                                          345.223634624144\n+                                      }), false);\n+        minpackTest(new MeyerFunction(new double[] { 0.2, 40000.0, 2500.0 },\n+                                      4168216.89130846, 792.917871779501,\n+                                      new double[] {\n+                                          1.42367074157994e-11,\n+                                          33695.7133432541,\n+                                          901.268527953801\n+                                      }), true);\n+    }\n+\n+    @Test\n+    public void testMinpackWatson() {\n+        minpackTest(new WatsonFunction(6, 0.0,\n+                                       5.47722557505166, 0.0478295939097601,\n+                                       new double[] {\n+                                           -0.0157249615083782, 1.01243488232965,\n+                                           -0.232991722387673,  1.26043101102818,\n+                                           -1.51373031394421,   0.99299727291842\n+                                       }), false);\n+        minpackTest(new WatsonFunction(6, 10.0,\n+                                       6433.12578950026, 0.0478295939096951,\n+                                       new double[] {\n+                                           -0.0157251901386677, 1.01243485860105,\n+                                           -0.232991545843829,  1.26042932089163,\n+                                           -1.51372776706575,   0.99299573426328\n+                                       }), false);\n+        minpackTest(new WatsonFunction(6, 100.0,\n+                                       674256.040605213, 0.047829593911544,\n+                                       new double[] {\n+                                           -0.0157247019712586, 1.01243490925658,\n+                                           -0.232991922761641,  1.26043292929555,\n+                                           -1.51373320452707,   0.99299901922322\n+                                       }), false);\n+        minpackTest(new WatsonFunction(9, 0.0,\n+                                       5.47722557505166, 0.00118311459212420,\n+                                       new double[] {\n+                                           -0.153070644166722e-4, 0.999789703934597,\n+                                           0.0147639634910978,   0.146342330145992,\n+                                           1.00082109454817,    -2.61773112070507,\n+                                           4.10440313943354,    -3.14361226236241,\n+                                           1.05262640378759\n+                                       }), false);\n+        minpackTest(new WatsonFunction(9, 10.0,\n+                                       12088.127069307, 0.00118311459212513,\n+                                       new double[] {\n+                                           -0.153071334849279e-4, 0.999789703941234,\n+                                           0.0147639629786217,   0.146342334818836,\n+                                           1.00082107321386,    -2.61773107084722,\n+                                           4.10440307655564,    -3.14361222178686,\n+                                           1.05262639322589\n+                                       }), false);\n+        minpackTest(new WatsonFunction(9, 100.0,\n+                                       1269109.29043834, 0.00118311459212384,\n+                                       new double[] {\n+                                           -0.153069523352176e-4, 0.999789703958371,\n+                                           0.0147639625185392,   0.146342341096326,\n+                                           1.00082104729164,    -2.61773101573645,\n+                                           4.10440301427286,    -3.14361218602503,\n+                                           1.05262638516774\n+                                       }), false);\n+        minpackTest(new WatsonFunction(12, 0.0,\n+                                       5.47722557505166, 0.217310402535861e-4,\n+                                       new double[] {\n+                                           -0.660266001396382e-8, 1.00000164411833,\n+                                           -0.000563932146980154, 0.347820540050756,\n+                                           -0.156731500244233,    1.05281515825593,\n+                                           -3.24727109519451,     7.2884347837505,\n+                                           -10.271848098614,       9.07411353715783,\n+                                           -4.54137541918194,     1.01201187975044\n+                                       }), false);\n+        minpackTest(new WatsonFunction(12, 10.0,\n+                                       19220.7589790951, 0.217310402518509e-4,\n+                                       new double[] {\n+                                           -0.663710223017410e-8, 1.00000164411787,\n+                                           -0.000563932208347327, 0.347820540486998,\n+                                           -0.156731503955652,    1.05281517654573,\n+                                           -3.2472711515214,      7.28843489430665,\n+                                           -10.2718482369638,      9.07411364383733,\n+                                           -4.54137546533666,     1.01201188830857\n+                                       }), false);\n+        minpackTest(new WatsonFunction(12, 100.0,\n+                                       2018918.04462367, 0.217310402539845e-4,\n+                                       new double[] {\n+                                           -0.663806046485249e-8, 1.00000164411786,\n+                                           -0.000563932210324959, 0.347820540503588,\n+                                           -0.156731504091375,    1.05281517718031,\n+                                           -3.24727115337025,     7.28843489775302,\n+                                           -10.2718482410813,      9.07411364688464,\n+                                           -4.54137546660822,     1.0120118885369\n+                                       }), false);\n+    }\n+\n+    @Test\n+    public void testMinpackBox3Dimensional() {\n+        minpackTest(new Box3DimensionalFunction(10, new double[] { 0.0, 10.0, 20.0 },\n+                                                32.1115837449572), false);\n+    }\n+\n+    @Test\n+    public void testMinpackJennrichSampson() {\n+        minpackTest(new JennrichSampsonFunction(10, new double[] { 0.3, 0.4 },\n+                                                64.5856498144943, 11.1517793413499,\n+                                                new double[] {\n+//                                                     0.2578330049, 0.257829976764542\n+                                                    0.2578199266368004, 0.25782997676455244\n+                                                }), false);\n+    }\n+\n+    @Test\n+    public void testMinpackBrownDennis() {\n+        minpackTest(new BrownDennisFunction(20,\n+                                            new double[] { 25.0, 5.0, -5.0, -1.0 },\n+                                            2815.43839161816, 292.954288244866,\n+                                            new double[] {\n+                                                -11.59125141003, 13.2024883984741,\n+                                                -0.403574643314272, 0.236736269844604\n+                                            }), false);\n+        minpackTest(new BrownDennisFunction(20,\n+                                            new double[] { 250.0, 50.0, -50.0, -10.0 },\n+                                            555073.354173069, 292.954270581415,\n+                                            new double[] {\n+                                                -11.5959274272203, 13.2041866926242,\n+                                                -0.403417362841545, 0.236771143410386\n+                                            }), false);\n+        minpackTest(new BrownDennisFunction(20,\n+                                            new double[] { 2500.0, 500.0, -500.0, -100.0 },\n+                                            61211252.2338581, 292.954306151134,\n+                                            new double[] {\n+                                                -11.5902596937374, 13.2020628854665,\n+                                                -0.403688070279258, 0.236665033746463\n+                                            }), false);\n+    }\n+\n+    @Test\n+    public void testMinpackChebyquad() {\n+        minpackTest(new ChebyquadFunction(1, 8, 1.0,\n+                                          1.88623796907732, 1.88623796907732,\n+                                          new double[] { 0.5 }), false);\n+        minpackTest(new ChebyquadFunction(1, 8, 10.0,\n+                                          5383344372.34005, 1.88424820499951,\n+                                          new double[] { 0.9817314924684 }), false);\n+        minpackTest(new ChebyquadFunction(1, 8, 100.0,\n+                                          0.118088726698392e19, 1.88424820499347,\n+                                          new double[] { 0.9817314852934 }), false);\n+        minpackTest(new ChebyquadFunction(8, 8, 1.0,\n+                                          0.196513862833975, 0.0593032355046727,\n+                                          new double[] {\n+                                              0.0431536648587336, 0.193091637843267,\n+                                              0.266328593812698,  0.499999334628884,\n+                                              0.500000665371116,  0.733671406187302,\n+                                              0.806908362156733,  0.956846335141266\n+                                          }), false);\n+        minpackTest(new ChebyquadFunction(9, 9, 1.0,\n+                                          0.16994993465202, 0.0,\n+                                          new double[] {\n+                                              0.0442053461357828, 0.199490672309881,\n+                                              0.23561910847106,   0.416046907892598,\n+                                              0.5,                0.583953092107402,\n+                                              0.764380891528940,  0.800509327690119,\n+                                              0.955794653864217\n+                                          }), false);\n+        minpackTest(new ChebyquadFunction(10, 10, 1.0,\n+                                          0.183747831178711, 0.0806471004038253,\n+                                          new double[] {\n+                                              0.0596202671753563, 0.166708783805937,\n+                                              0.239171018813509,  0.398885290346268,\n+                                              0.398883667870681,  0.601116332129320,\n+                                              0.60111470965373,   0.760828981186491,\n+                                              0.833291216194063,  0.940379732824644\n+                                          }), false);\n+    }\n+\n+    @Test\n+    public void testMinpackBrownAlmostLinear() {\n+        minpackTest(new BrownAlmostLinearFunction(10, 0.5,\n+                                                  16.5302162063499, 0.0,\n+                                                  new double[] {\n+                                                      0.979430303349862, 0.979430303349862,\n+                                                      0.979430303349862, 0.979430303349862,\n+                                                      0.979430303349862, 0.979430303349862,\n+                                                      0.979430303349862, 0.979430303349862,\n+                                                      0.979430303349862, 1.20569696650138\n+                                                  }), false);\n+        minpackTest(new BrownAlmostLinearFunction(10, 5.0,\n+                                                  9765624.00089211, 0.0,\n+                                                  new double[] {\n+                                                      0.979430303349865, 0.979430303349865,\n+                                                      0.979430303349865, 0.979430303349865,\n+                                                      0.979430303349865, 0.979430303349865,\n+                                                      0.979430303349865, 0.979430303349865,\n+                                                      0.979430303349865, 1.20569696650135\n+                                                  }), false);\n+        minpackTest(new BrownAlmostLinearFunction(10, 50.0,\n+                                                  0.9765625e17, 0.0,\n+                                                  new double[] {\n+                                                      1.0, 1.0, 1.0, 1.0, 1.0,\n+                                                      1.0, 1.0, 1.0, 1.0, 1.0\n+                                                  }), false);\n+        minpackTest(new BrownAlmostLinearFunction(30, 0.5,\n+                                                  83.476044467848, 0.0,\n+                                                  new double[] {\n+                                                      0.997754216442807, 0.997754216442807,\n+                                                      0.997754216442807, 0.997754216442807,\n+                                                      0.997754216442807, 0.997754216442807,\n+                                                      0.997754216442807, 0.997754216442807,\n+                                                      0.997754216442807, 0.997754216442807,\n+                                                      0.997754216442807, 0.997754216442807,\n+                                                      0.997754216442807, 0.997754216442807,\n+                                                      0.997754216442807, 0.997754216442807,\n+                                                      0.997754216442807, 0.997754216442807,\n+                                                      0.997754216442807, 0.997754216442807,\n+                                                      0.997754216442807, 0.997754216442807,\n+                                                      0.997754216442807, 0.997754216442807,\n+                                                      0.997754216442807, 0.997754216442807,\n+                                                      0.997754216442807, 0.997754216442807,\n+                                                      0.997754216442807, 1.06737350671578\n+                                                  }), false);\n+        minpackTest(new BrownAlmostLinearFunction(40, 0.5,\n+                                                  128.026364472323, 0.0,\n+                                                  new double[] {\n+                                                      1.00000000000002, 1.00000000000002,\n+                                                      1.00000000000002, 1.00000000000002,\n+                                                      1.00000000000002, 1.00000000000002,\n+                                                      1.00000000000002, 1.00000000000002,\n+                                                      1.00000000000002, 1.00000000000002,\n+                                                      1.00000000000002, 1.00000000000002,\n+                                                      1.00000000000002, 1.00000000000002,\n+                                                      1.00000000000002, 1.00000000000002,\n+                                                      1.00000000000002, 1.00000000000002,\n+                                                      1.00000000000002, 1.00000000000002,\n+                                                      1.00000000000002, 1.00000000000002,\n+                                                      1.00000000000002, 1.00000000000002,\n+                                                      1.00000000000002, 1.00000000000002,\n+                                                      1.00000000000002, 1.00000000000002,\n+                                                      1.00000000000002, 1.00000000000002,\n+                                                      1.00000000000002, 1.00000000000002,\n+                                                      1.00000000000002, 1.00000000000002,\n+                                                      0.999999999999121\n+                                                  }), false);\n+    }\n+\n+    @Test\n+    public void testMinpackOsborne1() {\n+        minpackTest(new Osborne1Function(new double[] { 0.5, 1.5, -1.0, 0.01, 0.02, },\n+                                         0.937564021037838, 0.00739249260904843,\n+                                         new double[] {\n+                                             0.375410049244025, 1.93584654543108,\n+                                             -1.46468676748716, 0.0128675339110439,\n+                                             0.0221227011813076\n+                                         }), false);\n+    }\n+\n+    @Test\n+    public void testMinpackOsborne2() {\n+        minpackTest(new Osborne2Function(new double[] {\n+                    1.3, 0.65, 0.65, 0.7, 0.6,\n+                    3.0, 5.0, 7.0, 2.0, 4.5, 5.5\n+                },\n+                1.44686540984712, 0.20034404483314,\n+                new double[] {\n+                    1.30997663810096,  0.43155248076,\n+                    0.633661261602859, 0.599428560991695,\n+                    0.754179768272449, 0.904300082378518,\n+                    1.36579949521007, 4.82373199748107,\n+                    2.39868475104871, 4.56887554791452,\n+                    5.67534206273052\n+                }), false);\n+    }\n+\n+    private void minpackTest(MinpackFunction function, boolean exceptionExpected) {\n+        final double tol = 2.22044604926e-16;\n+        LevenbergMarquardtOptimizer optimizer = LevenbergMarquardtOptimizer.create();\n+        optimizer = optimizer\n+            .withTuningParameters(optimizer.getInitialStepBoundFactor(),\n+                                  FastMath.sqrt(tol),\n+                                  FastMath.sqrt(tol),\n+                                  tol,\n+                                  optimizer.getRankingThreshold())\n+            .withMaxEvaluations(400 * (function.getN() + 1))\n+            .withMaxIterations(2000)\n+            .withModelAndJacobian(function.getModelFunction(),\n+                                  function.getModelFunctionJacobian())\n+            .withTarget(function.getTarget())\n+            .withWeight(new DiagonalMatrix(function.getWeight()))\n+            .withStartPoint(function.getStartPoint());\n+\n+        try {\n+            final double[] optimum = optimizer.optimize().getPoint();\n+            Assert.assertFalse(exceptionExpected);\n+            function.checkTheoreticalMinCost(optimizer.computeRMS(optimum));\n+            function.checkTheoreticalMinParams(optimum);\n+        } catch (TooManyEvaluationsException e) {\n+            Assert.assertTrue(exceptionExpected);\n+        }\n+    }\n+\n+    private static abstract class MinpackFunction {\n+        protected int      n;\n+        protected int      m;\n+        protected double[] startParams;\n+        protected double   theoreticalMinCost;\n+        protected double[] theoreticalMinParams;\n+        protected double   costAccuracy;\n+        protected double   paramsAccuracy;\n+\n+        protected MinpackFunction(int m, double[] startParams,\n+                                  double theoreticalMinCost,\n+                                  double[] theoreticalMinParams) {\n+            this.m = m;\n+            this.n = startParams.length;\n+            this.startParams          = startParams.clone();\n+            this.theoreticalMinCost   = theoreticalMinCost;\n+            this.theoreticalMinParams = theoreticalMinParams;\n+            this.costAccuracy         = 1.0e-8;\n+            this.paramsAccuracy       = 1.0e-5;\n+        }\n+\n+        protected static double[] buildArray(int n, double x) {\n+            double[] array = new double[n];\n+            Arrays.fill(array, x);\n+            return array;\n+        }\n+\n+        public double[] getTarget() {\n+            return buildArray(m, 0.0);\n+        }\n+\n+        public double[] getWeight() {\n+            return buildArray(m, 1.0);\n+        }\n+\n+        public double[] getStartPoint() {\n+            return startParams.clone();\n+        }\n+\n+        protected void setCostAccuracy(double costAccuracy) {\n+            this.costAccuracy = costAccuracy;\n+        }\n+\n+        protected void setParamsAccuracy(double paramsAccuracy) {\n+            this.paramsAccuracy = paramsAccuracy;\n+        }\n+\n+        public int getN() {\n+            return startParams.length;\n+        }\n+\n+        public void checkTheoreticalMinCost(double rms) {\n+            double threshold = costAccuracy * (1.0 + theoreticalMinCost);\n+            Assert.assertEquals(theoreticalMinCost, FastMath.sqrt(m) * rms, threshold);\n+        }\n+\n+        public void checkTheoreticalMinParams(double[] params) {\n+            if (theoreticalMinParams != null) {\n+                for (int i = 0; i < theoreticalMinParams.length; ++i) {\n+                    double mi = theoreticalMinParams[i];\n+                    double vi = params[i];\n+                    Assert.assertEquals(mi, vi, paramsAccuracy * (1.0 + FastMath.abs(mi)));\n+                }\n+            }\n+        }\n+\n+        public MultivariateVectorFunction getModelFunction() {\n+            return new MultivariateVectorFunction() {\n+                public double[] value(double[] point) {\n+                    return computeValue(point);\n+                }\n+            };\n+        }\n+\n+        public MultivariateMatrixFunction getModelFunctionJacobian() {\n+            return new MultivariateMatrixFunction() {\n+                public double[][] value(double[] point) {\n+                    return computeJacobian(point);\n+                }\n+            };\n+        }\n+\n+        public abstract double[][] computeJacobian(double[] variables);\n+        public abstract double[] computeValue(double[] variables);\n+    }\n+\n+    private static class LinearFullRankFunction extends MinpackFunction {\n+        private static final long serialVersionUID = -9030323226268039536L;\n+        \n+        public LinearFullRankFunction(int m, int n, double x0,\n+                                      double theoreticalStartCost,\n+                                      double theoreticalMinCost) {\n+            super(m, buildArray(n, x0), theoreticalMinCost,\n+                  buildArray(n, -1.0));\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double t = 2.0 / m;\n+            double[][] jacobian = new double[m][];\n+            for (int i = 0; i < m; ++i) {\n+                jacobian[i] = new double[n];\n+                for (int j = 0; j < n; ++j) {\n+                    jacobian[i][j] = (i == j) ? (1 - t) : -t;\n+                }\n+            }\n+            return jacobian;\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double sum = 0;\n+            for (int i = 0; i < n; ++i) {\n+                sum += variables[i];\n+            }\n+            double t  = 1 + 2 * sum / m;\n+            double[] f = new double[m];\n+            for (int i = 0; i < n; ++i) {\n+                f[i] = variables[i] - t;\n+            }\n+            Arrays.fill(f, n, m, -t);\n+            return f;\n+        }\n+    }\n+\n+    private static class LinearRank1Function extends MinpackFunction {\n+        private static final long serialVersionUID = 8494863245104608300L;\n+\n+        public LinearRank1Function(int m, int n, double x0,\n+                                   double theoreticalStartCost,\n+                                   double theoreticalMinCost) {\n+            super(m, buildArray(n, x0), theoreticalMinCost, null);\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double[][] jacobian = new double[m][];\n+            for (int i = 0; i < m; ++i) {\n+                jacobian[i] = new double[n];\n+                for (int j = 0; j < n; ++j) {\n+                    jacobian[i][j] = (i + 1) * (j + 1);\n+                }\n+            }\n+            return jacobian;\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double[] f = new double[m];\n+            double sum = 0;\n+            for (int i = 0; i < n; ++i) {\n+                sum += (i + 1) * variables[i];\n+            }\n+            for (int i = 0; i < m; ++i) {\n+                f[i] = (i + 1) * sum - 1;\n+            }\n+            return f;\n+        }\n+    }\n+\n+    private static class LinearRank1ZeroColsAndRowsFunction extends MinpackFunction {\n+        private static final long serialVersionUID = -3316653043091995018L;\n+\n+        public LinearRank1ZeroColsAndRowsFunction(int m, int n, double x0) {\n+            super(m, buildArray(n, x0),\n+                  FastMath.sqrt((m * (m + 3) - 6) / (2.0 * (2 * m - 3))),\n+                  null);\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double[][] jacobian = new double[m][];\n+            for (int i = 0; i < m; ++i) {\n+                jacobian[i] = new double[n];\n+                jacobian[i][0] = 0;\n+                for (int j = 1; j < (n - 1); ++j) {\n+                    if (i == 0) {\n+                        jacobian[i][j] = 0;\n+                    } else if (i != (m - 1)) {\n+                        jacobian[i][j] = i * (j + 1);\n+                    } else {\n+                        jacobian[i][j] = 0;\n+                    }\n+                }\n+                jacobian[i][n - 1] = 0;\n+            }\n+            return jacobian;\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double[] f = new double[m];\n+            double sum = 0;\n+            for (int i = 1; i < (n - 1); ++i) {\n+                sum += (i + 1) * variables[i];\n+            }\n+            for (int i = 0; i < (m - 1); ++i) {\n+                f[i] = i * sum - 1;\n+            }\n+            f[m - 1] = -1;\n+            return f;\n+        }\n+    }\n+\n+    private static class RosenbrockFunction extends MinpackFunction {\n+        private static final long serialVersionUID = 2893438180956569134L;\n+        public RosenbrockFunction(double[] startParams, double theoreticalStartCost) {\n+            super(2, startParams, 0.0, buildArray(2, 1.0));\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double x1 = variables[0];\n+            return new double[][] { { -20 * x1, 10 }, { -1, 0 } };\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double x1 = variables[0];\n+            double x2 = variables[1];\n+            return new double[] { 10 * (x2 - x1 * x1), 1 - x1 };\n+        }\n+    }\n+\n+    private static class HelicalValleyFunction extends MinpackFunction {\n+        private static final long serialVersionUID = 220613787843200102L;\n+        public HelicalValleyFunction(double[] startParams,\n+                                     double theoreticalStartCost) {\n+            super(3, startParams, 0.0, new double[] { 1.0, 0.0, 0.0 });\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double x1 = variables[0];\n+            double x2 = variables[1];\n+            double tmpSquare = x1 * x1 + x2 * x2;\n+            double tmp1 = twoPi * tmpSquare;\n+            double tmp2 = FastMath.sqrt(tmpSquare);\n+            return new double[][] {\n+                {  100 * x2 / tmp1, -100 * x1 / tmp1, 10 },\n+                { 10 * x1 / tmp2, 10 * x2 / tmp2, 0 },\n+                { 0, 0, 1 }\n+            };\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double x1 = variables[0];\n+            double x2 = variables[1];\n+            double x3 = variables[2];\n+            double tmp1;\n+            if (x1 == 0) {\n+                tmp1 = (x2 >= 0) ? 0.25 : -0.25;\n+            } else {\n+                tmp1 = FastMath.atan(x2 / x1) / twoPi;\n+                if (x1 < 0) {\n+                    tmp1 += 0.5;\n+                }\n+            }\n+            double tmp2 = FastMath.sqrt(x1 * x1 + x2 * x2);\n+            return new double[] {\n+                10.0 * (x3 - 10 * tmp1),\n+                10.0 * (tmp2 - 1),\n+                x3\n+            };\n+        }\n+\n+        private static final double twoPi = 2.0 * FastMath.PI;\n+    }\n+\n+    private static class PowellSingularFunction extends MinpackFunction {\n+        private static final long serialVersionUID = 7298364171208142405L;\n+\n+        public PowellSingularFunction(double[] startParams,\n+                                      double theoreticalStartCost) {\n+            super(4, startParams, 0.0, buildArray(4, 0.0));\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double x1 = variables[0];\n+            double x2 = variables[1];\n+            double x3 = variables[2];\n+            double x4 = variables[3];\n+            return new double[][] {\n+                { 1, 10, 0, 0 },\n+                { 0, 0, sqrt5, -sqrt5 },\n+                { 0, 2 * (x2 - 2 * x3), -4 * (x2 - 2 * x3), 0 },\n+                { 2 * sqrt10 * (x1 - x4), 0, 0, -2 * sqrt10 * (x1 - x4) }\n+            };\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double x1 = variables[0];\n+            double x2 = variables[1];\n+            double x3 = variables[2];\n+            double x4 = variables[3];\n+            return new double[] {\n+                x1 + 10 * x2,\n+                sqrt5 * (x3 - x4),\n+                (x2 - 2 * x3) * (x2 - 2 * x3),\n+                sqrt10 * (x1 - x4) * (x1 - x4)\n+            };\n+        }\n+\n+        private static final double sqrt5  = FastMath.sqrt( 5.0);\n+        private static final double sqrt10 = FastMath.sqrt(10.0);\n+  }\n+\n+    private static class FreudensteinRothFunction extends MinpackFunction {\n+        private static final long serialVersionUID = 2892404999344244214L;\n+\n+        public FreudensteinRothFunction(double[] startParams,\n+                                        double theoreticalStartCost,\n+                                        double theoreticalMinCost,\n+                                        double[] theoreticalMinParams) {\n+            super(2, startParams, theoreticalMinCost,\n+                  theoreticalMinParams);\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double x2 = variables[1];\n+            return new double[][] {\n+                { 1, x2 * (10 - 3 * x2) -  2 },\n+                { 1, x2 * ( 2 + 3 * x2) - 14, }\n+            };\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double x1 = variables[0];\n+            double x2 = variables[1];\n+            return new double[] {\n+                -13.0 + x1 + ((5.0 - x2) * x2 -  2.0) * x2,\n+                -29.0 + x1 + ((1.0 + x2) * x2 - 14.0) * x2\n+            };\n+        }\n+    }\n+\n+    private static class BardFunction extends MinpackFunction {\n+        private static final long serialVersionUID = 5990442612572087668L;\n+\n+        public BardFunction(double x0,\n+                            double theoreticalStartCost,\n+                            double theoreticalMinCost,\n+                            double[] theoreticalMinParams) {\n+            super(15, buildArray(3, x0), theoreticalMinCost,\n+                  theoreticalMinParams);\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double   x2 = variables[1];\n+            double   x3 = variables[2];\n+            double[][] jacobian = new double[m][];\n+            for (int i = 0; i < m; ++i) {\n+                double tmp1 = i  + 1;\n+                double tmp2 = 15 - i;\n+                double tmp3 = (i <= 7) ? tmp1 : tmp2;\n+                double tmp4 = x2 * tmp2 + x3 * tmp3;\n+                tmp4 *= tmp4;\n+                jacobian[i] = new double[] { -1, tmp1 * tmp2 / tmp4, tmp1 * tmp3 / tmp4 };\n+            }\n+            return jacobian;\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double   x1 = variables[0];\n+            double   x2 = variables[1];\n+            double   x3 = variables[2];\n+            double[] f = new double[m];\n+            for (int i = 0; i < m; ++i) {\n+                double tmp1 = i + 1;\n+                double tmp2 = 15 - i;\n+                double tmp3 = (i <= 7) ? tmp1 : tmp2;\n+                f[i] = y[i] - (x1 + tmp1 / (x2 * tmp2 + x3 * tmp3));\n+            }\n+            return f;\n+        }\n+\n+        private static final double[] y = {\n+            0.14, 0.18, 0.22, 0.25, 0.29,\n+            0.32, 0.35, 0.39, 0.37, 0.58,\n+            0.73, 0.96, 1.34, 2.10, 4.39\n+        };\n+    }\n+\n+    private static class KowalikOsborneFunction extends MinpackFunction {\n+        private static final long serialVersionUID = -4867445739880495801L;\n+\n+        public KowalikOsborneFunction(double[] startParams,\n+                                      double theoreticalStartCost,\n+                                      double theoreticalMinCost,\n+                                      double[] theoreticalMinParams) {\n+            super(11, startParams, theoreticalMinCost,\n+                  theoreticalMinParams);\n+            if (theoreticalStartCost > 20.0) {\n+                setCostAccuracy(2.0e-4);\n+                setParamsAccuracy(5.0e-3);\n+            }\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double   x1 = variables[0];\n+            double   x2 = variables[1];\n+            double   x3 = variables[2];\n+            double   x4 = variables[3];\n+            double[][] jacobian = new double[m][];\n+            for (int i = 0; i < m; ++i) {\n+                double tmp = v[i] * (v[i] + x3) + x4;\n+                double j1  = -v[i] * (v[i] + x2) / tmp;\n+                double j2  = -v[i] * x1 / tmp;\n+                double j3  = j1 * j2;\n+                double j4  = j3 / v[i];\n+                jacobian[i] = new double[] { j1, j2, j3, j4 };\n+            }\n+            return jacobian;\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double x1 = variables[0];\n+            double x2 = variables[1];\n+            double x3 = variables[2];\n+            double x4 = variables[3];\n+            double[] f = new double[m];\n+            for (int i = 0; i < m; ++i) {\n+                f[i] = y[i] - x1 * (v[i] * (v[i] + x2)) / (v[i] * (v[i] + x3) + x4);\n+            }\n+            return f;\n+        }\n+\n+        private static final double[] v = {\n+            4.0, 2.0, 1.0, 0.5, 0.25, 0.167, 0.125, 0.1, 0.0833, 0.0714, 0.0625\n+        };\n+\n+        private static final double[] y = {\n+            0.1957, 0.1947, 0.1735, 0.1600, 0.0844, 0.0627,\n+            0.0456, 0.0342, 0.0323, 0.0235, 0.0246\n+        };\n+    }\n+\n+    private static class MeyerFunction extends MinpackFunction {\n+        private static final long serialVersionUID = -838060619150131027L;\n+\n+        public MeyerFunction(double[] startParams,\n+                             double theoreticalStartCost,\n+                             double theoreticalMinCost,\n+                             double[] theoreticalMinParams) {\n+            super(16, startParams, theoreticalMinCost,\n+                  theoreticalMinParams);\n+            if (theoreticalStartCost > 1.0e6) {\n+                setCostAccuracy(7.0e-3);\n+                setParamsAccuracy(2.0e-2);\n+            }\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double   x1 = variables[0];\n+            double   x2 = variables[1];\n+            double   x3 = variables[2];\n+            double[][] jacobian = new double[m][];\n+            for (int i = 0; i < m; ++i) {\n+                double temp = 5.0 * (i + 1) + 45.0 + x3;\n+                double tmp1 = x2 / temp;\n+                double tmp2 = FastMath.exp(tmp1);\n+                double tmp3 = x1 * tmp2 / temp;\n+                jacobian[i] = new double[] { tmp2, tmp3, -tmp1 * tmp3 };\n+            }\n+            return jacobian;\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double x1 = variables[0];\n+            double x2 = variables[1];\n+            double x3 = variables[2];\n+            double[] f = new double[m];\n+            for (int i = 0; i < m; ++i) {\n+                f[i] = x1 * FastMath.exp(x2 / (5.0 * (i + 1) + 45.0 + x3)) - y[i];\n+            }\n+            return f;\n+        }\n+\n+        private static final double[] y = {\n+            34780.0, 28610.0, 23650.0, 19630.0,\n+            16370.0, 13720.0, 11540.0,  9744.0,\n+            8261.0,  7030.0,  6005.0,  5147.0,\n+            4427.0,  3820.0,  3307.0,  2872.0\n+        };\n+    }\n+\n+    private static class WatsonFunction extends MinpackFunction {\n+        private static final long serialVersionUID = -9034759294980218927L;\n+\n+        public WatsonFunction(int n, double x0,\n+                              double theoreticalStartCost,\n+                              double theoreticalMinCost,\n+                              double[] theoreticalMinParams) {\n+            super(31, buildArray(n, x0), theoreticalMinCost,\n+                  theoreticalMinParams);\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double[][] jacobian = new double[m][];\n+\n+            for (int i = 0; i < (m - 2); ++i) {\n+                double div = (i + 1) / 29.0;\n+                double s2  = 0.0;\n+                double dx  = 1.0;\n+                for (int j = 0; j < n; ++j) {\n+                    s2 += dx * variables[j];\n+                    dx *= div;\n+                }\n+                double temp= 2 * div * s2;\n+                dx = 1.0 / div;\n+                jacobian[i] = new double[n];\n+                for (int j = 0; j < n; ++j) {\n+                    jacobian[i][j] = dx * (j - temp);\n+                    dx *= div;\n+                }\n+            }\n+\n+            jacobian[m - 2]    = new double[n];\n+            jacobian[m - 2][0] = 1;\n+\n+            jacobian[m - 1]   = new double[n];\n+            jacobian[m - 1][0]= -2 * variables[0];\n+            jacobian[m - 1][1]= 1;\n+\n+            return jacobian;\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double[] f = new double[m];\n+            for (int i = 0; i < (m - 2); ++i) {\n+                double div = (i + 1) / 29.0;\n+                double s1 = 0;\n+                double dx = 1;\n+                for (int j = 1; j < n; ++j) {\n+                    s1 += j * dx * variables[j];\n+                    dx *= div;\n+                }\n+                double s2 = 0;\n+                dx = 1;\n+                for (int j = 0; j < n; ++j) {\n+                    s2 += dx * variables[j];\n+                    dx *= div;\n+                }\n+                f[i] = s1 - s2 * s2 - 1;\n+            }\n+\n+            double x1 = variables[0];\n+            double x2 = variables[1];\n+            f[m - 2] = x1;\n+            f[m - 1] = x2 - x1 * x1 - 1;\n+\n+            return f;\n+        }\n+    }\n+\n+    private static class Box3DimensionalFunction extends MinpackFunction {\n+        private static final long serialVersionUID = 5511403858142574493L;\n+\n+        public Box3DimensionalFunction(int m, double[] startParams,\n+                                       double theoreticalStartCost) {\n+            super(m, startParams, 0.0,\n+                  new double[] { 1.0, 10.0, 1.0 });\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double   x1 = variables[0];\n+            double   x2 = variables[1];\n+            double[][] jacobian = new double[m][];\n+            for (int i = 0; i < m; ++i) {\n+                double tmp = (i + 1) / 10.0;\n+                jacobian[i] = new double[] {\n+                    -tmp * FastMath.exp(-tmp * x1),\n+                    tmp * FastMath.exp(-tmp * x2),\n+                    FastMath.exp(-i - 1) - FastMath.exp(-tmp)\n+                };\n+            }\n+            return jacobian;\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double x1 = variables[0];\n+            double x2 = variables[1];\n+            double x3 = variables[2];\n+            double[] f = new double[m];\n+            for (int i = 0; i < m; ++i) {\n+                double tmp = (i + 1) / 10.0;\n+                f[i] = FastMath.exp(-tmp * x1) - FastMath.exp(-tmp * x2)\n+                    + (FastMath.exp(-i - 1) - FastMath.exp(-tmp)) * x3;\n+            }\n+            return f;\n+        }\n+    }\n+\n+    private static class JennrichSampsonFunction extends MinpackFunction {\n+        private static final long serialVersionUID = -2489165190443352947L;\n+\n+        public JennrichSampsonFunction(int m, double[] startParams,\n+                                       double theoreticalStartCost,\n+                                       double theoreticalMinCost,\n+                                       double[] theoreticalMinParams) {\n+            super(m, startParams, theoreticalMinCost,\n+                  theoreticalMinParams);\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double   x1 = variables[0];\n+            double   x2 = variables[1];\n+            double[][] jacobian = new double[m][];\n+            for (int i = 0; i < m; ++i) {\n+                double t = i + 1;\n+                jacobian[i] = new double[] { -t * FastMath.exp(t * x1), -t * FastMath.exp(t * x2) };\n+            }\n+            return jacobian;\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double x1 = variables[0];\n+            double x2 = variables[1];\n+            double[] f = new double[m];\n+            for (int i = 0; i < m; ++i) {\n+                double temp = i + 1;\n+                f[i] = 2 + 2 * temp - FastMath.exp(temp * x1) - FastMath.exp(temp * x2);\n+            }\n+            return f;\n+        }\n+    }\n+\n+    private static class BrownDennisFunction extends MinpackFunction {\n+        private static final long serialVersionUID = 8340018645694243910L;\n+\n+        public BrownDennisFunction(int m, double[] startParams,\n+                                   double theoreticalStartCost,\n+                                   double theoreticalMinCost,\n+                                   double[] theoreticalMinParams) {\n+            super(m, startParams, theoreticalMinCost,\n+                theoreticalMinParams);\n+            setCostAccuracy(2.5e-8);\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double   x1 = variables[0];\n+            double   x2 = variables[1];\n+            double   x3 = variables[2];\n+            double   x4 = variables[3];\n+            double[][] jacobian = new double[m][];\n+            for (int i = 0; i < m; ++i) {\n+                double temp = (i + 1) / 5.0;\n+                double ti   = FastMath.sin(temp);\n+                double tmp1 = x1 + temp * x2 - FastMath.exp(temp);\n+                double tmp2 = x3 + ti   * x4 - FastMath.cos(temp);\n+                jacobian[i] = new double[] {\n+                    2 * tmp1, 2 * temp * tmp1, 2 * tmp2, 2 * ti * tmp2\n+                };\n+            }\n+            return jacobian;\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double x1 = variables[0];\n+            double x2 = variables[1];\n+            double x3 = variables[2];\n+            double x4 = variables[3];\n+            double[] f = new double[m];\n+            for (int i = 0; i < m; ++i) {\n+                double temp = (i + 1) / 5.0;\n+                double tmp1 = x1 + temp * x2 - FastMath.exp(temp);\n+                double tmp2 = x3 + FastMath.sin(temp) * x4 - FastMath.cos(temp);\n+                f[i] = tmp1 * tmp1 + tmp2 * tmp2;\n+            }\n+            return f;\n+        }\n+    }\n+\n+    private static class ChebyquadFunction extends MinpackFunction {\n+        private static final long serialVersionUID = -2394877275028008594L;\n+\n+        private static double[] buildChebyquadArray(int n, double factor) {\n+            double[] array = new double[n];\n+            double inv = factor / (n + 1);\n+            for (int i = 0; i < n; ++i) {\n+                array[i] = (i + 1) * inv;\n+            }\n+            return array;\n+        }\n+\n+        public ChebyquadFunction(int n, int m, double factor,\n+                                 double theoreticalStartCost,\n+                                 double theoreticalMinCost,\n+                                 double[] theoreticalMinParams) {\n+            super(m, buildChebyquadArray(n, factor), theoreticalMinCost,\n+                  theoreticalMinParams);\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double[][] jacobian = new double[m][];\n+            for (int i = 0; i < m; ++i) {\n+                jacobian[i] = new double[n];\n+            }\n+\n+            double dx = 1.0 / n;\n+            for (int j = 0; j < n; ++j) {\n+                double tmp1 = 1;\n+                double tmp2 = 2 * variables[j] - 1;\n+                double temp = 2 * tmp2;\n+                double tmp3 = 0;\n+                double tmp4 = 2;\n+                for (int i = 0; i < m; ++i) {\n+                    jacobian[i][j] = dx * tmp4;\n+                    double ti = 4 * tmp2 + temp * tmp4 - tmp3;\n+                    tmp3 = tmp4;\n+                    tmp4 = ti;\n+                    ti   = temp * tmp2 - tmp1;\n+                    tmp1 = tmp2;\n+                    tmp2 = ti;\n+                }\n+            }\n+\n+            return jacobian;\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double[] f = new double[m];\n+\n+            for (int j = 0; j < n; ++j) {\n+                double tmp1 = 1;\n+                double tmp2 = 2 * variables[j] - 1;\n+                double temp = 2 * tmp2;\n+                for (int i = 0; i < m; ++i) {\n+                    f[i] += tmp2;\n+                    double ti = temp * tmp2 - tmp1;\n+                    tmp1 = tmp2;\n+                    tmp2 = ti;\n+                }\n+            }\n+\n+            double dx = 1.0 / n;\n+            boolean iev = false;\n+            for (int i = 0; i < m; ++i) {\n+                f[i] *= dx;\n+                if (iev) {\n+                    f[i] += 1.0 / (i * (i + 2));\n+                }\n+                iev = ! iev;\n+            }\n+\n+            return f;\n+        }\n+    }\n+\n+    private static class BrownAlmostLinearFunction extends MinpackFunction {\n+        private static final long serialVersionUID = 8239594490466964725L;\n+\n+        public BrownAlmostLinearFunction(int m, double factor,\n+                                         double theoreticalStartCost,\n+                                         double theoreticalMinCost,\n+                                         double[] theoreticalMinParams) {\n+            super(m, buildArray(m, factor), theoreticalMinCost,\n+                  theoreticalMinParams);\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double[][] jacobian = new double[m][];\n+            for (int i = 0; i < m; ++i) {\n+                jacobian[i] = new double[n];\n+            }\n+\n+            double prod = 1;\n+            for (int j = 0; j < n; ++j) {\n+                prod *= variables[j];\n+                for (int i = 0; i < n; ++i) {\n+                    jacobian[i][j] = 1;\n+                }\n+                jacobian[j][j] = 2;\n+            }\n+\n+            for (int j = 0; j < n; ++j) {\n+                double temp = variables[j];\n+                if (temp == 0) {\n+                    temp = 1;\n+                    prod = 1;\n+                    for (int k = 0; k < n; ++k) {\n+                        if (k != j) {\n+                            prod *= variables[k];\n+                        }\n+                    }\n+                }\n+                jacobian[n - 1][j] = prod / temp;\n+            }\n+\n+            return jacobian;\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double[] f = new double[m];\n+            double sum  = -(n + 1);\n+            double prod = 1;\n+            for (int j = 0; j < n; ++j) {\n+                sum  += variables[j];\n+                prod *= variables[j];\n+            }\n+            for (int i = 0; i < n; ++i) {\n+                f[i] = variables[i] + sum;\n+            }\n+            f[n - 1] = prod - 1;\n+            return f;\n+        }\n+    }\n+\n+    private static class Osborne1Function extends MinpackFunction {\n+        private static final long serialVersionUID = 4006743521149849494L;\n+\n+        public Osborne1Function(double[] startParams,\n+                                double theoreticalStartCost,\n+                                double theoreticalMinCost,\n+                                double[] theoreticalMinParams) {\n+            super(33, startParams, theoreticalMinCost,\n+                  theoreticalMinParams);\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double   x2 = variables[1];\n+            double   x3 = variables[2];\n+            double   x4 = variables[3];\n+            double   x5 = variables[4];\n+            double[][] jacobian = new double[m][];\n+            for (int i = 0; i < m; ++i) {\n+                double temp = 10.0 * i;\n+                double tmp1 = FastMath.exp(-temp * x4);\n+                double tmp2 = FastMath.exp(-temp * x5);\n+                jacobian[i] = new double[] {\n+                    -1, -tmp1, -tmp2, temp * x2 * tmp1, temp * x3 * tmp2\n+                };\n+            }\n+            return jacobian;\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double x1 = variables[0];\n+            double x2 = variables[1];\n+            double x3 = variables[2];\n+            double x4 = variables[3];\n+            double x5 = variables[4];\n+            double[] f = new double[m];\n+            for (int i = 0; i < m; ++i) {\n+                double temp = 10.0 * i;\n+                double tmp1 = FastMath.exp(-temp * x4);\n+                double tmp2 = FastMath.exp(-temp * x5);\n+                f[i] = y[i] - (x1 + x2 * tmp1 + x3 * tmp2);\n+            }\n+            return f;\n+        }\n+        \n+        private static final double[] y = {\n+            0.844, 0.908, 0.932, 0.936, 0.925, 0.908, 0.881, 0.850, 0.818, 0.784, 0.751,\n+            0.718, 0.685, 0.658, 0.628, 0.603, 0.580, 0.558, 0.538, 0.522, 0.506, 0.490,\n+            0.478, 0.467, 0.457, 0.448, 0.438, 0.431, 0.424, 0.420, 0.414, 0.411, 0.406\n+        };\n+    }\n+\n+    private static class Osborne2Function extends MinpackFunction {\n+        private static final long serialVersionUID = -8418268780389858746L;\n+\n+        public Osborne2Function(double[] startParams,\n+                                double theoreticalStartCost,\n+                                double theoreticalMinCost,\n+                                double[] theoreticalMinParams) {\n+            super(65, startParams, theoreticalMinCost,\n+                  theoreticalMinParams);\n+        }\n+\n+        @Override\n+        public double[][] computeJacobian(double[] variables) {\n+            double   x01 = variables[0];\n+            double   x02 = variables[1];\n+            double   x03 = variables[2];\n+            double   x04 = variables[3];\n+            double   x05 = variables[4];\n+            double   x06 = variables[5];\n+            double   x07 = variables[6];\n+            double   x08 = variables[7];\n+            double   x09 = variables[8];\n+            double   x10 = variables[9];\n+            double   x11 = variables[10];\n+            double[][] jacobian = new double[m][];\n+            for (int i = 0; i < m; ++i) {\n+                double temp = i / 10.0;\n+                double tmp1 = FastMath.exp(-x05 * temp);\n+                double tmp2 = FastMath.exp(-x06 * (temp - x09) * (temp - x09));\n+                double tmp3 = FastMath.exp(-x07 * (temp - x10) * (temp - x10));\n+                double tmp4 = FastMath.exp(-x08 * (temp - x11) * (temp - x11));\n+                jacobian[i] = new double[] {\n+                    -tmp1,\n+                    -tmp2,\n+                    -tmp3,\n+                    -tmp4,\n+                    temp * x01 * tmp1,\n+                    x02 * (temp - x09) * (temp - x09) * tmp2,\n+                    x03 * (temp - x10) * (temp - x10) * tmp3,\n+                    x04 * (temp - x11) * (temp - x11) * tmp4,\n+                    -2 * x02 * x06 * (temp - x09) * tmp2,\n+                    -2 * x03 * x07 * (temp - x10) * tmp3,\n+                    -2 * x04 * x08 * (temp - x11) * tmp4\n+                };\n+            }\n+            return jacobian;\n+        }\n+\n+        @Override\n+        public double[] computeValue(double[] variables) {\n+            double x01 = variables[0];\n+            double x02 = variables[1];\n+            double x03 = variables[2];\n+            double x04 = variables[3];\n+            double x05 = variables[4];\n+            double x06 = variables[5];\n+            double x07 = variables[6];\n+            double x08 = variables[7];\n+            double x09 = variables[8];\n+            double x10 = variables[9];\n+            double x11 = variables[10];\n+            double[] f = new double[m];\n+            for (int i = 0; i < m; ++i) {\n+                double temp = i / 10.0;\n+                double tmp1 = FastMath.exp(-x05 * temp);\n+                double tmp2 = FastMath.exp(-x06 * (temp - x09) * (temp - x09));\n+                double tmp3 = FastMath.exp(-x07 * (temp - x10) * (temp - x10));\n+                double tmp4 = FastMath.exp(-x08 * (temp - x11) * (temp - x11));\n+                f[i] = y[i] - (x01 * tmp1 + x02 * tmp2 + x03 * tmp3 + x04 * tmp4);\n+            }\n+            return f;\n+        }\n+\n+        private static final double[] y = {\n+            1.366, 1.191, 1.112, 1.013, 0.991,\n+            0.885, 0.831, 0.847, 0.786, 0.725,\n+            0.746, 0.679, 0.608, 0.655, 0.616,\n+            0.606, 0.602, 0.626, 0.651, 0.724,\n+            0.649, 0.649, 0.694, 0.644, 0.624,\n+            0.661, 0.612, 0.558, 0.533, 0.495,\n+            0.500, 0.423, 0.395, 0.375, 0.372,\n+            0.391, 0.396, 0.405, 0.428, 0.429,\n+            0.523, 0.562, 0.607, 0.653, 0.672,\n+            0.708, 0.633, 0.668, 0.645, 0.632,\n+            0.591, 0.559, 0.597, 0.625, 0.739,\n+            0.710, 0.729, 0.720, 0.636, 0.581,\n+            0.428, 0.292, 0.162, 0.098, 0.054\n+        };\n+    }\n+}\n--- /dev/null\n+++ b/src/test/java/org/apache/commons/math3/fitting/leastsquares/RandomCirclePointGenerator.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import org.apache.commons.math3.random.RandomGenerator;\n+import org.apache.commons.math3.random.Well44497b;\n+import org.apache.commons.math3.util.MathUtils;\n+import org.apache.commons.math3.util.FastMath;\n+import org.apache.commons.math3.distribution.RealDistribution;\n+import org.apache.commons.math3.distribution.UniformRealDistribution;\n+import org.apache.commons.math3.distribution.NormalDistribution;\n+import org.apache.commons.math3.geometry.euclidean.twod.Vector2D;\n+\n+/**\n+ * Factory for generating a cloud of points that approximate a circle.\n+ */\n+public class RandomCirclePointGenerator {\n+    /** RNG for the x-coordinate of the center. */\n+    private final RealDistribution cX;\n+    /** RNG for the y-coordinate of the center. */\n+    private final RealDistribution cY;\n+    /** RNG for the parametric position of the point. */\n+    private final RealDistribution tP;\n+    /** Radius of the circle. */\n+    private final double radius;\n+\n+    /**\n+     * @param x Abscissa of the circle center.\n+     * @param y Ordinate of the circle center.\n+     * @param radius Radius of the circle.\n+     * @param xSigma Error on the x-coordinate of the circumference points.\n+     * @param ySigma Error on the y-coordinate of the circumference points.\n+     * @param seed RNG seed.\n+     */\n+    public RandomCirclePointGenerator(double x,\n+                                      double y,\n+                                      double radius,\n+                                      double xSigma,\n+                                      double ySigma,\n+                                      long seed) {\n+        final RandomGenerator rng = new Well44497b(seed);\n+        this.radius = radius;\n+        cX = new NormalDistribution(rng, x, xSigma,\n+                                    NormalDistribution.DEFAULT_INVERSE_ABSOLUTE_ACCURACY);\n+        cY = new NormalDistribution(rng, y, ySigma,\n+                                    NormalDistribution.DEFAULT_INVERSE_ABSOLUTE_ACCURACY);\n+        tP = new UniformRealDistribution(rng, 0, MathUtils.TWO_PI,\n+                                         UniformRealDistribution.DEFAULT_INVERSE_ABSOLUTE_ACCURACY);\n+    }\n+\n+    /**\n+     * Point generator.\n+     *\n+     * @param n Number of points to create.\n+     * @return the cloud of {@code n} points.\n+     */\n+    public Vector2D[] generate(int n) {\n+        final Vector2D[] cloud = new Vector2D[n];\n+        for (int i = 0; i < n; i++) {\n+            cloud[i] = create();\n+        }\n+        return cloud;\n+    }\n+\n+    /**\n+     * Create one point.\n+     *\n+     * @return a point.\n+     */\n+    private Vector2D create() {\n+        final double t = tP.sample();\n+        final double pX = cX.sample() + radius * FastMath.cos(t);\n+        final double pY = cY.sample() + radius * FastMath.sin(t);\n+\n+        return new Vector2D(pX, pY);\n+    }\n+}\n--- /dev/null\n+++ b/src/test/java/org/apache/commons/math3/fitting/leastsquares/RandomStraightLinePointGenerator.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import java.awt.geom.Point2D;\n+import org.apache.commons.math3.random.RandomGenerator;\n+import org.apache.commons.math3.random.Well44497b;\n+import org.apache.commons.math3.distribution.RealDistribution;\n+import org.apache.commons.math3.distribution.UniformRealDistribution;\n+import org.apache.commons.math3.distribution.NormalDistribution;\n+\n+/**\n+ * Factory for generating a cloud of points that approximate a straight line.\n+ */\n+public class RandomStraightLinePointGenerator {\n+    /** Slope. */\n+    private final double slope;\n+    /** Intercept. */\n+    private final double intercept;\n+    /** RNG for the x-coordinate. */\n+    private final RealDistribution x;\n+    /** RNG for the error on the y-coordinate. */\n+    private final RealDistribution error;\n+\n+    /**\n+     * The generator will create a cloud of points whose x-coordinates\n+     * will be randomly sampled between {@code xLo} and {@code xHi}, and\n+     * the corresponding y-coordinates will be computed as\n+     * <pre><code>\n+     *  y = a x + b + N(0, error)\n+     * </code></pre>\n+     * where {@code N(mean, sigma)} is a Gaussian distribution with the\n+     * given mean and standard deviation.\n+     *\n+     * @param a Slope.\n+     * @param b Intercept.\n+     * @param sigma Standard deviation on the y-coordinate of the point.\n+     * @param lo Lowest value of the x-coordinate.\n+     * @param hi Highest value of the x-coordinate.\n+     * @param seed RNG seed.\n+     */\n+    public RandomStraightLinePointGenerator(double a,\n+                                            double b,\n+                                            double sigma,\n+                                            double lo,\n+                                            double hi,\n+                                            long seed) {\n+        final RandomGenerator rng = new Well44497b(seed);\n+        slope = a;\n+        intercept = b;\n+        error = new NormalDistribution(rng, 0, sigma,\n+                                       NormalDistribution.DEFAULT_INVERSE_ABSOLUTE_ACCURACY);\n+        x = new UniformRealDistribution(rng, lo, hi,\n+                                        UniformRealDistribution.DEFAULT_INVERSE_ABSOLUTE_ACCURACY);\n+    }\n+\n+    /**\n+     * Point generator.\n+     *\n+     * @param n Number of points to create.\n+     * @return the cloud of {@code n} points.\n+     */\n+    public Point2D.Double[] generate(int n) {\n+        final Point2D.Double[] cloud = new Point2D.Double[n];\n+        for (int i = 0; i < n; i++) {\n+            cloud[i] = create();\n+        }\n+        return cloud;\n+    }\n+\n+    /**\n+     * Create one point.\n+     *\n+     * @return a point.\n+     */\n+    private Point2D.Double create() {\n+        final double abscissa = x.sample();\n+        final double yModel = slope * abscissa + intercept;\n+        final double ordinate = yModel + error.sample();\n+\n+        return new Point2D.Double(abscissa, ordinate);\n+    }\n+}\n--- /dev/null\n+++ b/src/test/java/org/apache/commons/math3/fitting/leastsquares/StatisticalReferenceDataset.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import org.apache.commons.math3.analysis.MultivariateVectorFunction;\n+import org.apache.commons.math3.analysis.MultivariateMatrixFunction;\n+import org.apache.commons.math3.util.MathArrays;\n+\n+/**\n+ * This class gives access to the statistical reference datasets provided by the\n+ * NIST (available\n+ * <a href=\"http://www.itl.nist.gov/div898/strd/general/dataarchive.html\">here</a>).\n+ * Instances of this class can be created by invocation of the\n+ * {@link StatisticalReferenceDatasetFactory}.\n+ */\n+public abstract class StatisticalReferenceDataset {\n+    /** The name of this dataset. */\n+    private final String name;\n+    /** The total number of observations (data points). */\n+    private final int numObservations;\n+    /** The total number of parameters. */\n+    private final int numParameters;\n+    /** The total number of starting points for the optimizations. */\n+    private final int numStartingPoints;\n+    /** The values of the predictor. */\n+    private final double[] x;\n+    /** The values of the response. */\n+    private final double[] y;\n+    /**\n+     * The starting values. {@code startingValues[j][i]} is the value of the\n+     * {@code i}-th parameter in the {@code j}-th set of starting values.\n+     */\n+    private final double[][] startingValues;\n+    /** The certified values of the parameters. */\n+    private final double[] a;\n+    /** The certified values of the standard deviation of the parameters. */\n+    private final double[] sigA;\n+    /** The certified value of the residual sum of squares. */\n+    private double residualSumOfSquares;\n+    /** The least-squares problem. */\n+    private final LeastSquaresProblem problem;\n+\n+    /**\n+     * Creates a new instance of this class from the specified data file. The\n+     * file must follow the StRD format.\n+     *\n+     * @param in the data file\n+     * @throws IOException if an I/O error occurs\n+     */\n+    public StatisticalReferenceDataset(final BufferedReader in)\n+        throws IOException {\n+\n+        final ArrayList<String> lines = new ArrayList<String>();\n+        for (String line = in.readLine(); line != null; line = in.readLine()) {\n+            lines.add(line);\n+        }\n+        int[] index = findLineNumbers(\"Data\", lines);\n+        if (index == null) {\n+            throw new AssertionError(\"could not find line indices for data\");\n+        }\n+        this.numObservations = index[1] - index[0] + 1;\n+        this.x = new double[this.numObservations];\n+        this.y = new double[this.numObservations];\n+        for (int i = 0; i < this.numObservations; i++) {\n+            final String line = lines.get(index[0] + i - 1);\n+            final String[] tokens = line.trim().split(\" ++\");\n+            // Data columns are in reverse order!!!\n+            this.y[i] = Double.parseDouble(tokens[0]);\n+            this.x[i] = Double.parseDouble(tokens[1]);\n+        }\n+\n+        index = findLineNumbers(\"Starting Values\", lines);\n+        if (index == null) {\n+            throw new AssertionError(\n+                                     \"could not find line indices for starting values\");\n+        }\n+        this.numParameters = index[1] - index[0] + 1;\n+\n+        double[][] start = null;\n+        this.a = new double[numParameters];\n+        this.sigA = new double[numParameters];\n+        for (int i = 0; i < numParameters; i++) {\n+            final String line = lines.get(index[0] + i - 1);\n+            final String[] tokens = line.trim().split(\" ++\");\n+            if (start == null) {\n+                start = new double[tokens.length - 4][numParameters];\n+            }\n+            for (int j = 2; j < tokens.length - 2; j++) {\n+                start[j - 2][i] = Double.parseDouble(tokens[j]);\n+            }\n+            this.a[i] = Double.parseDouble(tokens[tokens.length - 2]);\n+            this.sigA[i] = Double.parseDouble(tokens[tokens.length - 1]);\n+        }\n+        if (start == null) {\n+            throw new IOException(\"could not find starting values\");\n+        }\n+        this.numStartingPoints = start.length;\n+        this.startingValues = start;\n+\n+        double dummyDouble = Double.NaN;\n+        String dummyString = null;\n+        for (String line : lines) {\n+            if (line.contains(\"Dataset Name:\")) {\n+                dummyString = line\n+                    .substring(line.indexOf(\"Dataset Name:\") + 13,\n+                               line.indexOf(\"(\")).trim();\n+            }\n+            if (line.contains(\"Residual Sum of Squares\")) {\n+                final String[] tokens = line.split(\" ++\");\n+                dummyDouble = Double.parseDouble(tokens[4].trim());\n+            }\n+        }\n+        if (Double.isNaN(dummyDouble)) {\n+            throw new IOException(\n+                                  \"could not find certified value of residual sum of squares\");\n+        }\n+        this.residualSumOfSquares = dummyDouble;\n+\n+        if (dummyString == null) {\n+            throw new IOException(\"could not find dataset name\");\n+        }\n+        this.name = dummyString;\n+\n+        this.problem = new LeastSquaresProblem();\n+    }\n+\n+    class LeastSquaresProblem {\n+        public MultivariateVectorFunction getModelFunction() {\n+            return new MultivariateVectorFunction() {\n+                public double[] value(final double[] a) {\n+                    final int n = getNumObservations();\n+                    final double[] yhat = new double[n];\n+                    for (int i = 0; i < n; i++) {\n+                        yhat[i] = getModelValue(getX(i), a);\n+                    }\n+                    return yhat;\n+                }\n+            };\n+        }\n+\n+        public MultivariateMatrixFunction getModelFunctionJacobian() {\n+            return new MultivariateMatrixFunction() {\n+                public double[][] value(final double[] a)\n+                    throws IllegalArgumentException {\n+                    final int n = getNumObservations();\n+                    final double[][] j = new double[n][];\n+                    for (int i = 0; i < n; i++) {\n+                        j[i] = getModelDerivatives(getX(i), a);\n+                    }\n+                    return j;\n+                }\n+            };\n+        }\n+    }\n+\n+    /**\n+     * Returns the name of this dataset.\n+     *\n+     * @return the name of the dataset\n+     */\n+    public String getName() {\n+        return name;\n+    }\n+\n+    /**\n+     * Returns the total number of observations (data points).\n+     *\n+     * @return the number of observations\n+     */\n+    public int getNumObservations() {\n+        return numObservations;\n+    }\n+\n+    /**\n+     * Returns a copy of the data arrays. The data is laid out as follows <li>\n+     * {@code data[0][i] = x[i]},</li> <li>{@code data[1][i] = y[i]},</li>\n+     *\n+     * @return the array of data points.\n+     */\n+    public double[][] getData() {\n+        return new double[][] {\n+            MathArrays.copyOf(x), MathArrays.copyOf(y)\n+        };\n+    }\n+\n+    /**\n+     * Returns the x-value of the {@code i}-th data point.\n+     *\n+     * @param i the index of the data point\n+     * @return the x-value\n+     */\n+    public double getX(final int i) {\n+        return x[i];\n+    }\n+\n+    /**\n+     * Returns the y-value of the {@code i}-th data point.\n+     *\n+     * @param i the index of the data point\n+     * @return the y-value\n+     */\n+    public double getY(final int i) {\n+        return y[i];\n+    }\n+\n+    /**\n+     * Returns the total number of parameters.\n+     *\n+     * @return the number of parameters\n+     */\n+    public int getNumParameters() {\n+        return numParameters;\n+    }\n+\n+    /**\n+     * Returns the certified values of the paramters.\n+     *\n+     * @return the values of the parameters\n+     */\n+    public double[] getParameters() {\n+        return MathArrays.copyOf(a);\n+    }\n+\n+    /**\n+     * Returns the certified value of the {@code i}-th parameter.\n+     *\n+     * @param i the index of the parameter\n+     * @return the value of the parameter\n+     */\n+    public double getParameter(final int i) {\n+        return a[i];\n+    }\n+\n+    /**\n+     * Reurns the certified values of the standard deviations of the parameters.\n+     *\n+     * @return the standard deviations of the parameters\n+     */\n+    public double[] getParametersStandardDeviations() {\n+        return MathArrays.copyOf(sigA);\n+    }\n+\n+    /**\n+     * Returns the certified value of the standard deviation of the {@code i}-th\n+     * parameter.\n+     *\n+     * @param i the index of the parameter\n+     * @return the standard deviation of the parameter\n+     */\n+    public double getParameterStandardDeviation(final int i) {\n+        return sigA[i];\n+    }\n+\n+    /**\n+     * Returns the certified value of the residual sum of squares.\n+     *\n+     * @return the residual sum of squares\n+     */\n+    public double getResidualSumOfSquares() {\n+        return residualSumOfSquares;\n+    }\n+\n+    /**\n+     * Returns the total number of starting points (initial guesses for the\n+     * optimization process).\n+     *\n+     * @return the number of starting points\n+     */\n+    public int getNumStartingPoints() {\n+        return numStartingPoints;\n+    }\n+\n+    /**\n+     * Returns the {@code i}-th set of initial values of the parameters.\n+     *\n+     * @param i the index of the starting point\n+     * @return the starting point\n+     */\n+    public double[] getStartingPoint(final int i) {\n+        return MathArrays.copyOf(startingValues[i]);\n+    }\n+\n+    /**\n+     * Returns the least-squares problem corresponding to fitting the model to\n+     * the specified data.\n+     *\n+     * @return the least-squares problem\n+     */\n+    public LeastSquaresProblem getLeastSquaresProblem() {\n+        return problem;\n+    }\n+\n+    /**\n+     * Returns the value of the model for the specified values of the predictor\n+     * variable and the parameters.\n+     *\n+     * @param x the predictor variable\n+     * @param a the parameters\n+     * @return the value of the model\n+     */\n+    public abstract double getModelValue(final double x, final double[] a);\n+\n+    /**\n+     * Returns the values of the partial derivatives of the model with respect\n+     * to the parameters.\n+     *\n+     * @param x the predictor variable\n+     * @param a the parameters\n+     * @return the partial derivatives\n+     */\n+    public abstract double[] getModelDerivatives(final double x,\n+                                                 final double[] a);\n+\n+    /**\n+     * <p>\n+     * Parses the specified text lines, and extracts the indices of the first\n+     * and last lines of the data defined by the specified {@code key}. This key\n+     * must be one of\n+     * </p>\n+     * <ul>\n+     * <li>{@code \"Starting Values\"},</li>\n+     * <li>{@code \"Certified Values\"},</li>\n+     * <li>{@code \"Data\"}.</li>\n+     * </ul>\n+     * <p>\n+     * In the NIST data files, the line indices are separated by the keywords\n+     * {@code \"lines\"} and {@code \"to\"}.\n+     * </p>\n+     *\n+     * @param lines the line of text to be parsed\n+     * @return an array of two {@code int}s. First value is the index of the\n+     *         first line, second value is the index of the last line.\n+     *         {@code null} if the line could not be parsed.\n+     */\n+    private static int[] findLineNumbers(final String key,\n+                                         final Iterable<String> lines) {\n+        for (String text : lines) {\n+            boolean flag = text.contains(key) && text.contains(\"lines\") &&\n+                           text.contains(\"to\") && text.contains(\")\");\n+            if (flag) {\n+                final int[] numbers = new int[2];\n+                final String from = text.substring(text.indexOf(\"lines\") + 5,\n+                                                   text.indexOf(\"to\"));\n+                numbers[0] = Integer.parseInt(from.trim());\n+                final String to = text.substring(text.indexOf(\"to\") + 2,\n+                                                 text.indexOf(\")\"));\n+                numbers[1] = Integer.parseInt(to.trim());\n+                return numbers;\n+            }\n+        }\n+        return null;\n+    }\n+}\n--- /dev/null\n+++ b/src/test/java/org/apache/commons/math3/fitting/leastsquares/StatisticalReferenceDatasetFactory.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import org.apache.commons.math3.util.FastMath;\n+\n+/**\n+ * A factory to create instances of {@link StatisticalReferenceDataset} from\n+ * available resources.\n+ */\n+public class StatisticalReferenceDatasetFactory {\n+\n+    private StatisticalReferenceDatasetFactory() {\n+        // Do nothing\n+    }\n+\n+    /**\n+     * Creates a new buffered reader from the specified resource name.\n+     *\n+     * @param name the name of the resource\n+     * @return a buffered reader\n+     * @throws IOException if an I/O error occured\n+     */\n+    public static BufferedReader createBufferedReaderFromResource(final String name)\n+        throws IOException {\n+        final InputStream resourceAsStream;\n+        resourceAsStream = StatisticalReferenceDatasetFactory.class\n+            .getResourceAsStream(name);\n+        if (resourceAsStream == null) {\n+            throw new IOException(\"could not find resource \" + name);\n+        }\n+        return new BufferedReader(new InputStreamReader(resourceAsStream));\n+    }\n+\n+    public static StatisticalReferenceDataset createKirby2()\n+        throws IOException {\n+        final BufferedReader in = createBufferedReaderFromResource(\"Kirby2.dat\");\n+        StatisticalReferenceDataset dataset = null;\n+        try {\n+            dataset = new StatisticalReferenceDataset(in) {\n+\n+                @Override\n+                public double getModelValue(final double x, final double[] a) {\n+                    final double p = a[0] + x * (a[1] + x * a[2]);\n+                    final double q = 1.0 + x * (a[3] + x * a[4]);\n+                    return p / q;\n+                }\n+\n+                @Override\n+                public double[] getModelDerivatives(final double x,\n+                                                    final double[] a) {\n+                    final double[] dy = new double[5];\n+                    final double p = a[0] + x * (a[1] + x * a[2]);\n+                    final double q = 1.0 + x * (a[3] + x * a[4]);\n+                    dy[0] = 1.0 / q;\n+                    dy[1] = x / q;\n+                    dy[2] = x * dy[1];\n+                    dy[3] = -x * p / (q * q);\n+                    dy[4] = x * dy[3];\n+                    return dy;\n+                }\n+            };\n+        } finally {\n+            in.close();\n+        }\n+        return dataset;\n+    }\n+\n+    public static StatisticalReferenceDataset createHahn1()\n+        throws IOException {\n+        final BufferedReader in = createBufferedReaderFromResource(\"Hahn1.dat\");\n+        StatisticalReferenceDataset dataset = null;\n+        try {\n+            dataset = new StatisticalReferenceDataset(in) {\n+\n+                @Override\n+                public double getModelValue(final double x, final double[] a) {\n+                    final double p = a[0] + x * (a[1] + x * (a[2] + x * a[3]));\n+                    final double q = 1.0 + x * (a[4] + x * (a[5] + x * a[6]));\n+                    return p / q;\n+                }\n+\n+                @Override\n+                public double[] getModelDerivatives(final double x,\n+                                                    final double[] a) {\n+                    final double[] dy = new double[7];\n+                    final double p = a[0] + x * (a[1] + x * (a[2] + x * a[3]));\n+                    final double q = 1.0 + x * (a[4] + x * (a[5] + x * a[6]));\n+                    dy[0] = 1.0 / q;\n+                    dy[1] = x * dy[0];\n+                    dy[2] = x * dy[1];\n+                    dy[3] = x * dy[2];\n+                    dy[4] = -x * p / (q * q);\n+                    dy[5] = x * dy[4];\n+                    dy[6] = x * dy[5];\n+                    return dy;\n+                }\n+            };\n+        } finally {\n+            in.close();\n+        }\n+        return dataset;\n+    }\n+\n+    public static StatisticalReferenceDataset createMGH17()\n+        throws IOException {\n+        final BufferedReader in = createBufferedReaderFromResource(\"MGH17.dat\");\n+        StatisticalReferenceDataset dataset = null;\n+        try {\n+            dataset = new StatisticalReferenceDataset(in) {\n+\n+                @Override\n+                public double getModelValue(final double x, final double[] a) {\n+                    return a[0] + a[1] * FastMath.exp(-a[3] * x) + a[2] *\n+                           FastMath.exp(-a[4] * x);\n+                }\n+\n+                @Override\n+                public double[] getModelDerivatives(final double x,\n+                                                    final double[] a) {\n+                    final double[] dy = new double[5];\n+                    dy[0] = 1.0;\n+                    dy[1] = FastMath.exp(-x * a[3]);\n+                    dy[2] = FastMath.exp(-x * a[4]);\n+                    dy[3] = -x * a[1] * dy[1];\n+                    dy[4] = -x * a[2] * dy[2];\n+                    return dy;\n+                }\n+            };\n+        } finally {\n+            in.close();\n+        }\n+        return dataset;\n+    }\n+\n+    public static StatisticalReferenceDataset createLanczos1()\n+        throws IOException {\n+        final BufferedReader in =\n+            createBufferedReaderFromResource(\"Lanczos1.dat\");\n+        StatisticalReferenceDataset dataset = null;\n+        try {\n+            dataset = new StatisticalReferenceDataset(in) {\n+\n+                @Override\n+                public double getModelValue(final double x, final double[] a) {\n+                    System.out.println(a[0]+\", \"+a[1]+\", \"+a[2]+\", \"+a[3]+\", \"+a[4]+\", \"+a[5]);\n+                    return a[0] * FastMath.exp(-a[3] * x) +\n+                           a[1] * FastMath.exp(-a[4] * x) +\n+                           a[2] * FastMath.exp(-a[5] * x);\n+                }\n+\n+                @Override\n+                public double[] getModelDerivatives(final double x,\n+                    final double[] a) {\n+                    final double[] dy = new double[6];\n+                    dy[0] = FastMath.exp(-x * a[3]);\n+                    dy[1] = FastMath.exp(-x * a[4]);\n+                    dy[2] = FastMath.exp(-x * a[5]);\n+                    dy[3] = -x * a[0] * dy[0];\n+                    dy[4] = -x * a[1] * dy[1];\n+                    dy[5] = -x * a[2] * dy[2];\n+                    return dy;\n+                }\n+            };\n+        } finally {\n+            in.close();\n+        }\n+        return dataset;\n+    }\n+\n+    /**\n+     * Returns an array with all available reference datasets.\n+     *\n+     * @return the array of datasets\n+     * @throws IOException if an I/O error occurs\n+     */\n+    public StatisticalReferenceDataset[] createAll()\n+        throws IOException {\n+        return new StatisticalReferenceDataset[] {\n+            createKirby2(), createMGH17()\n+        };\n+    }\n+}\n--- /dev/null\n+++ b/src/test/java/org/apache/commons/math3/fitting/leastsquares/StraightLineProblem.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.commons.math3.fitting.leastsquares;\n+\n+import java.util.ArrayList;\n+import org.apache.commons.math3.analysis.MultivariateVectorFunction;\n+import org.apache.commons.math3.analysis.MultivariateMatrixFunction;\n+import org.apache.commons.math3.analysis.UnivariateFunction;\n+import org.apache.commons.math3.stat.regression.SimpleRegression;\n+\n+/**\n+ * Class that models a straight line defined as {@code y = a x + b}.\n+ * The parameters of problem are:\n+ * <ul>\n+ *  <li>{@code a}</li>\n+ *  <li>{@code b}</li>\n+ * </ul>\n+ * The model functions are:\n+ * <ul>\n+ *  <li>for each pair (a, b), the y-coordinate of the line.</li>\n+ * </ul>\n+ */\n+class StraightLineProblem {\n+    /** Cloud of points assumed to be fitted by a straight line. */\n+    private final ArrayList<double[]> points;\n+    /** Error (on the y-coordinate of the points). */\n+    private final double sigma;\n+\n+    /**\n+     * @param error Assumed error for the y-coordinate.\n+     */\n+    public StraightLineProblem(double error) {\n+        points = new ArrayList<double[]>();\n+        sigma = error;\n+    }\n+\n+    public void addPoint(double px, double py) {\n+        points.add(new double[] { px, py });\n+    }\n+\n+    /**\n+     * @return the list of x-coordinates.\n+     */\n+    public double[] x() {\n+        final double[] v = new double[points.size()];\n+        for (int i = 0; i < points.size(); i++) {\n+            final double[] p = points.get(i);\n+            v[i] = p[0]; // x-coordinate.\n+        }\n+\n+        return v;\n+    }\n+\n+    /**\n+     * @return the list of y-coordinates.\n+     */\n+    public double[] y() {\n+        final double[] v = new double[points.size()];\n+        for (int i = 0; i < points.size(); i++) {\n+            final double[] p = points.get(i);\n+            v[i] = p[1]; // y-coordinate.\n+        }\n+\n+        return v;\n+    }\n+\n+    public double[] target() {\n+        return y();\n+    }\n+\n+    public double[] weight() {\n+        final double weight = 1 / (sigma * sigma);\n+        final double[] w = new double[points.size()];\n+        for (int i = 0; i < points.size(); i++) {\n+            w[i] = weight;\n+        }\n+\n+        return w;\n+    }\n+\n+    public MultivariateVectorFunction getModelFunction() {\n+        return new MultivariateVectorFunction() {\n+            public double[] value(double[] params) {\n+                final Model line = new Model(params[0], params[1]);\n+\n+                final double[] model = new double[points.size()];\n+                for (int i = 0; i < points.size(); i++) {\n+                    final double[] p = points.get(i);\n+                    model[i] = line.value(p[0]);\n+                }\n+\n+                return model;\n+            }\n+        };\n+    }\n+\n+    public MultivariateMatrixFunction getModelFunctionJacobian() {\n+        return new MultivariateMatrixFunction() {\n+            public double[][] value(double[] point) {\n+                return jacobian(point);\n+            }\n+        };\n+    }\n+\n+    /**\n+     * Directly solve the linear problem, using the {@link SimpleRegression}\n+     * class.\n+     */\n+    public double[] solve() {\n+        final SimpleRegression regress = new SimpleRegression(true);\n+        for (double[] d : points) {\n+            regress.addData(d[0], d[1]);\n+        }\n+\n+        final double[] result = { regress.getSlope(), regress.getIntercept() };\n+        return result;\n+    }\n+\n+    private double[][] jacobian(double[] params) {\n+        final double[][] jacobian = new double[points.size()][2];\n+\n+        for (int i = 0; i < points.size(); i++) {\n+            final double[] p = points.get(i);\n+            // Partial derivative wrt \"a\".\n+            jacobian[i][0] = p[0];\n+            // Partial derivative wrt \"b\".\n+            jacobian[i][1] = 1;\n+        }\n+\n+        return jacobian;\n+    }\n+\n+    /**\n+     * Linear function.\n+     */\n+    public static class Model implements UnivariateFunction {\n+        final double a;\n+        final double b;\n+\n+        public Model(double a,\n+                     double b) {\n+            this.a = a;\n+            this.b = b;\n+        }\n+\n+        public double value(double x) {\n+            return a * x + b;\n+        }\n+    }\n+}", "timestamp": 1375196662, "metainfo": ""}