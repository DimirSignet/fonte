{"sha": "b1e524891f908cd667b8c54b4a04b3af2e65969a", "log": "improve docs", "commit": "\n--- a/src/main/java/org/apache/commons/compress/compressors/lz77support/AbstractLZ77CompressorInputStream.java\n+++ b/src/main/java/org/apache/commons/compress/compressors/lz77support/AbstractLZ77CompressorInputStream.java\n  * implementation delegates to the no-arg version, leading to infinite\n  * mutual recursion and a {@code StackOverflowError} otherwise.</p>\n  *\n- * <p>The contract for subclasses {@code read} implementation is:</p>\n+ * <p>The contract for subclasses' {@code read} implementation is:</p>\n  * <ul>\n  *\n  *  <li>keep track of the current state of the stream. Is it inside a\n     /** Size of the window - must be bigger than the biggest offset expected. */\n     private final int windowSize;\n \n-    /** Buffer to write decompressed bytes to for back-references */\n+    /**\n+     * Buffer to write decompressed bytes to for back-references, will\n+     * be three times windowSize big.\n+     *\n+     * <p>Three times so we can slide the whole buffer a windowSize to\n+     * the left once we've read twice windowSize and still have enough\n+     * data inside of it to satisfy back-references.</p>\n+     */\n     private final byte[] buf;\n \n-    /** One behind the index of the last byte in the buffer that was written */\n+    /** One behind the index of the last byte in the buffer that was written, i.e. the next position to write to */\n     private int writeIndex;\n \n     /** Index of the next byte to be read. */\n         if (writeIndex != 0) {\n             throw new IllegalStateException(\"the stream has already been read from, can't prefill anymore\");\n         }\n+        // we don't need more data than the big offset could refer to, so cap it\n         int len = Math.min(windowSize, data.length);\n+        // we need the last data as we are dealing with *back*-references\n         System.arraycopy(data, data.length - len, buf, 0, len);\n         writeIndex += len;\n         readIndex += len;\n     }\n \n     private void tryToReadLiteral(int bytesToRead) throws IOException {\n+        // min of \"what is still inside the literal\", \"what does the user want\" and \"how muc can fit into the buffer\"\n         final int reallyTryToRead = (int) Math.min(Math.min(bytesToRead, bytesRemaining),\n                                                    buf.length - writeIndex);\n         final int bytesRead = reallyTryToRead > 0\n             System.arraycopy(buf, writeIndex - backReferenceOffset, buf, writeIndex, copy);\n             writeIndex += copy;\n         } else {\n+            // back-reference overlaps with the bytes created from it\n+            // like go back two bytes and then copy six (by copying\n+            // the last two bytes three time).\n             final int fullRots = copy / backReferenceOffset;\n             for (int i = 0; i < fullRots; i++) {\n                 System.arraycopy(buf, writeIndex - backReferenceOffset, buf, writeIndex, backReferenceOffset);\n--- a/src/main/java/org/apache/commons/compress/compressors/lz77support/LZ77Compressor.java\n+++ b/src/main/java/org/apache/commons/compress/compressors/lz77support/LZ77Compressor.java\n  * back-references - so it can be re-used. It follows the algorithm\n  * explained in section 4 of RFC 1951 (DEFLATE) and currently doesn't\n  * implement the \"lazy match\" optimization. The three-byte hash\n- * function used in this class is the same used by zlib and InfoZIP's\n- * ZIP implementation of DEFLATE. Strongly inspired by InfoZIP's\n- * implementation.</p>\n+ * function used in this class is the same as the one used by zlib and\n+ * InfoZIP's ZIP implementation of DEFLATE. The whole class is\n+ * strongly inspired by InfoZIP's implementation.</p>\n  *\n  * <p>LZ77 is used vaguely here (as well as many other places that\n  * talk about it :-), LZSS would likely be closer to the truth but\n  * <p>The API consists of a compressor that is fed <code>byte</code>s\n  * and emits {@link Block}s to a registered callback where the blocks\n  * represent either {@link LiteralBlock literal blocks}, {@link\n- * BackReference back references} or {@link EOD end of data\n+ * BackReference back-references} or {@link EOD end of data\n  * markers}. In order to ensure the callback receives all information,\n  * the {@code #finish} method must be used once all data has been fed\n  * into the compressor.</p>\n     private int blockStart = 0;\n     // position of the current match\n     private int matchStart = NO_MATCH;\n-    // number of insertString calls for the up to three last bytes of the last match\n+    // number of missed insertString calls for the up to three last\n+    // bytes of the last match that can only be performed once more\n+    // data has been read\n     private int missedInserts = 0;\n \n     /**\n      */\n     public void compress(byte[] data, int off, int len) throws IOException {\n         final int wSize = params.getWindowSize();\n-        while (len > wSize) {\n+        while (len > wSize) { // chop into windowSize sized chunks\n             doCompress(data, off, wSize);\n             off += wSize;\n             len -= wSize;\n         if (currentPosition != 0 || lookahead != 0) {\n             throw new IllegalStateException(\"the compressor has already started to accept data, can't prefill anymore\");\n         }\n+\n+        // don't need more than windowSize for back-references\n         final int len = Math.min(params.getWindowSize(), data.length);\n         System.arraycopy(data, data.length - len, window, 0, len);\n+\n         if (len >= NUMBER_OF_BYTES_IN_HASH) {\n             initialize();\n             final int stop = len - NUMBER_OF_BYTES_IN_HASH + 1;\n                 insertString(i);\n             }\n             missedInserts = NUMBER_OF_BYTES_IN_HASH - 1;\n-        } else {\n+        } else { // not enough data to hash anything\n             missedInserts = len;\n         }\n         blockStart = currentPosition = len;", "timestamp": 1486572908, "metainfo": ""}