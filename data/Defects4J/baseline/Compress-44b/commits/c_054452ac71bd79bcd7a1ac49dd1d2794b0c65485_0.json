{"sha": "054452ac71bd79bcd7a1ac49dd1d2794b0c65485", "log": "Initial code base for read-only Unix dump support contributed by Bear Giles.  COMPRESS-132  ", "commit": "\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/compress/archivers/dump/Dirent.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.commons.compress.archivers.dump;\n+\n+\n+/**\n+ * Directory entry.\n+ */\n+public class Dirent {\n+    private int ino;\n+    private int parentIno;\n+    private int type;\n+    private String name;\n+\n+    /**\n+     * Constructor\n+     *\n+     * @param ino\n+     * @param parentIno\n+     * @param type\n+     * @param name\n+     */\n+    public Dirent(int ino, int parentIno, int type, String name) {\n+        this.ino = ino;\n+        this.parentIno = parentIno;\n+        this.type = type;\n+        this.name = name;\n+    }\n+\n+    /**\n+     * Get ino.\n+     * @return\n+     */\n+    public int getIno() {\n+        return ino;\n+    }\n+\n+    /**\n+     * Get ino of parent directory.\n+     * @return\n+     */\n+    public int getParentIno() {\n+        return parentIno;\n+    }\n+\n+    /**\n+     * Get entry type.\n+     * @return\n+     */\n+    public int getType() {\n+        return type;\n+    }\n+\n+    /**\n+     * Get name of directory entry.\n+     * @return\n+     */\n+    public String getName() {\n+        return name;\n+    }\n+\n+    /**\n+     * @see java.lang.Object#toString()\n+     */\n+    @Override\n+    public String toString() {\n+        return String.format(\"[%d]: %s\", ino, name);\n+    }\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/compress/archivers/dump/DumpArchiveConstants.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.commons.compress.archivers.dump;\n+\n+/**\n+ * Various constants associated with dump archives.\n+ */\n+public final class DumpArchiveConstants {\n+    public static final int TP_SIZE = 1024;\n+    public static final int NTREC = 10;\n+    public static final int HIGH_DENSITY_NTREC = 32;\n+    public static final int OFS_MAGIC = 60011;\n+    public static final int NFS_MAGIC = 60012;\n+    public static final int FS_UFS2_MAGIC = 0x19540119;\n+    public static final int CHECKSUM = 84446;\n+    public static final int LBLSIZE = 16;\n+    public static final int NAMELEN = 64;\n+\n+    /* do not instantiate */\n+    private DumpArchiveConstants() {\n+    }\n+\n+    /**\n+     * The type of tape segment.\n+     */\n+    public enum SEGMENT_TYPE {\n+        TAPE(1),\n+        INODE(2),\n+        BITS(3),\n+        ADDR(4),\n+        END(5),\n+        CLRI(6);\n+   \n+        int code;\n+\n+        private SEGMENT_TYPE(int code) {\n+            this.code = code;\n+        }\n+\n+        public static SEGMENT_TYPE find(int code) {\n+            for (SEGMENT_TYPE t : values()) {\n+                if (t.code == code) {\n+                    return t;\n+                }\n+            }\n+\n+            return null;\n+        }\n+    }\n+\n+    /**\n+     * The type of compression.\n+     */\n+    public enum COMPRESSION_TYPE {\n+        ZLIB(0),\n+        BZLIB(1),\n+        LZO(2);\n+\n+        int code;\n+\n+        private COMPRESSION_TYPE(int code) {\n+            this.code = code;\n+        }\n+\n+        public static COMPRESSION_TYPE find(int code) {\n+            for (COMPRESSION_TYPE t : values()) {\n+                if (t.code == code) {\n+                    return t;\n+                }\n+            }\n+\n+            return null;\n+        }\n+    }\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/compress/archivers/dump/DumpArchiveEntry.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.commons.compress.archivers.dump;\n+\n+import java.util.Collections;\n+import java.util.Date;\n+import java.util.EnumSet;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.commons.compress.archivers.ArchiveEntry;\n+\n+/**\n+ * This class represents an entry in a Dump archive. It consists\n+ * of the entry's header, the entry's File and any extended attributes.\n+ * <p>\n+ * DumpEntries that are created from the header bytes read from\n+ * an archive are instantiated with the DumpArchiveEntry( byte[] )\n+ * constructor. These entries will be used when extracting from\n+ * or listing the contents of an archive. These entries have their\n+ * header filled in using the header bytes. They also set the File\n+ * to null, since they reference an archive entry not a file.\n+ * <p>\n+ * DumpEntries can also be constructed from nothing but a name.\n+ * This allows the programmer to construct the entry by hand, for\n+ * instance when only an InputStream is available for writing to\n+ * the archive, and the header information is constructed from\n+ * other information. In this case the header fields are set to\n+ * defaults and the File is set to null.\n+ *\n+ * <p>\n+ * The C structure for a Dump Entry's header is:\n+ * <pre>\n+ * #define TP_BSIZE    1024          // size of each file block\n+ * #define NTREC       10            // number of blocks to write at once\n+ * #define HIGHDENSITYTREC 32        // number of blocks to write on high-density tapes\n+ * #define TP_NINDIR   (TP_BSIZE/2)  // number if indirect inodes in record\n+ * #define TP_NINOS    (TP_NINDIR / sizeof (int32_t))\n+ * #define LBLSIZE     16\n+ * #define NAMELEN     64\n+ *\n+ * #define OFS_MAGIC     (int)60011  // old format magic value\n+ * #define NFS_MAGIC     (int)60012  // new format magic value\n+ * #define FS_UFS2_MAGIC (int)0x19540119\n+ * #define CHECKSUM      (int)84446  // constant used in checksum algorithm\n+ *\n+ * struct  s_spcl {\n+ *   int32_t c_type;             // record type (see below)\n+ *   int32_t <b>c_date</b>;             // date of this dump\n+ *   int32_t <b>c_ddate</b>;            // date of previous dump\n+ *   int32_t c_volume;           // dump volume number\n+ *   u_int32_t c_tapea;          // logical block of this record\n+ *   dump_ino_t c_ino;           // number of inode\n+ *   int32_t <b>c_magic</b>;            // magic number (see above)\n+ *   int32_t c_checksum;         // record checksum\n+ * #ifdef  __linux__\n+ *   struct  new_bsd_inode c_dinode;\n+ * #else\n+ * #ifdef sunos\n+ *   struct  new_bsd_inode c_dinode;\n+ * #else\n+ *   struct  dinode  c_dinode;   // ownership and mode of inode\n+ * #endif\n+ * #endif\n+ *   int32_t c_count;            // number of valid c_addr entries\n+ *   union u_data c_data;        // see above\n+ *   char    <b>c_label[LBLSIZE]</b>;   // dump label\n+ *   int32_t <b>c_level</b>;            // level of this dump\n+ *   char    <b>c_filesys[NAMELEN]</b>; // name of dumpped file system\n+ *   char    <b>c_dev[NAMELEN]</b>;     // name of dumpped device\n+ *   char    <b>c_host[NAMELEN]</b>;    // name of dumpped host\n+ *   int32_t c_flags;            // additional information (see below)\n+ *   int32_t c_firstrec;         // first record on volume\n+ *   int32_t c_ntrec;            // blocksize on volume\n+ *   int32_t c_extattributes;    // additional inode info (see below)\n+ *   int32_t c_spare[30];        // reserved for future uses\n+ * } s_spcl;\n+ *\n+ * //\n+ * // flag values\n+ * //\n+ * #define DR_NEWHEADER     0x0001  // new format tape header\n+ * #define DR_NEWINODEFMT   0x0002  // new format inodes on tape\n+ * #define DR_COMPRESSED    0x0080  // dump tape is compressed\n+ * #define DR_METAONLY      0x0100  // only the metadata of the inode has been dumped\n+ * #define DR_INODEINFO     0x0002  // [SIC] TS_END header contains c_inos information\n+ * #define DR_EXTATTRIBUTES 0x8000\n+ *\n+ * //\n+ * // extattributes inode info\n+ * //\n+ * #define EXT_REGULAR         0\n+ * #define EXT_MACOSFNDRINFO   1\n+ * #define EXT_MACOSRESFORK    2\n+ * #define EXT_XATTR           3\n+ *\n+ * // used for EA on tape\n+ * #define EXT2_GOOD_OLD_INODE_SIZE    128\n+ * #define EXT2_XATTR_MAGIC        0xEA020000  // block EA\n+ * #define EXT2_XATTR_MAGIC2       0xEA020001  // in inode EA\n+ * </pre>\n+ * The fields in <b>bold</b> are the same for all blocks. (This permitted\n+ * multiple dumps to be written to a single tape.)\n+ * </p>\n+ *\n+ * <p>\n+ * The C structure for the inode (file) information is:\n+ * <pre>\n+ * struct bsdtimeval {           //  **** alpha-*-linux is deviant\n+ *   __u32   tv_sec;\n+ *   __u32   tv_usec;\n+ * };\n+ *\n+ * #define NDADDR      12\n+ * #define NIADDR       3\n+ *\n+ * //\n+ * // This is the new (4.4) BSD inode structure\n+ * // copied from the FreeBSD 2.0 <ufs/ufs/dinode.h> include file\n+ * //\n+ * struct new_bsd_inode {\n+ *   __u16       di_mode;           // file type, standard Unix permissions\n+ *   __s16       di_nlink;          // number of hard links to file.\n+ *   union {\n+ *      __u16       oldids[2];\n+ *      __u32       inumber;\n+ *   }           di_u;\n+ *   u_quad_t    di_size;           // file size\n+ *   struct bsdtimeval   di_atime;  // time file was last accessed\n+ *   struct bsdtimeval   di_mtime;  // time file was last modified\n+ *   struct bsdtimeval   di_ctime;  // time file was created\n+ *   __u32       di_db[NDADDR];\n+ *   __u32       di_ib[NIADDR];\n+ *   __u32       di_flags;          //\n+ *   __s32       di_blocks;         // number of disk blocks\n+ *   __s32       di_gen;            // generation number\n+ *   __u32       di_uid;            // user id (see /etc/passwd)\n+ *   __u32       di_gid;            // group id (see /etc/group)\n+ *   __s32       di_spare[2];       // unused\n+ * };\n+ * </pre>\n+ * It is important to note that the header DOES NOT have the name of the\n+ * file. It can't since hard links mean that you may have multiple filenames\n+ * for a single physical file. You must read the contents of the directory\n+ * entries to learn the mapping(s) from filename to inode.\n+ * </p>\n+ *\n+ * <p>\n+ * The C structure that indicates if a specific block is a real block\n+ * that contains data or is a sparse block that is not persisted to the\n+ * disk is:\n+ * <pre>\n+ * #define TP_BSIZE    1024\n+ * #define TP_NINDIR   (TP_BSIZE/2)\n+ *\n+ * union u_data {\n+ *   char    s_addrs[TP_NINDIR]; // 1 => data; 0 => hole in inode\n+ *   int32_t s_inos[TP_NINOS];   // table of first inode on each volume\n+ * } u_data;\n+ * </pre></p>\n+ *\n+ * @NotThreadSafe\n+ */\n+public class DumpArchiveEntry implements ArchiveEntry {\n+    private String name;\n+    private TYPE type = TYPE.UNKNOWN;\n+    private int mode;\n+    private Set<PERMISSION> permissions = Collections.emptySet();\n+    private long size;\n+    private Date atime;\n+    private Date mtime;\n+    private int uid;\n+    private int gid;\n+\n+    private DumpArchiveSummary summary = null;\n+\n+    // this information is available from standard index.\n+    private TapeSegmentHeader header = new TapeSegmentHeader();\n+    private String simpleName;\n+\n+    // this information is available from QFA index\n+    private int volume;\n+    private long offset;\n+    private int ino;\n+    private int nlink;\n+    private Date ctime;\n+    private int generation;\n+    private boolean isDeleted;\n+\n+    /**\n+     * Default constructor.\n+     */\n+    public DumpArchiveEntry() {\n+    }\n+\n+    /**\n+     * Constructor taking only filename.\n+     * @param name pathname\n+     * @param simpleName actual filename.\n+     */\n+    public DumpArchiveEntry(String name, String simpleName) {\n+        this.name = name;\n+        this.simpleName = simpleName;\n+    }\n+\n+    /**\n+     * Constructor taking name, inode and type.\n+     *\n+     * @param name\n+     * @param simpleName\n+     * @param ino\n+     * @param type\n+     */\n+    protected DumpArchiveEntry(String name, String simpleName, int ino,\n+                               TYPE type) {\n+        this(name, simpleName);\n+        setType(type);\n+        this.ino = ino;\n+        this.offset = 0;\n+    }\n+\n+    /**\n+     * Constructor taking tape buffer.\n+     * @param buffer\n+     * @param offset\n+     */\n+\n+    /**\n+     * Returns the path of the entry.\n+     * @return\n+     */\n+    public String getSimpleName() {\n+        return simpleName;\n+    }\n+\n+    /**\n+     * Sets the path of the entry.\n+     * @return\n+     */\n+    protected void setSimpleName(String simpleName) {\n+        this.simpleName = simpleName;\n+    }\n+\n+    /**\n+     * Returns the ino of the entry.\n+     */\n+    public int getIno() {\n+        return header.getIno();\n+    }\n+\n+    /**\n+     * Return the number of hard links to the entry.\n+     */\n+    public int getNlink() {\n+        return nlink;\n+    }\n+\n+    /**\n+     * Set the number of hard links.\n+     */\n+    public void setNlink(int nlink) {\n+        this.nlink = nlink;\n+    }\n+\n+    /**\n+     * Get file creation time.\n+     */\n+    public Date getCreationTime() {\n+        return ctime;\n+    }\n+\n+    /**\n+     * Set the file creation time.\n+     */\n+    public void setCreationTime(Date ctime) {\n+        this.ctime = ctime;\n+    }\n+\n+    /**\n+     * Return the generation of the file.\n+     */\n+    public int getGeneration() {\n+        return generation;\n+    }\n+\n+    /**\n+     * Set the generation of the file.\n+     */\n+    public void setGeneration(int generation) {\n+        this.generation = generation;\n+    }\n+\n+    /**\n+     * Has this file been deleted? (On valid on incremental dumps.)\n+     */\n+    public boolean isDeleted() {\n+        return isDeleted;\n+    }\n+\n+    /**\n+     * Set whether this file has been deleted.\n+     */\n+    public void setDeleted(boolean isDeleted) {\n+        this.isDeleted = isDeleted;\n+    }\n+\n+    /**\n+     * Return the offset within the archive\n+     */\n+    public long getOffset() {\n+        return offset;\n+    }\n+\n+    /**\n+     * Set the offset within the archive.\n+     */\n+    public void setOffset(long offset) {\n+        this.offset = offset;\n+    }\n+\n+    /**\n+     * Return the tape volume where this file is located.\n+     */\n+    public int getVolume() {\n+        return volume;\n+    }\n+\n+    /**\n+     * Set the tape volume.\n+     */\n+    public void setVolume(int volume) {\n+        this.volume = volume;\n+    }\n+\n+    /**\n+     * Return the type of the tape segment header.\n+     */\n+    public DumpArchiveConstants.SEGMENT_TYPE getHeaderType() {\n+        return header.getType();\n+    }\n+\n+    /**\n+     * Return the number of records in this segment.\n+     */\n+    public int getHeaderCount() {\n+        return header.getCount();\n+    }\n+\n+    /**\n+     * Return the number of sparse records in this segment.\n+     */\n+    public int getHeaderHoles() {\n+        return header.getHoles();\n+    }\n+\n+    /**\n+     * Is this a sparse record?\n+     */\n+    public boolean isSparseRecord(int idx) {\n+        return (header.getCdata(idx) & 0x01) == 0;\n+    }\n+\n+    /**\n+     * @see java.lang.Object#hashCode()\n+     */\n+    @Override\n+    public int hashCode() {\n+        return ino;\n+    }\n+\n+    /**\n+     * @see java.lang.Object#equals(Object o)\n+     */\n+    @Override\n+    public boolean equals(Object o) {\n+        if (o == this) {\n+            return true;\n+        } else if (!o.getClass().equals(getClass())) {\n+            return false;\n+        }\n+\n+        DumpArchiveEntry rhs = (DumpArchiveEntry) o;\n+\n+        if ((header == null) || (rhs.header == null)) {\n+            return false;\n+        }\n+\n+        if (ino != rhs.ino) {\n+            return false;\n+        }\n+\n+        if ((summary != null) || summary.equals(rhs.summary)) {\n+            return false;\n+        }\n+\n+        return true;\n+    }\n+\n+    /**\n+     * @see java.lang.Object#toString()\n+     */\n+    @Override\n+    public String toString() {\n+        return getName();\n+    }\n+\n+    /**\n+     * Populate the dump archive entry and tape segment header with\n+     * the contents of the buffer.\n+     *\n+     * @param buffer\n+     * @throws Exception\n+     */\n+    public static DumpArchiveEntry parse(byte[] buffer) {\n+        DumpArchiveEntry entry = new DumpArchiveEntry();\n+        TapeSegmentHeader header = entry.header;\n+\n+        header.type = DumpArchiveConstants.SEGMENT_TYPE.find(DumpArchiveUtil.convert32(\n+                    buffer, 0));\n+\n+        //header.dumpDate = new Date(1000L * DumpArchiveUtil.convert32(buffer, 4));\n+        //header.previousDumpDate = new Date(1000L * DumpArchiveUtil.convert32(\n+        //            buffer, 8));\n+        header.volume = DumpArchiveUtil.convert32(buffer, 12);\n+        //header.tapea = DumpArchiveUtil.convert32(buffer, 16);\n+        entry.ino = header.ino = DumpArchiveUtil.convert32(buffer, 20);\n+\n+        //header.magic = DumpArchiveUtil.convert32(buffer, 24);\n+        //header.checksum = DumpArchiveUtil.convert32(buffer, 28);\n+        int m = DumpArchiveUtil.convert16(buffer, 32);\n+\n+        // determine the type of the file.\n+        entry.setType(TYPE.find((m >> 12) & 0x0F));\n+\n+        // determine the standard permissions\n+        entry.setMode(m);\n+\n+        entry.nlink = DumpArchiveUtil.convert16(buffer, 34);\n+        // inumber, oldids?\n+        entry.setSize(DumpArchiveUtil.convert64(buffer, 40));\n+\n+        long t = (1000L * DumpArchiveUtil.convert32(buffer, 48)) +\n+            (DumpArchiveUtil.convert32(buffer, 52) / 1000);\n+        entry.setAccessTime(new Date(t));\n+        t = (1000L * DumpArchiveUtil.convert32(buffer, 56)) +\n+            (DumpArchiveUtil.convert32(buffer, 60) / 1000);\n+        entry.setLastModifiedDate(new Date(t));\n+        t = (1000L * DumpArchiveUtil.convert32(buffer, 64)) +\n+            (DumpArchiveUtil.convert32(buffer, 68) / 1000);\n+        entry.ctime = new Date(t);\n+\n+        // db: 72-119 - direct blocks\n+        // id: 120-131 - indirect blocks\n+        //entry.flags = DumpArchiveUtil.convert32(buffer, 132);\n+        //entry.blocks = DumpArchiveUtil.convert32(buffer, 136);\n+        entry.generation = DumpArchiveUtil.convert32(buffer, 140);\n+        entry.setUserId(DumpArchiveUtil.convert32(buffer, 144));\n+        entry.setGroupId(DumpArchiveUtil.convert32(buffer, 148));\n+        // two 32-bit spare values.\n+        header.count = DumpArchiveUtil.convert32(buffer, 160);\n+\n+        header.holes = 0;\n+\n+        for (int i = 0; (i < 512) && (i < header.count); i++) {\n+            if (buffer[164 + i] == 0) {\n+                header.holes++;\n+            }\n+        }\n+\n+        System.arraycopy(buffer, 164, header.cdata, 0, 512);\n+\n+        entry.volume = header.getVolume();\n+\n+        //entry.isSummaryOnly = false;\n+        return entry;\n+    }\n+\n+    /**\n+     * Update entry with information from next tape segment header.\n+     */\n+    public void update(byte[] buffer) {\n+        header.volume = DumpArchiveUtil.convert32(buffer, 16);\n+        header.count = DumpArchiveUtil.convert32(buffer, 160);\n+\n+        header.holes = 0;\n+\n+        for (int i = 0; (i < 512) && (i < header.count); i++) {\n+            if (buffer[164 + i] == 0) {\n+                header.holes++;\n+            }\n+        }\n+\n+        System.arraycopy(buffer, 164, header.cdata, 0, 512);\n+    }\n+\n+    /**\n+     * Archive entry as stored on tape. There is one TSH for (at most)\n+     * every 512k in the file.\n+     */\n+    public static class TapeSegmentHeader {\n+        private DumpArchiveConstants.SEGMENT_TYPE type;\n+        private int volume;\n+        private int ino;\n+        private int count;\n+        private int holes;\n+        private byte[] cdata = new byte[512]; // map of any 'holes'\n+\n+        public DumpArchiveConstants.SEGMENT_TYPE getType() {\n+            return type;\n+        }\n+\n+        public int getVolume() {\n+            return volume;\n+        }\n+\n+        public int getIno() {\n+            return ino;\n+        }\n+\n+        void setIno(int ino) {\n+            this.ino = ino;\n+        }\n+\n+        public int getCount() {\n+            return count;\n+        }\n+\n+        public int getHoles() {\n+            return holes;\n+        }\n+\n+        public int getCdata(int idx) {\n+            return cdata[idx];\n+        }\n+    }\n+\n+    /**\n+     * Returns the name of the entry.\n+     * @return\n+     */\n+    public String getName() {\n+        return name;\n+    }\n+\n+    /**\n+     * Sets the name of the entry.\n+     * @return\n+     */\n+    public void setName(String name) {\n+        this.name = name;\n+    }\n+\n+    /** {@inheritDoc} */\n+    public Date getLastModifiedDate() {\n+        return mtime;\n+    }\n+\n+    /**\n+     * Is this a directory?\n+     */\n+    public boolean isDirectory() {\n+        return type == TYPE.DIRECTORY;\n+    }\n+\n+    /**\n+     * Is this a regular file?\n+     */\n+    public boolean isFile() {\n+        return type == TYPE.FILE;\n+    }\n+\n+    /**\n+     * Is this a network device?\n+     */\n+    public boolean isSocket() {\n+        return type == TYPE.SOCKET;\n+    }\n+\n+    /**\n+     * Is this a character device?\n+     */\n+    public boolean isChrDev() {\n+        return type == TYPE.CHRDEV;\n+    }\n+\n+    /**\n+     * Is this a block device?\n+     */\n+    public boolean isBlkDev() {\n+        return type == TYPE.BLKDEV;\n+    }\n+\n+    /**\n+     * Is this a fifo/pipe?\n+     */\n+    public boolean isFifo() {\n+        return type == TYPE.FIFO;\n+    }\n+\n+    /**\n+     * Get the type of the entry.\n+     */\n+    public TYPE getType() {\n+        return type;\n+    }\n+\n+    /**\n+     * Set the type of the entry.\n+     */\n+    public void setType(TYPE type) {\n+        this.type = type;\n+    }\n+\n+    /**\n+     * Return the access permissions on the entry.\n+     */\n+    public int getMode() {\n+        return mode;\n+    }\n+\n+    /**\n+     * Set the access permissions on the entry.\n+     */\n+    public void setMode(int mode) {\n+        this.mode = mode & 07777;\n+        this.permissions = PERMISSION.find(mode);\n+    }\n+\n+    /**\n+     * Returns the permissions on the entry.\n+     */\n+    public Set<PERMISSION> getPermissions() {\n+        return permissions;\n+    }\n+\n+    /**\n+     * Returns the size of the entry.\n+     */\n+    public long getSize() {\n+        return size;\n+    }\n+\n+    /**\n+     * Set the size of the entry.\n+     */\n+    public void setSize(long size) {\n+        this.size = size;\n+    }\n+\n+    /**\n+     * Set the time the file was last modified.\n+     */\n+    public void setLastModifiedDate(Date mtime) {\n+        this.mtime = mtime;\n+    }\n+\n+    /**\n+     * Returns the time the file was last accessed.\n+     */\n+    public Date getAccessTime() {\n+        return atime;\n+    }\n+\n+    /**\n+     * Set the time the file was last accessed.\n+     */\n+    public void setAccessTime(Date atime) {\n+        this.atime = atime;\n+    }\n+\n+    /**\n+     * Return the user id.\n+     */\n+    public int getUserId() {\n+        return uid;\n+    }\n+\n+    /**\n+     * Set the user id.\n+     */\n+    public void setUserId(int uid) {\n+        this.uid = uid;\n+    }\n+\n+    /**\n+     * Return the group id\n+     */\n+    public int getGroupId() {\n+        return gid;\n+    }\n+\n+    /**\n+     * Set the group id.\n+     */\n+    public void setGroupId(int gid) {\n+        this.gid = gid;\n+    }\n+\n+    public enum TYPE {\n+        WHITEOUT(14),\n+        SOCKET(12),\n+        LINK(10),\n+        FILE(8),\n+        BLKDEV(6),\n+        DIRECTORY(4),\n+        CHRDEV(2),\n+        FIFO(1),\n+        UNKNOWN(15);\n+    \t\n+        private int code;\n+\n+        private TYPE(int code) {\n+            this.code = code;\n+        }\n+\n+        public static TYPE find(int code) {\n+            TYPE type = UNKNOWN;\n+\n+            for (TYPE t : TYPE.values()) {\n+                if (code == t.code) {\n+                    type = t;\n+                }\n+            }\n+\n+            return type;\n+        }\n+    }\n+    \n+    public enum PERMISSION {\n+        SETUID(04000),\n+        SETGUI(02000),\n+        STICKY(01000),\n+        USER_READ(00400),\n+        USER_WRITE(00200),\n+        USER_EXEC(00100),\n+        GROUP_READ(00040),\n+        GROUP_WRITE(00020),\n+        GROUP_EXEC(00010),\n+        WORLD_READ(00004),\n+        WORLD_WRITE(00002),\n+        WORLD_EXEC(00001);\n+    \t\n+        private int code;\n+\n+        private PERMISSION(int code) {\n+            this.code = code;\n+        }\n+\n+        public static Set<PERMISSION> find(int code) {\n+            Set<PERMISSION> set = new HashSet<PERMISSION>();\n+\n+            for (PERMISSION p : PERMISSION.values()) {\n+                if ((code & p.code) == p.code) {\n+                    set.add(p);\n+                }\n+            }\n+\n+            if (set.isEmpty()) {\n+                return Collections.emptySet();\n+            }\n+\n+            return EnumSet.copyOf(set);\n+        }\n+    }\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/compress/archivers/dump/DumpArchiveException.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.commons.compress.archivers.dump;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Dump Archive Exception\n+ */\n+public class DumpArchiveException extends IOException {\n+    private static final long serialVersionUID = 1L;\n+\n+    public DumpArchiveException() {\n+    }\n+\n+    public DumpArchiveException(String msg) {\n+        super(msg);\n+    }\n+\n+    public DumpArchiveException(Throwable cause) {\n+        super(cause);\n+    }\n+\n+    public DumpArchiveException(String msg, Throwable cause) {\n+        super(msg, cause);\n+    }\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/compress/archivers/dump/DumpArchiveInputStream.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.commons.compress.archivers.dump;\n+\n+import org.apache.commons.compress.archivers.ArchiveInputStream;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.Queue;\n+import java.util.Stack;\n+\n+\n+/**\n+ * The DumpArchiveInputStream reads a UNIX dump archive as an InputStream.\n+ * Methods are provided to position at each successive entry in\n+ * the archive, and the read each entry as a normal input stream\n+ * using read().\n+ *\n+ * @NotThreadSafe\n+ */\n+public class DumpArchiveInputStream extends ArchiveInputStream {\n+    private DumpArchiveSummary summary;\n+    private DumpArchiveEntry active;\n+    private boolean isClosed;\n+    private boolean hasHitEOF;\n+    private long entrySize;\n+    private long entryOffset;\n+    private int readIdx;\n+    private byte[] readBuf = new byte[DumpArchiveConstants.TP_SIZE];\n+    private byte[] blockBuffer;\n+    private int recordOffset;\n+    private long filepos;\n+    protected TapeInputStream raw;\n+\n+    // map of ino -> dirent entry. We can use this to reconstruct full paths.\n+    private Map<Integer, Dirent> names = new HashMap<Integer, Dirent>();\n+\n+    // map of ino -> (directory) entry when we're missing one or more elements in the path.\n+    private Map<Integer, DumpArchiveEntry> pending = new HashMap<Integer, DumpArchiveEntry>();\n+\n+    // queue of (directory) entries where we now have the full path.\n+    private Queue<DumpArchiveEntry> queue;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param is\n+     * @throws Exception\n+     */\n+    public DumpArchiveInputStream(InputStream is) throws IOException {\n+        this.raw = new TapeInputStream(is);\n+        this.hasHitEOF = false;\n+\n+        // read header, verify it's a dump archive.\n+        byte[] headerBytes = raw.readRecord();\n+\n+        if (!DumpArchiveUtil.verify(headerBytes)) {\n+            throw new UnrecognizedFormatException();\n+        }\n+\n+        // get summary information\n+        summary = new DumpArchiveSummary(headerBytes);\n+\n+        // reset buffer with actual block size.\n+        raw.resetBlockSize(summary.getNTRec(), summary.isCompressed());\n+\n+        // allocate our read buffer.\n+        blockBuffer = new byte[4 * DumpArchiveConstants.TP_SIZE];\n+\n+        // skip past CLRI and BITS segments since we don't handle them yet.\n+        readCLRI();\n+        readBITS();\n+\n+        // put in a dummy record for the root node.\n+        Dirent root = new Dirent(2, 2, 4, \".\");\n+        names.put(2, root);\n+\n+        // use priority based on queue to ensure parent directories are\n+        // released first.\n+        queue = new PriorityQueue<DumpArchiveEntry>(10,\n+                new Comparator<DumpArchiveEntry>() {\n+                    public int compare(DumpArchiveEntry p, DumpArchiveEntry q) {\n+                        if ((p.getName() == null) || (q.getName() == null)) {\n+                            return Integer.MAX_VALUE;\n+                        }\n+\n+                        return p.getName().compareTo(q.getName());\n+                    }\n+                });\n+    }\n+\n+    /**\n+     * Return the archive summary information.\n+     */\n+    public DumpArchiveSummary getSummary() {\n+        return summary;\n+    }\n+\n+    /**\n+     * Read CLRI (deleted inode) segment.\n+     */\n+    public void readCLRI() throws IOException {\n+        byte[] readBuf = raw.readRecord();\n+\n+        if (!DumpArchiveUtil.verify(readBuf)) {\n+            throw new InvalidFormatException();\n+        }\n+\n+        active = DumpArchiveEntry.parse(readBuf);\n+\n+        if (DumpArchiveConstants.SEGMENT_TYPE.CLRI != active.getHeaderType()) {\n+            throw new InvalidFormatException();\n+        }\n+\n+        // we don't do anything with this yet.\n+        raw.skip(DumpArchiveConstants.TP_SIZE * active.getHeaderCount());\n+        readIdx = active.getHeaderCount();\n+    }\n+\n+    /**\n+     * Read BITS segment.\n+     */\n+    public void readBITS() throws IOException {\n+        byte[] readBuf = raw.readRecord();\n+\n+        if (!DumpArchiveUtil.verify(readBuf)) {\n+            throw new InvalidFormatException();\n+        }\n+\n+        active = DumpArchiveEntry.parse(readBuf);\n+\n+        if (DumpArchiveConstants.SEGMENT_TYPE.BITS != active.getHeaderType()) {\n+            throw new InvalidFormatException();\n+        }\n+\n+        // we don't do anything with this yet.\n+        raw.skip(DumpArchiveConstants.TP_SIZE * active.getHeaderCount());\n+        readIdx = active.getHeaderCount();\n+    }\n+\n+    /**\n+     * Read the next entry.\n+     */\n+    public DumpArchiveEntry getNextEntry() throws IOException {\n+        DumpArchiveEntry entry = null;\n+        String path = null;\n+\n+        // is there anything in the queue?\n+        if (!queue.isEmpty()) {\n+            return queue.remove();\n+        }\n+\n+        while (entry == null) {\n+            if (hasHitEOF) {\n+                return null;\n+            }\n+\n+            // skip any remaining records in this segment for prior file.\n+            // we might still have holes... easiest to do it\n+            // block by block. We may want to revisit this if\n+            // the unnecessary decompression time adds up.\n+            while (readIdx < active.getHeaderCount()) {\n+                if (!active.isSparseRecord(readIdx++)) {\n+                    raw.skip(DumpArchiveConstants.TP_SIZE);\n+                }\n+            }\n+\n+            readIdx = 0;\n+            filepos = raw.getBytesRead();\n+\n+            byte[] headerBytes = raw.readRecord();\n+\n+            if (!DumpArchiveUtil.verify(headerBytes)) {\n+                throw new InvalidFormatException();\n+            }\n+\n+            active = DumpArchiveEntry.parse(headerBytes);\n+\n+            // skip any remaining segments for prior file.\n+            while (DumpArchiveConstants.SEGMENT_TYPE.ADDR == active.getHeaderType()) {\n+                raw.skip(DumpArchiveConstants.TP_SIZE * (active.getHeaderCount() -\n+                    active.getHeaderHoles()));\n+\n+                filepos = raw.getBytesRead();\n+                headerBytes = raw.readRecord();\n+\n+                if (!DumpArchiveUtil.verify(headerBytes)) {\n+                    throw new InvalidFormatException();\n+                }\n+\n+                active = DumpArchiveEntry.parse(headerBytes);\n+            }\n+\n+            // check if this is an end-of-volume marker.\n+            if (DumpArchiveConstants.SEGMENT_TYPE.END == active.getHeaderType()) {\n+                hasHitEOF = true;\n+                raw.close();\n+\n+                return null;\n+            }\n+\n+            entry = active;\n+\n+            if (entry.isDirectory()) {\n+                readDirectoryEntry(active);\n+\n+                // now we create an empty InputStream.\n+                entryOffset = 0;\n+                entrySize = 0;\n+                readIdx = active.getHeaderCount();\n+            } else {\n+                entryOffset = 0;\n+                entrySize = active.getSize();\n+                readIdx = 0;\n+            }\n+\n+            recordOffset = readBuf.length;\n+\n+            path = getPath(entry);\n+\n+            if (path == null) {\n+                entry = null;\n+            }\n+        }\n+\n+        entry.setName(path);\n+        entry.setSimpleName(names.get(entry.getIno()).getName());\n+        entry.setOffset(filepos);\n+\n+        isClosed = false;\n+\n+        return entry;\n+    }\n+\n+    /**\n+     * Read directory entry.\n+     */\n+    public void readDirectoryEntry(DumpArchiveEntry entry)\n+        throws IOException {\n+        long size = entry.getSize();\n+        boolean first = true;\n+\n+        while (first ||\n+                (DumpArchiveConstants.SEGMENT_TYPE.ADDR == entry.getHeaderType())) {\n+            // read the header that we just peeked at.\n+            if (!first) {\n+                raw.readRecord();\n+            }\n+\n+            if (!names.containsKey(entry.getIno()) &&\n+                    (DumpArchiveConstants.SEGMENT_TYPE.INODE == entry.getHeaderType())) {\n+                pending.put(entry.getIno(), entry);\n+            }\n+\n+            int datalen = DumpArchiveConstants.TP_SIZE * entry.getHeaderCount();\n+\n+            if (blockBuffer.length < datalen) {\n+                blockBuffer = new byte[datalen];\n+            }\n+\n+            raw.read(blockBuffer, 0, datalen);\n+\n+            int reclen = 0;\n+\n+            for (int i = 0; (i < (datalen - 8)) && (i < (size - 8));\n+                    i += reclen) {\n+                int ino = DumpArchiveUtil.convert32(blockBuffer, i);\n+                reclen = DumpArchiveUtil.convert16(blockBuffer, i + 4);\n+\n+                byte type = blockBuffer[i + 6];\n+\n+                String name = new String(blockBuffer, i + 8, blockBuffer[i + 7]);\n+\n+                if (\".\".equals(name) || \"..\".equals(name)) {\n+                    // do nothing...\n+                    continue;\n+                }\n+\n+                Dirent d = new Dirent(ino, entry.getIno(), type, name);\n+\n+                if ((type == 4) && names.containsKey(ino)) {\n+                    //System.out.println(\"we already have ino: \" +\n+                    //    names.get(ino));\n+                }\n+\n+                names.put(ino, d);\n+\n+                // check whether this allows us to fill anything in the pending list.\n+                for (Map.Entry<Integer, DumpArchiveEntry> e : pending.entrySet()) {\n+                    String path = getPath(e.getValue());\n+\n+                    if (path != null) {\n+                        e.getValue().setName(path);\n+                        e.getValue()\n+                         .setSimpleName(names.get(e.getKey()).getName());\n+                        queue.add(e.getValue());\n+                    }\n+                }\n+\n+                // remove anything that we found. (We can't do it earlier\n+                // because of concurrent modification exceptions.)\n+                for (DumpArchiveEntry e : queue) {\n+                    pending.remove(e.getIno());\n+                }\n+            }\n+\n+            byte[] peekBytes = raw.peek();\n+\n+            if (!DumpArchiveUtil.verify(peekBytes)) {\n+                throw new InvalidFormatException();\n+            }\n+\n+            entry = DumpArchiveEntry.parse(peekBytes);\n+            first = false;\n+            size -= DumpArchiveConstants.TP_SIZE;\n+        }\n+    }\n+\n+    /**\n+     * Get full path for specified archive entry, or null if there's a gap.\n+     *\n+     * @param entry\n+     * @return\n+     */\n+    public String getPath(DumpArchiveEntry entry) {\n+        // build the stack of elements. It's possible that we're \n+        // still missing an intermediate value and if so we\n+        Stack<String> elements = new Stack<String>();\n+        Dirent dirent = null;\n+\n+outer: \n+        for (int i = entry.getIno();; i = dirent.getParentIno()) {\n+            if (!names.containsKey(i)) {\n+                elements.clear();\n+\n+                break outer;\n+            }\n+\n+            dirent = names.get(i);\n+            elements.push(dirent.getName());\n+\n+            if (dirent.getIno() == dirent.getParentIno()) {\n+                break;\n+            }\n+        }\n+\n+        // if an element is missing defer the work and read next entry.\n+        if (elements.isEmpty()) {\n+            pending.put(entry.getIno(), entry);\n+\n+            return null;\n+        }\n+\n+        // generate full path from stack of elements.\n+        StringBuilder sb = new StringBuilder(elements.pop());\n+\n+        while (!elements.isEmpty()) {\n+            sb.append('/');\n+            sb.append(elements.pop());\n+        }\n+\n+        return sb.toString();\n+    }\n+\n+    /**\n+     * Reads bytes from the current dump archive entry.\n+     *\n+     * This method is aware of the boundaries of the current\n+     * entry in the archive and will deal with them as if they\n+     * were this stream's start and EOF.\n+     *\n+     * @param buf The buffer into which to place bytes read.\n+     * @param off The offset at which to place bytes read.\n+     * @param len The number of bytes to read.\n+     * @return The number of bytes read, or -1 at EOF.\n+     * @throws IOException on error\n+     */\n+    public int read(byte[] buf, int off, int len) throws IOException {\n+        int totalRead = 0;\n+\n+        if (isClosed || (entryOffset >= entrySize)) {\n+            return -1;\n+        }\n+\n+        if ((len + entryOffset) > entrySize) {\n+            len = (int) (entrySize - entryOffset);\n+        }\n+\n+        while (len > 0) {\n+            int sz = (len > (readBuf.length - recordOffset))\n+                ? (readBuf.length - recordOffset) : len;\n+\n+            // copy any data we have\n+            if ((recordOffset + sz) <= readBuf.length) {\n+                System.arraycopy(readBuf, recordOffset, buf, off, sz);\n+                totalRead += sz;\n+                recordOffset += sz;\n+                len -= sz;\n+                off += sz;\n+            }\n+\n+            // load next block if necessary.\n+            if (len > 0) {\n+                if (readIdx >= 512) {\n+                    byte[] headerBytes = raw.readRecord();\n+\n+                    if (!DumpArchiveUtil.verify(headerBytes)) {\n+                        throw new InvalidFormatException();\n+                    }\n+\n+                    active = DumpArchiveEntry.parse(headerBytes);\n+                    readIdx = 0;\n+                }\n+\n+                if (!active.isSparseRecord(readIdx++)) {\n+                    raw.read(readBuf, 0, readBuf.length);\n+                } else {\n+                    Arrays.fill(readBuf, (byte) 0);\n+                }\n+\n+                recordOffset = 0;\n+            }\n+        }\n+\n+        entryOffset += totalRead;\n+\n+        return totalRead;\n+    }\n+\n+    /**\n+     * Closes the stream for this entry.\n+     */\n+    @Override\n+    public void close() throws IOException {\n+        isClosed = true;\n+    }\n+\n+    /**\n+     * Look at the first few bytes of the file to decide if it's a dump\n+     * archive. With 32 bytes we can look at the magic value, with a full\n+     * 1k we can verify the checksum.\n+     */\n+    public static boolean matches(byte[] buffer, int length) {\n+        // do we have enough of the header?\n+        if (length < 32) {\n+            return false;\n+        }\n+\n+        // this is the best test\n+        if (length >= DumpArchiveConstants.TP_SIZE) {\n+            return DumpArchiveUtil.verify(buffer);\n+        }\n+\n+        // this will work in a pinch.\n+        return DumpArchiveConstants.NFS_MAGIC == DumpArchiveUtil.convert32(buffer,\n+            7);\n+    }\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/compress/archivers/dump/DumpArchiveSummary.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.commons.compress.archivers.dump;\n+\n+import java.util.Date;\n+\n+\n+/**\n+ * This class represents identifying information about a Dump archive volume.\n+ * It consists the archive's dump date, label, hostname, device name and possibly\n+ * last mount point plus the volume's volume id andfirst record number.\n+ *\n+ * For the corresponding C structure see the header of {@link DumpArchiveEntry}.\n+ */\n+public class DumpArchiveSummary {\n+    private Date dumpDate;\n+    private Date previousDumpDate;\n+    private int volume;\n+    private String label;\n+    private int level;\n+    private String filesys;\n+    private String devname;\n+    private String hostname;\n+    private int flags;\n+    private int firstrec;\n+    private int ntrec;\n+\n+    public DumpArchiveSummary(byte[] buffer) {\n+        dumpDate = new Date(1000L * DumpArchiveUtil.convert32(buffer, 4));\n+        previousDumpDate = new Date(1000L * DumpArchiveUtil.convert32(buffer, 8));\n+        volume = DumpArchiveUtil.convert32(buffer, 12);\n+        label = new String(buffer, 676, DumpArchiveConstants.LBLSIZE).trim();\n+        level = DumpArchiveUtil.convert32(buffer, 692);\n+        filesys = new String(buffer, 696, DumpArchiveConstants.NAMELEN).trim();\n+        devname = new String(buffer, 760, DumpArchiveConstants.NAMELEN).trim();\n+        hostname = new String(buffer, 824, DumpArchiveConstants.NAMELEN).trim();\n+        flags = DumpArchiveUtil.convert32(buffer, 888);\n+        firstrec = DumpArchiveUtil.convert32(buffer, 892);\n+        ntrec = DumpArchiveUtil.convert32(buffer, 896);\n+\n+        //extAttributes = DumpArchiveUtil.convert32(buffer, 900);\n+    }\n+\n+    /**\n+     * Get the date of this dump.\n+     * @return\n+     */\n+    public Date getDumpDate() {\n+        return dumpDate;\n+    }\n+\n+    /**\n+     * Set dump date.\n+     */\n+    public void setDumpDate(Date dumpDate) {\n+        this.dumpDate = dumpDate;\n+    }\n+\n+    /**\n+     * Get the date of the previous dump at this level higher.\n+     * @return dumpdate may be null\n+     */\n+    public Date getPreviousDumpDate() {\n+        return previousDumpDate;\n+    }\n+\n+    /**\n+     * Set previous dump date.\n+     */\n+    public void setPreviousDumpDate(Date previousDumpDate) {\n+        this.previousDumpDate = previousDumpDate;\n+    }\n+\n+    /**\n+     * Get volume (tape) number.\n+     * @return\n+     */\n+    public int getVolume() {\n+        return volume;\n+    }\n+\n+    /**\n+     * Set volume (tape) number.\n+     */\n+    public void setVolume(int volume) {\n+        this.volume = volume;\n+    }\n+\n+    /**\n+     * Get the level of this dump. This is a number between 0 and 9, inclusive,\n+     * and a level 0 dump is a complete dump of the partition. For any other dump\n+     * 'n' this dump contains all files that have changed since the last dump\n+     * at this level or lower. This is used to support different levels of\n+     * incremental backups.\n+     * @return\n+     */\n+    public int getLevel() {\n+        return level;\n+    }\n+\n+    /**\n+     * Set level.\n+     */\n+    public void setLevel(int level) {\n+        this.level = level;\n+    }\n+\n+    /**\n+     * Get dump label. This may be autogenerated or it may be specified\n+     * bu the user.\n+     * @return\n+     */\n+    public String getLabel() {\n+        return label;\n+    }\n+\n+    /**\n+     * Set dump label.\n+     * @param label\n+     */\n+    public void setLabel(String label) {\n+        this.label = label;\n+    }\n+\n+    /**\n+     * Get the last mountpoint, e.g., /home.\n+     * @return\n+     */\n+    public String getFilesystem() {\n+        return filesys;\n+    }\n+\n+    /**\n+     * Set the last mountpoint.\n+     */\n+    public void setFilesystem(String filesystem) {\n+        this.filesys = filesystem;\n+    }\n+\n+    /**\n+     * Get the device name, e.g., /dev/sda3 or /dev/mapper/vg0-home.\n+     * @return\n+     */\n+    public String getDevname() {\n+        return devname;\n+    }\n+\n+    /**\n+     * Set the device name.\n+     * @param devname\n+     */\n+    public void setDevname(String devname) {\n+        this.devname = devname;\n+    }\n+\n+    /**\n+     * Get the hostname of the system where the dump was performed.\n+     * @return\n+     */\n+    public String getHostname() {\n+        return hostname;\n+    }\n+\n+    /**\n+     * Set the hostname.\n+     */\n+    public void setHostname(String hostname) {\n+        this.hostname = hostname;\n+    }\n+\n+    /**\n+     * Get the miscellaneous flags. See below.\n+     * @return\n+     */\n+    public int getFlags() {\n+        return flags;\n+    }\n+\n+    /**\n+     * Set the miscellaneous flags.\n+     * @param flags\n+     */\n+    public void setFlags(int flags) {\n+        this.flags = flags;\n+    }\n+\n+    /**\n+     * Get the inode of the first record on this volume.\n+     * @return\n+     */\n+    public int getFirstRecord() {\n+        return firstrec;\n+    }\n+\n+    /**\n+     * Set the inode of the first record.\n+     * @param firstrec\n+     */\n+    public void setFirstRecord(int firstrec) {\n+        this.firstrec = firstrec;\n+    }\n+\n+    /**\n+     * Get the number of records per tape block. This is typically\n+     * between 10 and 32.\n+     * @return\n+     */\n+    public int getNTRec() {\n+        return ntrec;\n+    }\n+\n+    /**\n+     * Set the number of records per tape block.\n+     */\n+    public void setNTRec(int ntrec) {\n+        this.ntrec = ntrec;\n+    }\n+\n+    /**\n+     * Is this the new header format? (We do not currently support the\n+     * old format.)\n+     *\n+     * @return\n+     */\n+    public boolean isNewHeader() {\n+        return (flags & 0x0001) == 0x0001;\n+    }\n+\n+    /**\n+     * Is this the new inode format? (We do not currently support the\n+     * old format.)\n+     * @return\n+     */\n+    public boolean isNewInode() {\n+        return (flags & 0x0002) == 0x0002;\n+    }\n+\n+    /**\n+     * Is this volume compressed? N.B., individual blocks may or may not be compressed.\n+     * The first block is never compressed.\n+     * @return\n+     */\n+    public boolean isCompressed() {\n+        return (flags & 0x0080) == 0x0080;\n+    }\n+\n+    /**\n+     * Does this volume only contain metadata?\n+     * @return\n+     */\n+    public boolean isMetaDataOnly() {\n+        return (flags & 0x0100) == 0x0100;\n+    }\n+\n+    /**\n+     * Does this volume cotain extended attributes.\n+     * @return\n+     */\n+    public boolean isExtendedAttributes() {\n+        return (flags & 0x8000) == 0x8000;\n+    }\n+\n+    /**\n+     * @see java.lang.hashCode()\n+     */\n+    @Override\n+    public int hashCode() {\n+        int hash = 17;\n+\n+        if (label != null) {\n+            hash = label.hashCode();\n+        }\n+\n+        if (dumpDate != null) {\n+            hash = (31 * dumpDate.hashCode()) + 17;\n+        }\n+\n+        if (hostname != null) {\n+            hash = (31 * hostname.hashCode()) + 17;\n+        }\n+\n+        if (devname != null) {\n+            hash = (31 * devname.hashCode()) + 17;\n+        }\n+\n+        return hash;\n+    }\n+\n+    /**\n+     * @see java.lang.Object#equals(Object)\n+     */\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) {\n+            return true;\n+        }\n+\n+        if (!(o instanceof DumpArchiveSummary)) {\n+            return false;\n+        }\n+\n+        DumpArchiveSummary rhs = (DumpArchiveSummary) o;\n+\n+        if ((dumpDate == null) || !dumpDate.equals(rhs.dumpDate)) {\n+            return false;\n+        }\n+\n+        if ((getHostname() == null) ||\n+                !getHostname().equals(rhs.getHostname())) {\n+            return false;\n+        }\n+\n+        if ((getDevname() == null) || !getDevname().equals(rhs.getDevname())) {\n+            return false;\n+        }\n+\n+        return true;\n+    }\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/compress/archivers/dump/DumpArchiveUtil.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.commons.compress.archivers.dump;\n+\n+\n+/**\n+ * Various utilities for dump archives.\n+ */\n+public class DumpArchiveUtil {\n+    /**\n+     * Private constructor to prevent instantiation.\n+     */\n+    private DumpArchiveUtil() {\n+    }\n+\n+    /**\n+     * Calculate checksum for buffer.\n+     *\n+     * @param buffer buffer containing tape segment header\n+     * @returns checksum\n+     */\n+    public static int calculateChecksum(byte[] buffer) {\n+        int calc = 0;\n+\n+        for (int i = 0; i < 256; i++) {\n+            calc += DumpArchiveUtil.convert32(buffer, 4 * i);\n+        }\n+\n+        return DumpArchiveConstants.CHECKSUM -\n+        (calc - DumpArchiveUtil.convert32(buffer, 28));\n+    }\n+\n+    /**\n+     * Verify that the buffer contains a tape segment header.\n+     *\n+     * @param buffer\n+     * @throws Exception\n+     */\n+    public static final boolean verify(byte[] buffer) {\n+        // verify magic. for now only accept NFS_MAGIC.\n+        int magic = convert32(buffer, 24);\n+\n+        if (magic != DumpArchiveConstants.NFS_MAGIC) {\n+            return false;\n+        }\n+\n+        //verify checksum...\n+        int checksum = convert32(buffer, 28);\n+\n+        if (checksum != calculateChecksum(buffer)) {\n+            return false;\n+        }\n+\n+        return true;\n+    }\n+\n+    /**\n+     * Get the ino associated with this buffer.\n+     *\n+     * @param buffer\n+     * @throws Exception\n+     */\n+    public static final int getIno(byte[] buffer) {\n+        return convert32(buffer, 20);\n+    }\n+\n+    /**\n+     * Read 8-byte integer from buffer.\n+     *\n+     * @param buffer\n+     * @param offset\n+     * @return\n+     */\n+    public static final long convert64(byte[] buffer, int offset) {\n+        long i = 0;\n+        i += (((long) buffer[offset + 7]) << 56);\n+        i += (((long) buffer[offset + 6] << 48) & 0x00FF000000000000L);\n+        i += (((long) buffer[offset + 5] << 40) & 0x0000FF0000000000L);\n+        i += (((long) buffer[offset + 4] << 32) & 0x000000FF00000000L);\n+        i += (((long) buffer[offset + 3] << 24) & 0x00000000FF000000L);\n+        i += (((long) buffer[offset + 2] << 16) & 0x0000000000FF0000L);\n+        i += (((long) buffer[offset + 1] << 8) & 0x000000000000FF00L);\n+        i += (((long) buffer[offset]) & 0x00000000000000FFL);\n+\n+        return i;\n+    }\n+\n+    /**\n+     * Read 4-byte integer from buffer.\n+     *\n+     * @param buffer\n+     * @param offset\n+     * @return\n+     */\n+    public static final int convert32(byte[] buffer, int offset) {\n+        int i = 0;\n+        i = ((int) buffer[offset + 3]) << 24;\n+        i += (((int) buffer[offset + 2] << 16) & 0x00FF0000);\n+        i += (((int) buffer[offset + 1] << 8) & 0x0000FF00);\n+        i += (((int) buffer[offset]) & 0x000000FF);\n+\n+        return i;\n+    }\n+\n+    /**\n+     * Read 2-byte integer from buffer.\n+     *\n+     * @param buffer\n+     * @param offset\n+     * @return\n+     */\n+    public static final int convert16(byte[] buffer, int offset) {\n+        int i = 0;\n+        i += (((int) buffer[offset + 1] << 8) & 0x0000FF00);\n+        i += (((int) buffer[offset]) & 0x000000FF);\n+\n+        return i;\n+    }\n+\n+    /**\n+     * Dump the start of a block.\n+     *\n+     * @param buffer\n+     */\n+    public static final void dumpBlock(byte[] buffer) {\n+        for (int i = 0; i < 128; i += 32) {\n+            System.out.printf(\"%08x \", DumpArchiveUtil.convert32(buffer, i));\n+            System.out.printf(\"%08x \", DumpArchiveUtil.convert32(buffer, i + 4));\n+            System.out.printf(\"%08x \", DumpArchiveUtil.convert32(buffer, i + 8));\n+            System.out.printf(\"%08x - \",\n+                DumpArchiveUtil.convert32(buffer, i + 12));\n+            System.out.printf(\"%08x \", DumpArchiveUtil.convert32(buffer, i +\n+                    16));\n+            System.out.printf(\"%08x \", DumpArchiveUtil.convert32(buffer, i +\n+                    20));\n+            System.out.printf(\"%08x \", DumpArchiveUtil.convert32(buffer, i +\n+                    24));\n+            System.out.printf(\"%08x \", DumpArchiveUtil.convert32(buffer, i +\n+                    28));\n+            System.out.println();\n+        }\n+    }\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/compress/archivers/dump/InvalidFormatException.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.commons.compress.archivers.dump;\n+\n+\n+/**\n+ * Invalid Format Exception. There was an error decoding a\n+ * tape segment header.\n+ */\n+public class InvalidFormatException extends DumpArchiveException {\n+    private static final long serialVersionUID = 1L;\n+    protected long offset;\n+\n+    public InvalidFormatException() {\n+        super(\"there was an error decoding a tape segment\");\n+    }\n+\n+    public InvalidFormatException(long offset) {\n+        super(\"there was an error decoding a tape segment header at offset \" +\n+            offset + \".\");\n+        this.offset = offset;\n+    }\n+\n+    public long getOffset() {\n+        return offset;\n+    }\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/compress/archivers/dump/ShortFileException.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.commons.compress.archivers.dump;\n+\n+\n+/**\n+ * Short File Exception. There was an unexpected EOF when reading\n+ * the input stream.\n+ */\n+public class ShortFileException extends DumpArchiveException {\n+    private static final long serialVersionUID = 1L;\n+\n+    public ShortFileException() {\n+        super(\"unexpected EOF\");\n+    }\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/compress/archivers/dump/TapeInputStream.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.commons.compress.archivers.dump;\n+\n+import java.io.FilterInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+import java.util.Arrays;\n+import java.util.zip.DataFormatException;\n+import java.util.zip.Inflater;\n+\n+\n+/**\n+ * Filter stream that mimics a physical tape drive capable of compressing\n+ * the data stream.\n+ *\n+ * @NotThreadSafe\n+ */\n+public class TapeInputStream extends FilterInputStream {\n+    private byte[] blockBuffer = new byte[DumpArchiveConstants.TP_SIZE];\n+    private int currBlkIdx = -1;\n+    private int blockSize = DumpArchiveConstants.TP_SIZE;\n+    private int recordSize = DumpArchiveConstants.TP_SIZE;\n+    private int readOffset = DumpArchiveConstants.TP_SIZE;\n+    private boolean isCompressed = false;\n+    private long bytesRead = 0;\n+\n+    /**\n+     * Constructor\n+     */\n+    public TapeInputStream(InputStream in) {\n+        super(in);\n+    }\n+\n+    /**\n+     * Set the DumpArchive Buffer's block size. We need to sync the block size with the\n+     * dump archive's actual block size since compression is handled at the\n+     * block level.\n+     *\n+     * @param blockSize\n+     *            actual blockSize according to dump archive\n+     * @param isCompressed\n+     *            true if the archive is compressed\n+     * @throws IOException\n+     *             more than one block has been read\n+     * @throws IOException\n+     *             there was an error reading additional blocks.\n+     */\n+    public void resetBlockSize(int recsPerBlock, boolean isCompressed)\n+        throws IOException {\n+        this.isCompressed = isCompressed;\n+\n+        blockSize = recordSize * recsPerBlock;\n+\n+        // save first block in case we need it again\n+        byte[] oldBuffer = blockBuffer;\n+\n+        // read rest of new block\n+        blockBuffer = new byte[blockSize];\n+        System.arraycopy(oldBuffer, 0, blockBuffer, 0, recordSize);\n+        readFully(blockBuffer, recordSize, blockSize - recordSize);\n+\n+        this.currBlkIdx = 0;\n+        this.readOffset = recordSize;\n+    }\n+\n+    /**\n+     * {@see java.io.InputStream#available}\n+     */\n+    @Override\n+    public int available() throws IOException {\n+        if (readOffset < blockSize) {\n+            return blockSize - readOffset;\n+        }\n+\n+        return in.available();\n+    }\n+\n+    /**\n+     * {@see java.io.InputStream#read()\n+     */\n+    @Override\n+    public int read() throws IOException {\n+        throw new IllegalArgumentException(\n+            \"all reads must be multiple of record size (\" + recordSize +\n+            \" bytes.\");\n+    }\n+\n+    /**\n+     * {@see java.io.InputStream#read(byte[], int, int) }\n+     */\n+    @Override\n+    public int read(byte[] b, int off, int len) throws IOException {\n+        if ((len % recordSize) != 0) {\n+            throw new IllegalArgumentException(\n+                \"all reads must be multiple of record size (\" + recordSize +\n+                \" bytes.\");\n+        }\n+\n+        int bytes = 0;\n+\n+        while (bytes < len) {\n+            // we need to read from the underlying stream.\n+            // this will reset readOffset value.\n+            // return -1 if there's a problem.\n+            if ((readOffset == blockSize) && !readBlock(true)) {\n+                return -1;\n+            }\n+\n+            int n = 0;\n+\n+            if ((readOffset + (len - bytes)) <= blockSize) {\n+                // we can read entirely from the buffer.\n+                n = len - bytes;\n+            } else {\n+                // copy what we can from the buffer.\n+                n = blockSize - readOffset;\n+            }\n+\n+            // copy data, increment counters.\n+            System.arraycopy(blockBuffer, readOffset, b, off, n);\n+            readOffset += n;\n+            bytes += n;\n+            off += n;\n+        }\n+\n+        return bytes;\n+    }\n+\n+    /**\n+     * Skip bytes. Same as read but without the arraycopy.\n+     * {@see java.io.InputStream#read(byte[], int, int) }\n+     */\n+    @Override\n+    public long skip(long len) throws IOException {\n+        if ((len % recordSize) != 0) {\n+            throw new IllegalArgumentException(\n+                \"all reads must be multiple of record size (\" + recordSize +\n+                \" bytes.\");\n+        }\n+\n+        long bytes = 0;\n+\n+        while (bytes < len) {\n+            // we need to read from the underlying stream.\n+            // this will reset readOffset value. We do not perform\n+            // any decompression if we won't eventually read the data.\n+            // return -1 if there's a problem.\n+            if ((readOffset == blockSize) &&\n+                    !readBlock((len - bytes) < blockSize)) {\n+                return -1;\n+            }\n+\n+            long n = 0;\n+\n+            if ((readOffset + (len - bytes)) <= blockSize) {\n+                // we can read entirely from the buffer.\n+                n = len - bytes;\n+            } else {\n+                // copy what we can from the buffer.\n+                n = blockSize - readOffset;\n+            }\n+\n+            // do not copy data but still increment counters.\n+            readOffset += n;\n+            bytes += n;\n+        }\n+\n+        return bytes;\n+    }\n+\n+    /**\n+     * Close the input stream.\n+     *\n+     * @throws IOException on error\n+     */\n+    @Override\n+    public void close() throws IOException {\n+        if (in != null) {\n+            if (in != System.in) {\n+                in.close();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Peek at the next record from the input stream and return the data.\n+     *\n+     * @return The record data.\n+     * @throws IOException on error\n+     */\n+    public byte[] peek() throws IOException {\n+        // we need to read from the underlying stream. This\n+        // isn't a problem since it would be the first step in\n+        // any subsequent read() anyway.\n+        if ((readOffset == blockSize) && !readBlock(true)) {\n+            return null;\n+        }\n+\n+        // copy data, increment counters.\n+        byte[] b = new byte[recordSize];\n+        System.arraycopy(blockBuffer, readOffset, b, 0, b.length);\n+\n+        return b;\n+    }\n+\n+    /**\n+     * Read a record from the input stream and return the data.\n+     *\n+     * @return The record data.\n+     * @throws IOException on error\n+     */\n+    public byte[] readRecord() throws IOException {\n+        byte[] result = new byte[recordSize];\n+\n+        if (-1 == read(result, 0, result.length)) {\n+            throw new ShortFileException();\n+        }\n+\n+        return result;\n+    }\n+\n+    /**\n+     * Read next block. All decompression is handled here.\n+     *\n+     * @param decompress if false the buffer will not be decompressed.\n+     *        This is an optimization for longer seeks.\n+     * @return false if End-Of-File, else true\n+     */\n+    private boolean readBlock(boolean decompress) throws IOException {\n+        boolean success = true;\n+\n+        if (in == null) {\n+            throw new IOException(\"input buffer is closed\");\n+        }\n+\n+        if (!isCompressed || (currBlkIdx == -1)) {\n+            // file is not compressed\n+            success = readFully(blockBuffer, 0, blockSize);\n+            bytesRead += blockSize;\n+        } else {\n+            in.read(blockBuffer, 0, 4);\n+            bytesRead += 4;\n+\n+            int h = DumpArchiveUtil.convert32(blockBuffer, 0);\n+            boolean compressed = (h & 0x01) == 0x01;\n+\n+            if (!compressed) {\n+                // file is compressed but this block is not.\n+                success = readFully(blockBuffer, 0, blockSize);\n+                bytesRead += blockSize;\n+            } else {\n+                // this block is compressed.\n+                int flags = (h >> 1) & 0x07;\n+                int length = (h >> 4) & 0x0FFFFFFF;\n+                byte[] compBuffer = new byte[length];\n+                success = readFully(compBuffer, 0, length);\n+                bytesRead += length;\n+\n+                if (!decompress) {\n+                    // just in case someone reads the data.\n+                    Arrays.fill(blockBuffer, (byte) 0);\n+                } else {\n+                    switch (DumpArchiveConstants.COMPRESSION_TYPE.find(flags &\n+                        0x03)) {\n+                    case ZLIB:\n+\n+                        try {\n+                            Inflater inflator = new Inflater();\n+                            inflator.setInput(compBuffer, 0, compBuffer.length);\n+                            length = inflator.inflate(blockBuffer);\n+\n+                            if (length != blockSize) {\n+                                throw new ShortFileException();\n+                            }\n+\n+                            inflator.end();\n+                        } catch (DataFormatException e) {\n+                            throw new DumpArchiveException(\"bad data\", e);\n+                        }\n+\n+                        break;\n+\n+                    case BZLIB:\n+                        throw new UnsupportedCompressionAlgorithmException(\n+                            \"BZLIB2\");\n+\n+                    case LZO:\n+                        throw new UnsupportedCompressionAlgorithmException(\n+                            \"LZO\");\n+\n+                    default:\n+                        throw new UnsupportedCompressionAlgorithmException();\n+                    }\n+                }\n+            }\n+        }\n+\n+        currBlkIdx++;\n+        readOffset = 0;\n+\n+        return success;\n+    }\n+\n+    /**\n+     * Read buffer\n+     */\n+    private boolean readFully(byte[] b, int off, int len)\n+        throws IOException {\n+        int count = 0;\n+\n+        while (count < len) {\n+            int n = in.read(b, off + count, len - count);\n+\n+            if (n == -1) {\n+                throw new ShortFileException();\n+            }\n+\n+            count += n;\n+        }\n+\n+        return true;\n+    }\n+\n+    /**\n+     * Get number of bytes read.\n+     */\n+    public long getBytesRead() {\n+        return bytesRead;\n+    }\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/compress/archivers/dump/UnrecognizedFormatException.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.commons.compress.archivers.dump;\n+\n+\n+/**\n+ * Unrecognized Format Exception. This is either not a recognized dump archive or there's\n+ * a bad tape segment header.\n+ */\n+public class UnrecognizedFormatException extends DumpArchiveException {\n+    private static final long serialVersionUID = 1L;\n+\n+    public UnrecognizedFormatException() {\n+        super(\"this is not a recognized format.\");\n+    }\n+}\n--- /dev/null\n+++ b/src/main/java/org/apache/commons/compress/archivers/dump/UnsupportedCompressionAlgorithmException.java\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.commons.compress.archivers.dump;\n+\n+\n+/**\n+ * Unsupported compression algorithm. The dump archive uses an unsupported\n+ * compression algorithm (BZLIB2 or LZO).\n+ */\n+public class UnsupportedCompressionAlgorithmException\n+    extends DumpArchiveException {\n+    private static final long serialVersionUID = 1L;\n+\n+    public UnsupportedCompressionAlgorithmException() {\n+        super(\"this file uses an unsupported compression algorithm.\");\n+    }\n+\n+    public UnsupportedCompressionAlgorithmException(String alg) {\n+        super(\"this file uses an unsupported compression algorithm: \" + alg +\n+            \".\");\n+    }\n+}", "timestamp": 1313405224, "metainfo": ""}